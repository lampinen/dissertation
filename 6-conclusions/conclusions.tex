\chapter{Conclusions \& looking ahead} \label{chapter:conclusions}

Despite the recent success of deep learning, it still lacks some of the features of human intelligence. In this dissertation, I have focused on how humans are able to reuse our knowledge flexibly in new settings, before we acquire any experience in those settings. I have suggested this flexibility is supported by a computational ability to transform prior task representations to adapt them to a new task. I have proposed meta-mapping -- higher-order tasks that transform task representations -- as a computational model of this type of adaptation. \par
In order to evaluate this proposal, I have provided a parsimonious implementation of the meta-mapping framework in the form of Homoiconic Meta-Mapping (HoMM) architectures. I have demonstrated the effectiveness of HoMM by showing its zero-shot task performance across a wide variety of domains, from polynomial regression to visual classification and reinforcement learning. HoMM is often able to achieve 80-90\% performance on a new task with no data on that task at all. These results bring deep learning models a step closer to human-like flexibility. My work therefore has implications for both cognitive science and artificial intelligence. In this chapter I will review these contributions, and the broader implications of this project. \par  

\section{Contributions}

I have proposed meta-mappings as a computational account of the human ability to perform a novel task zero-shot (without any data), based on the relationship between the novel task and prior tasks. The fundamental idea is that tasks should be performed from a task representation, and that adaptation can be implemented as a transformation of a task representation. Thus adaptation can be interpreted as a meta-mapping, a higher-order task that maps between representations of more basic tasks. \par  

To instantiate this idea, I have proposed HoMM architectures. These architectures embed data points, tasks, and meta-mappings into a shared representational space, and use shared systems to infer and execute transformations of that space. That is, the same systems are used regardless of the entities (data, tasks, etc.) over which a computation is being executed. Sharing the representational space and transformation systems is parsimonious --- it does not multiply networks unnecessarily. Furthermore, I see this proposal as a logical development from the fundamental idea of meta-learning: that tasks themselves can be seen as data points in a higher-order task. This insight leads to the reciprocal idea of transforming task representations just like we manipulate data, i.e. a \emph{homoiconic} approach. This approach is both parsimonious, and outperforms a non-homoiconic approach.\par  

I have shown that meta-mapping, as implemented in the HoMM architecture, performs well across a wide range of settings. The computational paradigms I considered range from regression to classification to reinforcement learning; the inputs range from simple multi-hot vectors to images; and the cues to tasks and meta-mappings range from language to (input, output) tuple examples to (state, action, reward) examples. Across these varied settings, HoMM often achieves 80-90\% performance on a new task without data from that task, based on the relationship between the new task and a prior task. When given enough experience with the task space, as in the visual classification settings with enough training tasks, it is able to achieve perfect adaptation. In many runs (though not all), it is even able to do so with held-out meta-mappings. \par

\textbf{On the generality of meta-mapping:} Generality is a key advantage of the meta-mapping framework. Despite the substantial differences among the computational paradigms I used, the core meta-mapping architectures and approach functioned identically in all domains. This is because the meta-mapping idea only relies on the assumptions that there are task representations, and that adaptation can be represented as a transformation of a representation. Meta-mapping does not require detailed knowledge of the structure of the task space, or the details of the inputs and outputs. These domain specific details can be accomodated by domain-specific systems that would already be necessary for performing the basic tasks (for example, by using convolutional networks for input processing in domains with visual input). This makes meta-mapping relatively easy to apply in new domains. \par 

\textbf{Language generalization:} I compared HoMM to the standard paradigm of zero-shot learning --- constructing a task representation from natural language \citep[e.g.][also see below]{Larochelle2008}. While language-based generalization can be effective, our HoMM approach is generally more sample efficient at generalizing to tasks far outside its training experience, at least in the settings I considered. That is, HoMM needs fewer training tasks to generalize well zero-shot. While the language model performs comparably at interpolating to closely related tasks, as in the visual concepts domain, HoMM appears to offer stronger extrapolation to tasks farther from those on which it has been trained. This effect is demonstrated clearly when the new tasks directly contradict prior tasks, as in the card games and RL domains.\par

Furthermore, HoMM sometimes seems to exhibit more systematic generalization than the language-conditioned models. HoMM exhibited more strongly correlated performance on the held-out RL tasks --- when it was performing well on one of the tasks, it was performing well on the other. It was also better able to apply adaptation learned from tasks with switched colors to switching shapes. These results may reflect the relatively systematic behavior that humans sometimes exhibit. \par

It will require further exploration to determine with certainty why meta-mapping exhibits greater systematicity in our experiments. One possibility is that task transformation offers a more useful inductive bias than constructing a task representation from language alone in a new setting, and that this is what allows the greater systematicity of our model. That is, task transformation can allows more effective exploitation of prior task knowledge, and more targeted adaptation to the task at hand. \par

\textbf{Some notes of caution:} However, these results should not be interpreted as suggesting that language is not important or useful. Instead, language and meta-mapping should be seen as complementary. They may be applicable in different domains, and could potentially be mutually supporting. Cognition is multi-faceted, and any single model is guaranteed to be an oversimplified approximation of human cognitive processes in real-world situations. Indeed, an interesting future direction would be to consider how meta-mapping and language can mutually constrain one another when adapting to a new situation. I am not claiming that meta-mapping is the only cognitive mechanism for adaptation. Instead, my results demonstrate that meta-mapping may be useful as one tool for building models with more human-like adaptability.  \par

\textbf{Adaptation as a starting point:} I would also like to highlight the results showing that meta-mapping provides a useful starting point for later learning. While meta-learning approaches often construct a good starting point for learning any task from the known distribution, they do not use task relationships to offer a uniquely valuable starting point for each novel task. My results show that starting from adapting a prior task can substantially reduce the errors made along the way to mastering the new task. The efficiency of human learning may be partly explained by adaptation before beginning the task. \par 

\textbf{On cognitive modeling:} Cognitive modeling is always a trade-off between capturing details of the system and phenomena, and simplifying the system to make it more comprehensible; emergent behavior is generally harder to analyze than the behavior of simpler models. HoMM builds in an intermediate amount of structure, and may therefore offer a useful level of complexity that allows for flexible behavior to be learned (rather than built in) across complex tasks, while also enforcing a structure that makes the representational and computational basis for that flexibility available to analysis. \par

\textbf{Summary:} HoMM provides a model of a possible computational mechanism underlying cognitive adaptability, and the role that adaptability may play in future learning. Language likely plays an important role as well, and future work should explore uniting these approaches. 

\section{Limitations of the present explorations}

While I have explored HoMM in a relatively broad range of computational paradigms, I have still only explored a single family of architectures within a small subset of the computational paradigms that can be found in the literature. In this section I will outline a few limitations of the present work, and corresponding ideas for future research, but note these are simply examples of the many directions in which this project that could be explored further. \par

\textbf{Datasets and tasks:} First, I have demonstrated HoMM within relatively simple, small domains. The model adapts quite well, but has not achieved the fidelity of adaptation that would be expected from adult humans. I suggest that this result is partly because the model has experienced much less diversity and range of training than humans do by adulthood. Indeed, models that achieve human-level performance are generally trained on datasets several orders of magnitude larger than those I considered here, for example the the million examples in ImageNet \citep{Deng2009} or the millions of expert replays used to initialize a StarCraft model \citep{Vinyals2019}. While these datasets are quite large, the scale of data that a human has encountered by adulthood should not be underestimated. Eighteen years corresponds to \(6 \cdot 10^8\) seconds, which suggests that a lower-bound on the number of unsupervised visual training examples an adult could have had access to would be at least \(10^8\). Like all deep learning models, HoMM would likely perform better with larger datasets. Furthermore, recent work shows that more realistic environments can improve generalization \citep{Hill2019a}. Thus, evaluating meta-mapping approaches in richer, more realistic settings, with training more similar to human experience, will be an important future direction. \par

One challenge for this future work will be the construction of large sets of tasks, along with identifying the systematic relationships between them necessary for meta-mapping. The need to annotate relationships among tasks is another limitation of the meta-mapping approach. While large meta-learning datasets do exist, they do not generally contain relationships among the tasks (and often lack any interesting structure on which such relationships could be constructed). However, it is possible that in regimes with larger amounts of data, looser analogies between the tasks would be acceptable training data (just as humans don't need a perfect isomorphism to infer an analogy). This possibility will need to be explored in future work as well. \par

\textbf{Meta-mapping:} In addition, the flexibility offered by meta-mappings is still limited in some crucial ways. It requires exactly identifying the prior task to use as a source for the adaptation, and when that adaptation should occur (i.e. where the task boundary is). It would be more realistic to relax these assumptions. Approaches to tracking task-changes have been proposed in domains like meta-learning \citep[e.g.][]{Nagabandi2019}, and could likely be adapted to this setting. Exploring this would be an interesting direction for future research. \par

\textbf{HoMM architectures and algorithms:} There are a number of architectural and algorithmic aspects of the approach that could potentially be altered. Some of these are discussed in Sec. \ref{sec:HoMM:discussion:arch_and_alg_choices}. Here, I highlight only the broad observations that cognitive processing is much more complex than our model. While I relied on simple feed-forward computations for our simple tasks, using a recurrent task network, or even a more complex architecture involving external memory --- such as the Differentiable Neural Computer \citep{Graves2016} --- would likely increase the ability of the model to perform and adapt on complex tasks. More complex processing would likely be necessary to reach human-level performance in many domains.\par 

\textbf{Reliance on human choices:} Meta-mapping (like almost all machine learning approaches) is fundamentally restricted to computational paths that are carefully chosen by the humans who implement it. The model cannot decide when it is appropriate to meta-map, it is forced to do so when I thought it was appropriate. It would be interesting to explore whether a model could learn when meta-mapping was appropriate in an end-to-end manner given appropriate capabilities and input. That is, if a model had the \emph{capacity}, at any point in time, to transform prior task representations to adapt the present situation, could it learn when it \emph{should} do so? There are a number of linguistic cues that humans can exploit for when they should try to adapt, and a system that experiences many such transitions might learn when adaptation is warranted. This could potentially bring the flexibility of the model closer to that of humans.\par 

\textbf{Summary:} There are a number of limitations to the present work, and to many modern deep learning models, that should be explored in greater detail in the future. However, I believe this work also offers some useful perspective on various issues within the fields of cognitive science and artificial intelligence, which I will discuss in the subsequent sections. 


\section{On flexibility in natural and artificial intelligence}

In the introduction, I noted how researchers in cognitive science have critiqued deep learning for its lack of flexibility \citep[e.g.][]{Lake2015, Lake2016, Lake2017, Marcus2018}. I have addressed one challenging aspect of flexibility in this work -- the ability to take our knowledge of a task, and adapt to some variation. While this adaptation might be challenging for standard deep-learning models, I have shown that the general framework of meta-mapping makes it possible. Thus, at their most basic level, my results present a challenge for those who would say deep-learning models are too inflexible to be accurate cognitive models. \par  

Indeed, I see my project as following in the tradition of work that explores how systematic, structured generalization can emerge from the structure of learning experience, without needing to be built into the model itself \citep{McClelland2010a, McClelland2010, Hansen2017}. This tradition is a challenge to arguments that cognition must rely on strictly compositional representations in order to exhibit systematic and productive generalization \citep[e.g.][]{Fodor2001, Fodor2008lot2, Lake2017}. Without building in compositional representations of tasks, our model can learn to exploit the shared structure in the concept of ``losing'' across a few card games to achieve 85\% performance in losing a game it has never tried to lose before. Similarly, it can achieve perfect adaptation to held-out visual concepts via trained meta-mappings, and near-perfect adaptation from held-out meta-mappings. Hard-coded compositional structure does not appear necessary to achieve good adaptation. \par 

Furthermore, there are a number of potential benefits to letting the compositional structure emerge rather than building it in. First, the structure does not need to be hand-engineered specially for each domain. Our system required no special knowledge about the domains beyond the basic tasks and the relationships between them. The fact that some of these relationships corresponded to e.g. permutations of variables in the polynomial domain did not need to be hard-coded, instead the model was able to discover it from the patterns of the mappings (as indexed by its ability to generalize well to held-out permutations). \par

The second advantage of letting compositionality emerge is that it can potentially allow for novel decompositions at test time. The ability of our model to perform well on held-out meta-mappings supports this hope. Furthermore, the ability of the model to extrapolate a meta-mapping learned on color tasks to shape tasks in the RL domain provides further promising evidence. These results are suggestive of the ability of HoMM to extrapolate beyond what it has experienced with flexibility and systematicity closer to that of human cognition.\par

In summary, I suggest that meta-mapping offers a way to create deep learning models with flexibility slightly closer to that of the human mind. While I have demonstrated these results in some simple settings, one of the powerful features of deep learning is that its results tend to improve as datasets grow more complex and realistic \citep{Hill2019a,Radford2019,Sutton2019}. I hope that this research will help guide the way to building even more flexible models in more realistic domains. \par

\section{Relating to cognitive science}

Our work provides a tool for modeling human adaptability, which has many potential direct applications. It offers an explanation for how humans might be able to adapt, albeit imperfectly, when told ``watch out, the floor is slippery,'' or recognize a pink-and-green striped car even if they have never seen one before, by transforming their task representations based on prior experience. This adaptability is a fundamental aspect of human intelligence, but is often omitted from cognitive models. However, our work has broader relevance as well. \par 

\textbf{Fast and slow transfer:} In Chapter \ref{chapter:introduction}, I reviewed the cognitive science and machine learning literatures from a Complementary Learning Systems perspective. In particular, I sugested that humans' slow learning of shared structure in the world can itself provide transfer benefits, but also helps set up the representations necessary for faster transfer mechanisms. This idea is reflected in the organization of the HoMM architecture. A great deal of perceptual and action processing is shared across tasks (though see below), so that the model can exploit the shared visual features of different games or objects. The representations constructed by these shared systems are used for both task inference and for task performance. This approach allows the system to perform a novel task from a few examples (as in standard meta-learning), or based solely on its relationship to prior tasks, by meta-mapping. The fast zero-shot transfer achieved by meta-mapping thus relies on the representations of tasks and data that are constructed over the full development of the network. \par

Furthermore, this fast transfer ability itself must be learned over time. However, once it has been learned, it can then generalize to new examples and even new meta-mappings. This approach reflects my suggestion that humans not only learn good representations for fast transfer, but actively practice the act of adaptation. I hope my work will inspire broader thought about how different systems of transfer, operating over different timescales, can support each other in order to achieve the flexibility of human intelligence.\par

\textbf{Abstraction \& recursion:} Abstraction \& recursion offer one exciting area where our model could potentially offer a new modeling framework. It would be interesting to explore how concepts can be recursively built upon other concepts, as happens in learning of mathematics \citep{Wilensky1991, Hazzan1999, Lampinen2017b}. For example, addition can be seen as repeated succession, multiplication can be seen as repeated addition, exponentiation as repeated multiplication, and this process is recursively continued in up-arrow notation. A homoiconic system like HoMM may take us a step closer to capturing this recursive construction of concepts. It would be interesting to explore how our architectures could model this type of recursive construction of concepts. \par 

Relatedly, I believe that my model moves closer to capturing some of the recursive processing that Fodor and others have considered to be important \citep[e.g.][]{Fodor2008lot2}. I have drawn particular inspiration from the idea that humans re-represent our knowledge into more generalizable forms \citep{Karmiloff-Smith1986,Clark1993}. Karmiloff-Smith examines fascinating developmental trajectories where, even after initial behavioral mastery of some concept is achieved, various implicit and explicit measures of understanding continue to evolve. How could this process, which Karmiloff-Smith calls representational redescription, actually work? It would be interesting to explore whether auxiliary learning objectives over task representations and meta-mappings, and the shaping effects of language (see below), could model some of these phenomena. Could the change in behavior on a particular task be driven by the evolution of its task representation while learning a meta-mapping involving that task? Could unsupervised learning over task representations make clusters and structure within the space of tasks salient, thereby regularizing and structuring behavior? Exploring these questions will be an exciting direction for future work. \par 

\textbf{Consciousness:} I have also been inspired by computational models of how conscious knowledge may be built on top of implicit knowledge \citep{Cleeremans2014}, as well as by the Global Workspace Theory \citep{Baars2005}. HoMM's shared representational space for data points, tasks, and meta-mappings can be seen as a global workspace, over which task-specific computations can be executed. Indeed, the HoMM architecture could potentially shed light on issues about explicit vs. implicit knowledge --- it is plausible to assume that a task representation captures what we know explicitly about a task, while implicit knowledge could be captured both by the task representation and elsewhere (such as in the default weights of the task network). Exploring these connections further could provide another exciting direction for future work. \par

\textbf{Modularity vs. generality:} The issues above also relate to ideas Fodor expressed about the architecture and computations of the mind, for example the view of mental processes as ``transforming internal representations'' and that what is accessible about the stimulus is only ``what is given in [...] its \emph{proximal} representations'' \citep[][pp. 200-201]{fodor1975language}. The division of the HoMM architecture into input and output systems, with flexible, task-specific computations in the middle may seem very reminiscient of the type of modular architecture that he sometimes advocated \citep{Fodor1983modularity}. However, I chose this implementation as a simplifying assumption --- I believe that in reality processes such as perception are not completely task-independent, but involve the interaction of top-down and bottom-up constraints \citep{McClelland2014}. \par

Reciprocally, I also believe that higher-level computations are influenced and constrained by the modalities in which they are supported. This computational feature can emerge in the HoMM model, as despite the fact that different types of data and tasks are embedded in a shared latent space, the model generally learns to organize distinct types of inputs into somewhat distinct regions of this space. This organization means that the task-specific processing can potentially usefully exploit domain-specific features of the input, as for example humans do when they use gestures to think and learn in spatial contexts like mathematical reasoning \citep{Goldin-Meadow1999, Wakefield2018}. At the same time, the shared space can allow a graded overlap in the structure that is shared across different input domains. That is, the HoMM model is able to learn what should be shared and what should be separated, whereas approaches that build in such divisions are fundamentally more limited. HoMM has the desirable property that ``modularity may not be built in [but] may result from the relationship among representations'' \citep{Tanenhaus1987}.\par

\textbf{Language:} I noted above that our results should not be taken as a rejection of the role of language. Instead, they suggest that meta-mapping and language could be mutually supporting. It would be interesting to explore whether combining the representations produced by the language system and meta-mapping system could result in better performance than either alone, especially if this combination were weighted by some measure of uncertainty in the estimates. Furthermore, while I only considered language as an input in the present work, language output (explaining behavior) can play an important role in learning, both in humans \citep{Chi1994}, and in neural networks \citep{Mu2019}. While my use of task-representation-classification in some settings may have captured some aspects of this structuring, it did not appear to have a substantial effect. Requiring the model to produce richer explanations during learning will likely be important for achieving truly human-like representations and behavior. Architectures like the one I proposed could provide a basis for exploring the interactions between language and reasoning over development, and how the connection between language and thought ``originates, changes, and grows'' \citep{Vygotsky1934}. 

\textbf{Cognitive control:} Although it is not my primary focus in this project, the HoMM architecture may have interesting connections to cognitive control. Even without meta-mapping, HoMM instantiates an architecture that can compute flexibly in response to task demands, provided as examples or natural language. Furthermore, the ``default'' task-network weights output by the HyperNetwork could be used to model more automatic processing, which more cognitive, task-specific processing might need to override. I showed some initial experiments related to this in Chapter \ref{chapter:zero_shot_via_homm}. Meta-mapping adds many additional potential connections to control --- for example, a failure to meta-map perfectly could capture some of the challenges of task-switching. Exploring these ideas further would also be an interesting direction for future work. \par

\textbf{Neuroscience:} Finally, a major advantage of neural network models is their ability to make predictions about neuroscience. For example, neural network models have been used to understand aspects of the neural basis for perception \citep{Yamins2016a}, semantic cognition \citep{Rogers2004}, and cognitive control \citep{Shenhav2013}. I have not engaged with this level of analysis, but doing so would be an exciting direction for future work. Our HoMM architecture offers a framework that can unify perception, task representation, control, and decision/action, all within a single model. It would be possible to relate the different components of our model to different brain regions --- visual perception to visual cortex, higher level perceptual features to more semantic regions, the action network to motor cortex, and the meta/hyper/task networks to frontal regions associated with task representation/control/working memory. Thus, our architecture could potentially provide an integrative model spanning a wide range of brain regions, although it would likely require some modifications to account for neural data well (some of which are discussed in the limitations section above). \par 

\textbf{Summary:} I take an emergent perspective on the structure of the mind, and believe that all cognitive and perceptual processes are mutually influencing and supporting. For simplicity and clarity my model does not always fully reflect these perspectives. Furthermore, I believe my approach may be broadly useful, even to researchers with different perspectives. The functional approach relates to the ideas of Fodor and Karmiloff-Smith, the perspective on adaptation draws inspiration from prior work on analogy and transfer, and the HoMM architecture could even have interesting implications for researchers interested in cognitive control or neuroscience. I hope that researchers from many of these areas will find my work to provide a useful perspective when addressing these issues.

\section{Relating to artificial intelligence}

There are a number of potential direct applications in artificial intelligence, from building more flexible vision models to building better systems for robotics. Domains like robotics are especially interesting from the meta-mapping perspective, because exploration in real world settings is costly and must be safe \citep{Turchetta2016}, and so the substantial reduction in errors made when using meta-mapping as the starting point for learning a new task may be valuable. \par 

\textbf{Reinforcement learning:} Applying meta-mapping to different types of adaptation in RL also opens many possibilites, especially in combination with model-based methods. Meta-mapping could be used as a principled way of adapting transition functions or successor representations \citep[c.f.][]{Madarasz2019}, beyond the approach of adapting model-free reward or value estimates that I demonstrated. While adapting pure model-free RL will likely be challenging in more complex task spaces, combining meta-mapping with other insights could yield much greater flexibility. For example, meta-mapping could be used with hierarchical models where language has been used as a task or sub-task representation \citep[e.g.][]{Jiang2019}. Similarly, it could be applied in planning based models, for example using monte-carlo tree search \citep[as in e.g.][]{Silver2016, Silver2017}, but with task-representation-conditioned policy and value functions. More ambitiously, meta-mapping could be explored in models that learn to plan \citep{Guez2019} rather than having that planning hand-engineered into the architecture. Many contemporary RL frameworks could potentially be augmented with meta-mapping. \par

\textbf{Abstraction:} Many of the directions of future investigation from a cognitive perspective relate to pressing problems in artificial intelligence as well. The issue of flexible abstraction is challenging in deep learning --- while feed-forward neural networks generally construct progressively more abstract representations in higher layers, the relationships between those representations are fixed by the fixed computational pathway. The shared representational space and meta/hyper networks in our model provide a suggestion for how concepts at different levels of abstraction could be integrated and used more flexibly. A model that demonstrated this ability would be another important step towards human-like flexibility. \par 

\textbf{Continual learning:} The work in Chapter \ref{chapter:timescales} suggests new directions in continual learning. By off-loading much of the task-specific computation to a flexible hyper-network-based architecture, and inferring and optimizing a task representation, we can enable learning of a new task without even the possibility of interfering with prior tasks. Furthermore, we can leverage our knowledge of prior tasks to learn faster than we would have in an untrained model, or without meta-mapping. This positive transfer is very different from the standard in continual learning, which mostly focuses on stemming the catastrophic loss of accuracy on prior tasks caused by learning new tasks, even in more recent works that have also incorporated hyper networks \citep{Oswald2020}. Given the importance of learning rapidly on new tasks, without interfering with prior knowledge, this approach to continual learning is also an exciting future direction. \par 

\textbf{Hyper networks for multiple task domains:} As machine-learning research has begun to focus on multi-task- and meta-learning, an increasing number of approaches have been developed which construct a task representation \citep{Hermann2017, Zintgraf2018, Rusu2019}, although not all those works explicitly identify the task representation as such. Some of these works, such as  \citet{Rusu2019}, use the task representation to parameterize a task-specific-network, as in HoMM. However, most simply concatenate the task representation to other network inputs. I compared both of these architectural approaches, and found that while both performed well on the basic tasks, the hyper-network architecture performed better at meta-mapping, and allowed for better optimization of task representations. This raises the possibility that hyper-network approaches may be better able to accomodate performing qualitatively different \emph{types} of tasks in a shared architecture. It would be interesting to investigate this as multi-task learning moves beyond single-domain task distributions (such as a collection of vision tasks) to the generality of task domains that a human might encounter in a day (vision, language, control, and so on). 

\textbf{Summary:} In addition to the direct applications of the meta-mapping framework in building more flexible artificial intelligence systems, it suggests many exciting future directions in reinforcement learning, abstraction, multi-task learning, and continual learning. I hope that this project will help inspire the development of deep learning systems that can adapt and learn more like humans. \par 

\section{Looking ahead}

The next ten years will likely bring greater clarity about how far deep learning is from achieveing human-like intelligence. As I suggested in the introduction, it may be that greater scale and complexity of our architectures and training regimes will bring forth more flexible behavior from many deep neural network models. Indeed, flexibility will almost certainly increase to some extent --- overparameterization and larger datasets both tend to improve the generalization performance of deep neural networks. If translation abilities can emerge from a word-prediction model given enough data \citep{Radford2019, Brown2020}, couldn't the ability to adapt emerge from simple architectures trained in complex enough environments? It is not yet clear. What will the contribution of this dissertation be to our future knowledge? Will it be more than another ``bitter lesson'' demonstrating that ``building in how we think we think does not work in the long run'' \citep{Sutton2019}? \par 

From an artificial intelligence perspective, even if more flexible behavior emerges in other architectures when they are placed in richer training regimes, the architectural and training innovations I have proposed in this work may still provide useful insights that allow flexibility to emerge at more feasible data scales. It is not clear how much data would be required to learn human-like flexibility by brute force, or whether it is even possible at all. Many applications of artificial intelligence (from medical diagnosis to self-driving cars) would benefit from models with greater flexibility within a family of closely related tasks. The approach I have proposed may be useful in such cases. The perspective of building and transforming task representations may also inspire future work that leverages similar abstractions --- perhaps architectures that use task-representation and HyperNetwork-based approaches like ours will be better able to accommodate diverse sets of tasks from different domains, or will be useful for challenging new evaluations of intelligence \citep[e.g.][]{Chollet2019}. \par 

From a cognitive perspective, my work attempts to provide a computational basis for understanding the flexibility of the human mind. The HoMM approach allows for learning of flexible adaptation in complex task settings, within a framework that makes this behavior interpretable. Thus, HoMM may provide a useful tool for understanding human adaptation, that could be applied to many cognitive tasks. My work may also provide a framework for instantiating theories about the neural basis of higher-level cognitive processing. \par 

Ultimately, I have presented one computational perspective on how natural and artificial intelligence could flexibly adapt to new situations. I am excited to see the new perspectives the future will bring, and I hope my work will provide some inspiration for some of them. 
