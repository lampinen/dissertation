\chapter{Conclusions \& looking ahead} \label{chapter:conclusions}

While artificial intelligence has achieved incredible things in recent years, there are still immense differences between artifical and natural intelligence. In this dissertation, I have focused on how humans are able to reuse our knowledge flexibly in new tasks, before we have any data on them. I have suggested that one mechanism underlying this is an ability to transform prior task representations to adapt them to the new task. I have proposed meta-mapping -- higher-order tasks that transform task representations -- as a plausible computational mechanism for this adaptation. \par
In order to test this idea, I have provided a parsimonious implementation of this framework in the form of Homoiconic Meta-Mapping (HoMM) architectures. I have demonstrated the effectiveness of HoMM by showing its zero-shot task performance across a wide variety of domains, from polynomial regression to visual classification and reinforcement learning. HoMM is often able to achieve 90\% performance on a new task with no data on that task at all. This brings deep learning models a step closer to human-like flexibility. The present work therefore has implications for both cognitive science and artificial intelligence. \par  

\section{Contributions}

I have proposed meta-mappings as a computational account of the human ability to perform a novel task zero-shot (without any data), based on the relationship between the novel task and prior tasks. The fundamental idea of meta-mappings is that tasks should be performed from a task representation, and that adaptation can be seen as transforming this representation. This adaptation can be seen as a meta-mapping, a higher-order task that maps between representations of more basic tasks. \par  

To instantiate this idea, I have proposed HoMM architectures. These architectures embed data points, tasks, and meta-mappings in a shared representational space, and use shared systems to infer and execute transformations of that space, regardless of the type of transformation in question. I see this proposal as a logical development from the fundamental idea of meta-learning --- that tasks themselves can be seen as data points in a higher-order task of learning-to-learn. This naturally leads to the idea of transforming task representations just like we manipulate data. \par  

I have shown that the HoMM architecture performs well across a wide range of settings, ranging from regression to classification to reinforcement learning, with both toy and complex inputs. HoMM often achieves 90\% performance on a new task with no data on that task at all, based on its relationship to a prior task. When given enough experience with the task space, as in the visual classification settings with enough training tasks, it is able to achieve perfect adaptation. \par

I compared to the standard paradigm of zero-shot learning --- constructing a task representation from natural language \citep[e.g.][also see below]{Larochelle2008}. While language-based generalization can be effective, our HoMM approach is generally more sample efficient, at least in the settings we considered. That is, HoMM needs fewer training tasks to generalize well zero-shot. This effect is stronger when the new tasks strongly contradict prior tasks, as in the card games and RL setting. \par

Furthermore, HoMM seems to exhibit more systematic generalization than the language-conditioned models. For example, HoMM resulted in more runs with perfect generalization at moderate-sample sizes in the visual concept tasks, while the language generalization results were more graded. HoMM also exhibited more strongly correlated performance on the held-out RL tasks --- when it was performing well on one of the tasks, it was performing well on the other. This is more like the systematic behavior that humans often exhibit. \par

However, these results should not be interpreted as suggesting that language is not important or useful. Instead, language and meta-mapping should be seen as complementary. They may be applicable in different domains, and could potentially be mutually supporting. Cognition is complex, and any single model is guaranteed to be an oversimplified approximation of human cognitive processes in real-world situations. Indeed, an interesting future direction would be to consider how meta-mapping and language can mutually constrain one another when adapting to a new situation. Our results should not be taken as a suggestion that meta-mapping is the only cognitive mechanism for adaptation. Instead, our results demonstrate that meta-mapping may be useful as one tool for building models with more human-like adaptibility. \par

We would also like to highlight the results showing that meta-mapping provides a useful starting point for later learning. While meta-learning approaches often construct a good starting point for learning any task from the known distribution, they do not use task relationships to offer a uniquely valuable starting point for each novel task. Our results show that starting from adapting a prior task can substantially reduce the errors made along the way to mastering the new task. This may be an important reason why humans are able to learn efficiently. \par 

Our results thus represent a substantial contribution to understanding a possible computational mechanism underlying cognitive adaptibility, and the role it may play in future learning. 

\subsection{On flexibility in natural and artificial intelligence}

In the introduction, I noted how researchers in cognitive science have aggressively critiqued deep learning for its lack of flexibility \citep[e.g.][]{Lake2015, Lake2016, Lake2017, Marcus2018}. We have addressed one challenging aspect of flexibility in this work -- the ability to take our knowledge of a task, and adapt to some variation. While this might be challenging for standard deep-learning models, the general framework of meta-mapping makes it possible. Thus, at their most basic level, these results present a challenge for those who would say that deep learning models are inappropriate models for cognitive processes because they are inflexible. \par  

Indeed, I see my project as following in the tradition of work that explores how systematic, structured generalization can emerge from the structure of learning experience, without needing to be built into the model itself \citep{McClelland2010a, McClelland2010, Hansen2017}. This is a challenge to arguments that cognition must rely on strictly compositional representations in order to exhibit systematic and productive generalization \citep[e.g.][]{Fodor2001, Fodor2008lot2, Lake2017}. Without building in compositional representations of tasks, our model can learn to exploit the shared structure in the concept of ``losing'' across a few card games to achieve 85\% performance in losing a game it has never tried to lose before. \par 

There are a number of potential benefits to letting the compositional structure emerge --- for example, it could potentially allow for novel decompositions at test time. Indeed, the ability of our model to switch to losing a game it has never tried to lose before is somewhat like inferring a novel decomposition, but we would hope that a model with richer training data could make much larger inferential leaps. The ability of our model to perform well on held-out meta-mappings supports this hope, but further work will be needed to verify it. \par

With these caveats in mind, however, I believe that meta-mapping offers a way to create models that can bring the intelligence of deep learning systems closer to the flexibility of the human mind. While we have demonstrated these results in some simple settings, one of the powerful features of deep learning is that its results tend to improve as datasets grow more complex and realistic \citep{Hill2019a,Radford2019}. We hope that our research will help guide the way to building even more flexible models in more realistic domains.  \par

\subsection{Relating to cognitive science}

Our work provides a tool for modeling human adaptibility, which has many potential direct applications. It offers an explanation for how humans might be able to adapt when told ``watch out, the floor is slippery,'' or recognize a pink-and-green striped car even if they have never seen one before, by transforming their task representations. This is a fundamental aspect of human intelligence, that is often omitted from cognitive models. However, our work has broader relevance as well. \par 

First, our architecture opens up possibilites for exploring abstraction and recursion in cognition more broadly. It would be interesting to explore how concepts can be recursively built upon other concepts, as happens in learning of mathematics \citep{Wilensky1991, Hazzan1999, Lampinen2017b}. For example, addition can be seen as repeated succession, multiplication can be seen as repeated addition, exponentiation as repeated multiplication, and this process is recursively continued in up-arrow notation. A homoiconic system like HoMM seems closer to being able to capture this recursive construction of concepts. It would be interesting to explore how our architectures could be used for modeling learning in similar settings. \par 

Relatedly, I believe that my model moves closer to capturing some of the recursive processing that Jerry Fodor and others have considered to be important \citep[e.g.][]{Fodor2008lot2}. I have drawn particular inspiration from ideas about how humans re-represent their knowledge into more generalizable forms \citep{Karmiloff-Smith1986,Clark1993}. That work examines fascinating developmental trajectories where, even after initial behavioral mastery of some concept is achieved, various implicit and explicit measures of understanding continue to evolve. It would be interesting to explore whether learning over task representations and meta-mappings could model some of these phenomena. Could the change in behavior on a particular task be driven by the evolution of its task representation while learning a meta-mapping involving that task? Could unsupervised learning over task representations make clusters and structure within the space of tasks salient, thereby regularizing and structuring behavior? Exploring these questions will be an exciting direction for future work. \par 

I have also been inspired by computational models of how conscious knowledge may be built on top of implicit knowledge \citep{Cleeremans2014}, as well as by the Global Workspace Theory \citep{Baars2005}. HoMM's shared representational space for data points, tasks, and meta-mappings can be seen as a global workspace, over which task-specific computations can be executed. Exploring these connections further could provide an exciting direction for future work. \par

This also relates to ideas Fodor expressed about the modular structure of the mind, for example the view of mental processes as ``transforming internal representations'' and that what is accessible about the stimulus is only ``what is given in [...] its \emph{proximal} representations'' \citep[][pp. 200-201]{fodor1975language}. Indeed, our division of the architecture into input and output systems, with the flexible, task-specific computations in the middle may seem very reminiscient of the type of modularity that he sometimes advocated \citep{fodor1983modularity}. However, we chose this implementation as a simplifying assumption --- we believe that in reality processes such as perception are not task-independent, but involve the interaction of top-down and bottom-up constraints \citep{McClelland2014}. \par

Reciprocally, we also believe that higher-level computations are influenced and constrained by the modalities in which they are supported. This computational feature can emerge in our model, as despite the fact that different types of data and tasks are embedded in a shared latent space, the model generally learns to organize distinct types of inputs into somewhat distinct regions of this space. This means that the task-specific processing can potentially usefully exploit domain-specific features of the input, as for example humans do when they use gestures to think and learn \citep{Goldin-Meadow1999, Wakefield2018}. At the same time, the shared space can allow a graded overlap in the structure that is shared across different input domains. \par

Furthermore, although it is not my primary focus in this project, the HoMM architecture may have interesting connections to cognitive control. A failure to meta-map perfectly could capture some of the challenges of task-switching, as could contamination of the task representation by conflicting examples from other tasks. Futhermore, the ``default'' task-network weights output by the HyperNetwork could be used to model more automatic processing, which more cognitive, task-specific processing might need to override. I showed some initial experiments related to this in Chapter \ref{chapter:zero_shot_via_homm}. \par

Finally, a major advantage of neural network models is their ability to make predictions about neuroscience. For example, neural network models have been used to understand aspects of semantic representations in the brain \citep{Rogers2004}, cognitive control \citep{Shenhav2013}, and perception \citep{Yamins2016a}. We have not engaged with this level of analysis, but doing so would be an exciting direction for future work. Our HoMM architecture offers a framework that can unify task representation, perception, and motor outputs within a single model. It would be possible to relate the different components of our model to different brain regions --- visual perception to visual cortex, higher level perceptual features to more semantic regions, the action network to motor cortex, and the meta/hyper/task networks to frontal regions associated with task representation/control/working memory. Thus, our architecture could potentially provide an integrative model spanning a wide range of brain regions, although it would likely require some modifications to account for neural data well (some of which are discussed in the limitations section below). \par 

In summary, I take an emergent perspective on the structure of the mind, and believe that cognitive processes and perceptual ones are mutually influencing and supporting. For simplicity my model does not always fully reflect this. Furthermore, I believe my approach may be broadly useful, even to researchers with different perspectives. The functional approach relates to the ideas of Fodor and Karmiloff-Smith, the perspective on adaptation draws inspiration from prior work on analogy and transfer, and our architecture could even have interesting implications for researchers interested in cognitive control or neuroscience. We hope that researchers from many of these areas will find our work inspirational.

\subsection{Relating to artificial intelligence}

There are a number of potential direct applications in artificial intelligence, from building more flexible vision models to building better control models in robotics. Domains like robotics are especially interesting from the meta-mapping perspective, because exploration in real world settings is costly and must be safe \citep{Turchetta2016}, and so the substantial reduction in errors made when using meta-mapping as the starting point for learning a new task may be valuable. \par 

Applying meta-mapping to different types of adaptation in RL also opens many possibilites, especially in combination with model-based methods. Meta-mapping could be used as a principled way of adapting transition functions or successor representations \citep[c.f.][]{Madarasz2019}, as well as the possibility of adapting reward or value estimates as we demonstrated. Adapting pure model-free RL will likely be challenging in more complex task spaces. However, combining meta-mapping with other insights, could yield much greater flexibility. For example, meta-mapping could be used with hierarchical models where language has been used as a task or sub-task representation \citep[e.g.][]{Jiang2019}. Similarly, it could be applied in planning based model, for example using monte-carlo tree search \citep[as in e.g.][]{Silver2016, Silver2017}, but with task-representation-conditioned policy and value functions. More ambitiously, meta-mapping could be explored in models that learn to plan \citep{Guez2019} rather than having that planning hand-engineered into the architecture. Many contemporary RL frameworks could potentially be augmented with meta-mapping. \par

Many of the directions of future investigation from a cognitive perspective relate to pressing problems in artificial intelligence as well. The issue of flexible abstraction is challenging in deep learning --- while feed-forward neural networks generally construct progressively more abstract representations in higher layers, the relationships between these representations are fixed by the fixed computational pathway. The shared representational space and meta/hyper networks in our model provide a suggestion for how concepts at different levels of abstraction could be integrated and used more flexibly. \par 

Finally, the work in Chapter \ref{chapter:timescales} suggests new directions in continual learning. By off-loading much of the task-specific computation to a flexible hyper-network-based architecture, and inferring and optimizing a task representation, we can enable learning of a new task without even the possibility of interfering with prior tasks. This is a very different from the standard in continual learning, which mostly focuses on stemming the bleed in accuracy caused by new tasks, even in more recent works that have also incorporated hyper networks \citep{Oswald2020}. Given the importance of learning without interference, this is also an exciting future direction. \par 

In summary, in addition to the direct applications of the meta-mapping framework in building more flexible artificial intelligence systems, it suggests many exciting future directions in reinforcement learning, abstraction, and continual learning. I hope that this project will help inspire the development of deep learning systems that can adapt and learn more like humans. \par 

\subsection{Limitations of the present explorations}

While I have explored HoMM in a relatively broad range of computational paradigms, I have still only explored a single family of architecture within a small subset of the task-types that can be found in the literature. In this section I will outline a few limitations of the present work, but note these are simply examples of the many choices made in this project that could be explored further. \par

There are a number of architectural aspects of the approach that could be altered. For example, although we used HyperNetworks to parameterize our task network, it would also be reasonable to have a fixed task network which simply receives the task representation as an additional input. We tried this in some of our domains, and found it did not perform as well, but it might be a useful approach in some settings. We also noted in the visual categories domain that linear task networks seemed to improve meta-mapping, while nonlinear ones seemed to result in better basic task performance --- thus it might be reasonable to consider a deep, nonlinear task network, but with a linear skip-connection from beginning to end. \par 

Furthermore, although we found that homoiconic architectures were useful, it might be that in some task domains a shared representational space with shared networks across different types of tasks is detrimental. In general, whether sharing an architecture across different tasks is beneficial depends on the data regimes --- it can be a useful regularizer with small datasets, but correspondingly harmful with sufficiently large ones. The answer to this question will likely depend on the data regime. One cognitively-motivated intermediate option might be to have a domain general shared-system like ours, but with additional learning of more domain-specific mappings directly from inputs to outputs, which could potentially allow for the benefits of both. More generally, cognitive processing is much more complex than our model, and replacing the feed-forward task network with a recurrent network, or even a more complex architecture involving memory --- such as the Differentiable Neural Computer \citep{Graves2016} --- would likely increase the ability of the model to perform and adapt on complex tasks.\par 

Finally, the type of flexibility we have proposed is still limited in some crucial ways. It requires exactly identifying the task to be adapted, and when that adaptation should occur (i.e. where the task boundary is). It would be more realistic to relax these assumptions. This could be achieved by combining with techniques that have been used for task-change tracking in meta-learning \citep[e.g.][]{Nagabandi2019}, but could also potentially be learned end-to-end in an appropriate model given appropriate input. There are a number of linguistic cues that humans can exploit for when they should try to adapt, and a system that experiences many such transitions might learn when adaptation is warranted. \par 


\section{Looking ahead}

The next ten years will likely bring us a great deal more clarity about how far deep learning is from human-like intelligence. As I suggested in the introduction, it may be that greater scale and complexity of our architectures and training regimes will bring forth more flexible behavior from many deep neural network models. Indeed, this is almost certainly true to some extent --- overparameterization and more data both tend to improve the generalization of deep neural networks. If translation abilities can emerge from a word-prediction model given enough data \citep{Radford2019}, couldn't the ability to adapt emerge from simple architectures trained in complex enough environments? If so, what will the contribution of this dissertation be to our future knowledge? \par 

From an artificial intelligence perspective, even if more flexible behavior emerges in other architectures in richer training regimes, the architectural and training innovations I have proposed in this work may still provide useful insights that allow flexibility to emerge at more feasible data scales. Many applications of artificial intelligence (from medical diagnosis to self-driving cars) would benefit from models with greater flexibility within a family of related tasks. The perspective of building and transforming task representations may also inspire future work that leverages similar abstractions. \par 

From a cognitive science perspective, my work provides a computational basis for understanding the flexibility of the human mind. Cognitive modeling is always a trade-off between capturing details of the system and phenomena, and simplifying the system to make it more comprehensible; emergent behavior is generally harder to analyze than the behavior of simpler models. Simplified, somewhat structured models such as those we have proposed here may offer a useful level of complexity that allows for flexible behavior to be learned (rather than built in), while also making the representational and computational basis for that flexibility available to analysis. Our architectures may also provide a framework for instantiating neuroscientific theories of higher-level cognitive processing. \par 

Ultimately, I have presented one computational perspective on how natural and artificial intelligence could flexibly adapt. I am excited to see the new perspectives the future will bring, and I hope my work will provide some inspiration for some of them. 
