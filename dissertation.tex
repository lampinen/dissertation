\documentclass{report}

\usepackage{style/suthesis-2e}
\copyrightfalse
\signaturefalse

\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{epigraph}
\usepackage{float}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{mathrsfs}
\usepackage{natbib}
\usepackage{relsize}
\usepackage{subcaption}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage{refcount} % allows getting around stupid issues with references in headers + upercasing


\usepackage{tikz}

\usetikzlibrary{shapes,arrows}
\usetikzlibrary{decorations.pathreplacing, decorations.pathmorphing, snakes, calligraphy}

\tikzstyle{block} = [rectangle, draw, thick, align=center, rounded corners]
\tikzstyle{boundingbox} = [thick, lightgray]
\tikzstyle{dashblock} = [rectangle, draw, thick, align=center, dashed]
\tikzstyle{conc} = [ellipse, draw, thick, dashed, align=center]
\tikzstyle{netnode} = [circle, draw, very thick, inner sep=0pt, minimum size=0.5cm]
\tikzstyle{relunode} = [rectangle, draw, very thick, inner sep=0pt, minimum size=0.5cm]
\tikzstyle{line} = [draw, very thick, -latex']
\tikzstyle{arrow} = [draw, ->, thick]

\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}

\definecolor{bpurp}{HTML}{984ea3}
\definecolor{bblue}{HTML}{377eb8}
\definecolor{bgreen}{HTML}{4daf4a}
\definecolor{borange}{HTML}{ff7f00}
\definecolor{bred}{HTML}{a50f15}

% footnote without a marker
\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext} 
\makeatother

\include{header/header}

\dept{Psychology}

\begin{document}
\title{Toward understanding human adaptibility with deep learning models}
\author{Andrew Kyle Lampinen}
\principaladviser{Jay McClelland}
\firstreader{Noah Goodman}
\secondreader{Surya Ganguli}
%\thirdreader{Jane Supernumerary} %if needed
%\fourthreader{Severus Snape} %if needed

\beforepreface
\setcounter{page}{4}  % Stanford formatting requirements
\prefacesection{Abstract}

Human cognition is fundamentally flexible --- we can adapt to novel tasks rapidly. We can sometimes adapt to a novel task without any direct experience on that task, based on its relationship to previous tasks. By contrast, while deep-learning models can achieve superhuman performance on many tasks, they are often unable to adapt to even slight task alterations. I begin this dissertation by reviewing the literature on cognitive flexibility, and recent advances in building more flexible artificial intelligence. I provide a synthesis of these literatures, and outline the challenges that I believe remain. In particular, I focus on the ability to adapt to new tasks zero-shot, that is, without any data, based on their relationship to prior tasks.\par
To address this challenge, I propose a general computational framework for modeling adaptation to novel tasks based on their relationship to prior tasks. The framework is based on \emph{meta-mappings}, higher-order tasks that transform basic tasks. I propose a parsimonious implementation of this framework in the form of \emph{homoiconic meta-mapping} architectures. I demonstrate this framework across a wide variety of tasks and computational paradigms, ranging from regression to image classification and reinforcement learning. I compare to both human adaptibility, and language-based approaches to zero-shot task performance. I demonstrate that meta-mapping is quite succesful, often achieveing 80-90\% performance on a novel task, even when the new task directly contradicts prior experience. I further show that using this adaptation as a starting point can dramatically accelerate later learning on a task, and reduce the errors made on the way to mastery by nearly an order of magnitude. \par
Thus, I suggest that meta-mapping provides a plausible computational basis for human adaptability, and a basis for efficient learning. This dissertation therefore provides a framework for building better cognitive models and more flexible artificial intelligence systems. In the final chapter, I review the broader contributions of this work to an ongoing discussion about the computational principles necessary for intelligence, and highlight possible future directions ranging from understanding mathematical cognition to neuroscience. 

\prefacesection{Acknowledgements}

I have been very fortunate both while writing this dissertation, and along the longer road to the PhD. Many people played a role in guiding me on this journey.

First, I'd like to thank the other graduate students and researchers in my lab, who have helped shape my thinking and clarify my ideas, and collaborated on several fascinating projects. In particular, Kevin Mickey, Steven Hansen, Arianna Yuan, Katherine Hermann, Andrew Nam, and Effie Li, as well as Ir\'an Rom\'an, Amarjot Singh, and Rav Suri. I'd also like to thank the students in the cognitive area more broadly, especially Erin Bennett, MH Tessler and Robert Hawkins, who have really shaped my thinking as a cognitive scientist. 

I have been greatly inspired by conversations with many researchers outside my area as well. In particular, Dan Birman, Tyler Bonnen, Akshay Jagadeesh, Dawn Finzi, Nathan Kong, Ari Beller, Corey Fernandez, Ian Eisenberg, Em Reit, and Dan Bear. My apologies to the many important people I have likely forgotten to mention. I would also like to extend a special thanks to Fred Bertsch and Doug Eck, with whom I interned at Magenta (Google Brain) in 2017, and Adam Santoro and Felix Hill, with whom I interned at DeepMind in 2019. The latter have been particularly influential on my thinking about some of the issues, such as compositionality and systematicity, that I discuss in this dissertation. 

%I'd like to thank Mike Frank, Ewart Thomas, Benoit Monin, and Tobi Gerstenberg for being excellent statistics and methods instructors, both when I was a student and when I was a TA. I'd also like to thank the students and researchers whom I taught in statistics or methods courses, or who consulted with me in my role as a statistics consultant --- I am grateful for all we have learned together while exploring your questions.

I've been inspired by many conversations and questions from the incredible Stanford faculty. I'd particularly like to thank my reading committee members Surya Ganguli and Noah Goodman. Surya led the way on one of my most fascinating projects during the PhD, exploring how learning dynamics could explain generalization, and offered crucial comments at many points along the way. Noah has asked me good questions and offered useful suggestions since before I was admitted, and many of the ideas in this dissertation were inspired by those conversations. 

Most importantly, Jay has been the most wonderful advisor that I could imagine. You have always known the right question to ask and the right experiment to run. I have learned so much from you about designing models, simplifying problems, designing experiments for humans and machines, asking good questions, and teaching and writing clearly. I am continually inspired by the breadth of your expertise and knowledge. Thank you for everything.

I'd also like to thank my mentors and inspirations from my past academic lives, especially Yury Kolomensky \& Bernd Sturmfels from my time at Berkeley; and Javier Armendariz \& Amanda Galante from my time at Johns Hopkins; as well as all the wonderful people I learned from at both institutions.

On a less academic note, I would like to express my gratitude to my family. My father provided me with very early inspiration about science and research and asking good questions. My mother was a programmer as well as homeschooling my sister and I, and taught me a great deal about computers and everything else. My sister Linnea has always been an inspiration, and I'm very proud that she is completing her bachelor's degree in psychology and biology. Finally, my grandfather Hugh, who taught me about physics and math from a very young age, and helped put me on the trajectory that led me here. 

The Stanford climbing community was one of the best parts of my time at Stanford. In particular, I am grateful to Dan Birman for teaching me so much about climbing; to Robert Hawkins, Allison Ong, Mona Rosenke, Corey Fernandez, Andrei Kamalov, Adrienne Mueller, Erin Bennett, Em Reit, and Daniel O'Leary for many adventures in Yosemite, Pinnacles, Castle Rock, and beyond; and to Andrew de Torres and Sonia Rackelman for allowing me to work as a routesetter at the Stanford Climbing Wall. 

I'd also like to thank my friends from outside of Stanford, in particular Kyle Cameron, Hurshal Patel, Ben Sklaroff, Andrew Gearhart, Anita Satish, and Aditi Narayanan, for bringing me some much-needed outside perspective during my immersion here.

The other students in my cohort, who have journeyed through this program with me, have been an invaluable source of support and laughter along the way. Jesse Reynolds, Lester Tong, and Pam Wang have been great friends. Special thanks Erin Bennett, Mona Rosenke, and Yochai Shavit. Erin, you have been a great friend, climbing partner, and provided amazing feedback on ideas, papers, and talks. Mona, you have been an amazing friend, climbing and adventure partner. Yochai, you have been a wonderful friend and discussion partner for research, statistics, or just philosophy, sometimes to the annoyance of the rest of the cohort. Thank you all. 

Finally, I'd like to thank Julie, who I'm so grateful to have met toward the end of my time at Stanford. You bring so much joy to my life; our adventures and laughter have kept me sane through a pandemic and writing this dissertation. Thank you.  

%%\begin{figure}[H]
%%\centering
%%\includegraphics[width=\textwidth]{figures/Research_Visualization.png}
%%\caption{A visualization of the different research themes and projects I have explored during my PhD (including collaborations). This dissertation contains only a focused subset of the work.}
%%\end{figure}

\afterpreface

\include{1-introduction/introduction}
\include{2-HoMM/homoiconic_meta_mapping_main}
\include{3-human-adaptation/human-adaptation}
\include{4-extending/extending}
\include{5-timescales/timescales}
\include{6-conclusions/conclusions}

%\include{outline}

\appendix

\include{2-HoMM/homoiconic_meta_mapping_appendix}
\include{3-human-adaptation/human-adaptation-supplement}
\include{4-extending/extending-supplement}

\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{apalike}
\bibliography{arrr}

\end{document}
