\chapter{Learning across different timescales} \label{chapter:timescales}

%% TODO: probably mention SWIL somewhere in these first few paragraphs
A large amount of recent machine learning research can be seen as studying interactions of learning across different time-scales. In particular, the field of meta-learning is based on the idea that a model can slowly learn over many experiences how to learn rapidly in a new experience. Similarly, the previous chapters show how slowly-accumulated knowledge about tasks and their relationships can allow zero-shot inferences about a new task. \par 

However, both of these approaches examine how slowly learned knowledge can improve rapid learning. Yet one of the core motivations of complementary learning systems theory was that rapidly learned experiences could be integrated into our prior knowledge. There is a lack of research investigating how what we learn over short time scales, for example in the inner loop of a meta-learning algorithm, can be integrated with our longer term knowledge. \par 

Integration of knowledge in machine learning is mostly stuided under the auspices of continual learning. Most work on continual learning investigates the setting where a model, starting from \emph{tabular rasa}, must learn a sequence of tasks without forgetting \citep{Ven2018, Atkinson2018}. This is motivated by the clear ability of humans and animals to learn multiple tasks without forgetting. However, humans are not starting from a blank slate when they achieve this. Indeed, \citep{Velez2017} show that systems can meta-learn to learn without forgetting. This raises the possibility that works that examine continual learning from a blank slate are misleading, because prior learning can be an important part of the solution. \par 

We have shown in the prior chapters that using knowledge of prior tasks can allow the system to perform well on a new, related task without any data. Here, we highlight the impacts of this perspective on different time-scales of learning. In particular, we show that this zero-shot inference improves learning on the new task, and the knowledge encoded in the system can allow this learning to occur without even the possibility of interference with prior tasks. \par  
