\chapter{Learning across different timescales} \label{chapter:timescales}

A large amount of recent machine learning research can be seen as studying interactions of learning across different time-scales. In particular, the field of meta-learning is based on the idea that a model can slowly learn over many experiences how to learn rapidly in a new experience. Similarly, the previous chapters show how slowly-accumulated knowledge about tasks and their relationships can allow zero-shot inferences about a new task. \par 

However, both of these approaches examine how slowly learned knowledge can improve rapid learning. Yet one of the core motivations of complementary learning systems theory was that rapidly learned experiences could be integrated into our prior knowledge. There is a lack of research investigating how what we learn over short time scales, for example in the inner loop of a meta-learning algorithm, can be integrated with our longer term knowledge. \par 

Integration of knowledge in machine learning is mostly studied under the framework of continual learning. Most work on continual learning investigates the setting where a model, starting from \emph{tabula rasa}, must learn a sequence of tasks without forgetting \citep{Ven2018, Atkinson2018}. This is motivated by the clear ability of humans and animals to learn multiple tasks without forgetting. However, humans are not starting from a blank slate when they achieve this. In \citet{McClelland2020} we show how prior knowledge affects what is easier or harder to learn, and show that prior knowledge must only be replayed to the extent that it is similar to (and thus interferes with) new knowledge. Furthermore, \citet{Velez2017} show that systems can meta-learn to learn without forgetting. Thus works that examine continual learning from a blank slate are misleading, because the structure of prior knowledge changes what is easy or difficult to learn, and prior learning can be an important part of the solution to catastrophic interference. \par 

We have shown in the prior chapters that using knowledge of prior tasks can allow the system to perform well on a new, related task without any data. Here, we highlight the impacts of this perspective on different time-scales of learning. In particular, we first show that this zero-shot inference improves learning on the new task, and second, that the knowledge encoded in the system can allow this learning to occur without even the possibility of interference with prior tasks. \par  

\section{Starting points for learning}

When humans begin a novel task, we often receive some instructions as a starting point. These instructions often describe the relationship of the novel task to prior experiences. This observation served as the motivation for the previous chapters, in which we showed that using meta-mapping could improve zero-shot task performance. However, as soon as we start performing a task, it is no longer zero-shot. That is, zero-shot adaptation is most important insofar as it serves as a useful starting point for later learning. In this chapter, one of our primary goals is to compare the zero-shot ``guess'' at a task representation to other initializations of the task representation, to evaluate whether it is a beneficial starting point for learning. \par 
In order to do this, we use an approach related to our prior work on one-shot learning of word embeddings \citep{Lampinen2018a}. In that work, we integrated a novel word into a pre-trained language model by simply optimizing its embedding(s) to improve the model's prediction of it in context. We showed that this allowed reasonable learning of new words, in fact average performance with this method was not statistically different than if the word had been included in the training corpus from the beginning. Thus in a model that has been pre-trained to understand the latent structure of a system (such as language), optimizing the representation of a single object can often be sufficient to construct a high-quality representation of the object, without needing to alter the other parameters of the model. This is perhaps the strongest case of learning without interference to sufficiently dissimilar representations, which we observed in a more graded fashion in \citet{McClelland2020}. By design, this type of learning cannot alter prior knowledge.\par
Analogously, in this chapter we explore optimizing the task embedding of a novel task once the system has begun to perform it. We will show that optimizing the task embedding alone will often allow near-perfect performance on a novel task, provided the model that is pre-trained on sufficiently many other tasks from the same distribution. This means that a new task can be learned without the possibility of any interference with prior tasks, because only task-specific parameters are altered. This provides a new perspective on continual learning (though see \citep{Oswald2020} for some related observations). Rather than thinking about how a system can minimize interference when learning a sequence of tasks from \emph{tabula rasa}, we should perhaps ask how prior knowledge can allow learning without any interference at all. \par 
The important observation from our perspective is that a zero-shot guess at a task embedding provides a useful starting point for this optimization. In particular, we compare to a variety of other initializations, and show that the zero-shot guess provides faster learning, and lower cumulative error. This latter measure can be thought of as analogous to the notion of \emph{regret} in reinforcement learning, a measure of how sub-optimally the algorithm performs while learning to behave optimally. Starting from the output of a meta-mapping results in learning faster, and making fewer mistakes along the way. This may be part of the solution to why humans are able to learn faster and more accurately than deep learning models on novel tasks. It also has the potential to lead to much safer exploration in a new setting, if risks are related to prior experience. \par   

\section{The polynomials domain}

%% TODO: add updated results and baseline in untrained model 
%% TODO: add significance tests for these, maybe make regret barplot one panel of figure also plotting time to e.g. log loss = -3
We begin by demonstrating these results in the simple polynomial regression domain that we considered in chapter \ref{chapter:zero_shot_via_homm}. We compare three different initializations. First, a small random initialization, as is often used for parameters of a deep model. Second, initializing each task with the embedding of a random trained task, in case the distributions of these is helpful for later learning. Finally, we compare to the guess embedding produced by meta-mapping from a prior task. \par 
As noted above, we make this comparison by optimizing the task embeddings for the new tasks. We do this without altering any other parameters in the model. It is not clear that this would be sufficient to produce good performance on a novel task, indeed we show below that in an untrained model it is not. However, if the model has sufficient experience of related tasks, it does suffice. \par 
We used a similar distribution of tasks to our polynomial results presented above, except that we ensured every evaluation task was a trained meta-mapping of a trained task, but where that task had not been used as an example for the meta-mapping during training. This eliminates the uncertainty introduced by having to learn a new task or mapping from examples, as well as applying the transformation, which allows for a more controlled comparison. To be precise, we trained the system on 60 base tasks plus the results of applying 20 meta-mappings to them. We additionally trained the system on 40 new base tasks, and held out the results of applying the 20 meta-mappings to them. It is these \(40 \times 20 = 800\) novel tasks that we optimized the embeddings for. \par 
In Fig. \ref{fig:timescales_polynomial_optimization_curves}, we show how the log-loss on the new tasks changes over epochs of learning on the new tasks. We compare the output of the meta-mapping to a variety of sensible initializations, as well as an untrained network. While all embedding initializations suffice to produce good performance eventually, the better starting point provided by a meta-mapping initialization results in both lower initial error, and faster learning early on, so much lower cumulative error. \par
To demonstate this last point, in Fig. \ref{fig:timescales_polynomial_optimization_regret} we plot the average cumulative ``regret'' on the novel tasks for the different initializations. That is, we plot the integrated error over the course of learning. Initializing with the meta-mapping output results in an order of magnitude less cumulative error compared to the next best approach (the centroid of the embeddings of all trained tasks). \par
Thus we conclude that, at least in this simple setting, the zero-shot initialization is advantageous in reducing the time to reach near-optimal performance, and the cumulative regret (errors made along the way). \par 
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{5-timescales/figures/polynomial_optimization_curves.png}
\caption{Learning curves when optimizing the task embedding for held-out polynomials from various starting points. The meta-mapping initialization provides a much better starting-point, and reaches near-optimal performance much faster. (Note that the y-axis is log-scale.)} \label{fig:timescales_polynomial_optimization_curves}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{5-timescales/figures/polynomial_optimization_cumulative_regret.png}
\caption{Cumulative loss when optimizing the task embedding for held-out polynomials from various starting points. The meta-mapping initialization results in an order of magnitude less cumulative error over the course of learning.} \label{fig:timescales_polynomial_optimization_regret}
\end{figure}

\section{Discussion}
In this chapter, we have shown that a zero-shot initialization reduces both the time to learn a novel task, and the mistakes made along the way. This is a useful observation for multiple reasons. \par 
First, the idea that deep learning is ``data-hungry'' is commonly used as a critique \citep[e.g.]{Lake2016, Marcus2018}. These critiques ignore the success of meta-learning, as we noted in a previous commentary \citep{Hansen2017}. However, as shown in this chapter, zero-shot adaptation provides another perspective on how learning in a novel task can be accelerated. While using deep learning from scratch can be data-hungry, starting from a good task representation output by a meta-mapping might allow deep learning to go on a diet. \par  
Second, starting from a good representation substantially reduces the errors made on the way to mastery. This can be an important goal in its own right, since making errors can be quite costly in settings like robotics, where errors may damage the robot or its surroundings, or even injure bystanders. This has spawned an increasing amount of recent work on ``safe exploration'' \citep[e.g.][]{Turchetta2016, Turchetta2019}. While we have not explored applications to safe exploration in detail, our results suggest that zero-shot adaptation to a novel task might allow for much safer exploration, by reducing the potential errors and allowing the system to explore more productively. This provides an exciting direction for future work. \par  
We have assumed in this chapter that task boundaries and identities are known, but of course there are other settings for continual learning \citep{Ven2018}. Another interesting future direction would be to combine our approach with approaches to these settings that try to infer the current task \citep[e.g][]{Nagabandi2019}. \par
