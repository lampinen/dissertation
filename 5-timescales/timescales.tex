\chapter{Learning across different timescales} \label{chapter:timescales}

A large swath of recent machine learning research can be seen as studying interactions of learning across different time-scales. In particular, meta-learning focuses on the idea that a model can slowly learn over many tasks how to learn rapidly in a new task. Similarly, the results of the HoMM approach in previous chapters show how slowly-accumulated knowledge about tasks and their relationships can allow zero-shot inferences about a new task. \par 
However, both of these approaches examine how slowly learned knowledge can improve rapid learning. Yet one of the core motivations of complementary learning systems theory was that rapidly learned experiences could be integrated into our prior knowledge. There is a lack of research investigating how what a model learns over short time scales, for example in the inner loop of a meta-learning algorithm, can be integrated with its longer term knowledge. In standard meta-learning approaches, the inner loop knowledge is discarded before the next episode, or only a small update is made to incorporate it. Yet when humans learn something new, we can remember it in detail, and use that knowledge in the future. \par 
In machine learning, Integration of knowledge is mostly studied under the framework of continual learning. Most work on continual learning investigates the setting where a model, starting from \emph{tabula rasa}, must learn a sequence of tasks without forgetting \citep{Ven2018, Atkinson2018}. This is motivated by the clear ability of humans and animals to learn multiple tasks without forgetting. However, humans are not starting from a blank slate when we achieve this. In \citet{McClelland2020}, we show how prior knowledge affects what is easier or harder to learn, and show that prior knowledge must be replayed only to the extent that it is similar to (and thus interferes with) new knowledge. Furthermore, \citet{Velez2017} show that systems can meta-learn to learn without forgetting. Thus works that examine continual learning from a blank slate are misleading, because the structure of prior knowledge changes what is easy or difficult to learn, and prior learning can be an important part of the solution to catastrophic interference. \par 
We have shown in the prior chapters that using knowledge of prior tasks can allow the system to perform well on a new, related task, without any data. Here, we highlight the impacts of this rapid adaptation over longer time-scales of learning. In particular, we demonstrate a technique by which this zero-shot inference can improve learning on the new task, and that the knowledge encoded in the system can allow this learning to occur without even the possibility of interference with prior tasks. \par  

\section{Starting points for learning}

When humans begin a novel task, we often receive some instructions as a starting point. These instructions often describe the relationship of the novel task to prior experiences. This observation served as the motivation for the previous chapters, in which we showed that using meta-mapping could improve zero-shot task performance. However, as soon as we start performing a task, it is no longer zero-shot. That is, zero-shot adaptation is most important insofar as it serves as a useful starting point for later learning. In this chapter, one of our primary goals is to compare the zero-shot ``guess'' at a task representation to other task representation initializations, to evaluate whether adaptation is a beneficial starting point for learning. \par 
In order to do this, I adopt an approach related to some of my prior work on one-shot learning of word embeddings \citep{Lampinen2018a}. In that work, we integrated a novel word into a pre-trained language model by simply optimizing its embedding(s) to improve the model's prediction of it in context. We did this without altering any network parameters other than this embedding. We showed that this allowed reasonable learning of new words, in fact average performance with this method was not statistically different than if the word had been included in the training corpus from the beginning (at least for rare words). Thus in a model that has been pre-trained to understand the latent structure of a system (such as language), optimizing the representation of a single object can be a sufficient way to construct a high-quality representation of the object, without needing to alter the other parameters of the model. This is perhaps the strongest case of learning without interference.\footnote{The structure of model's representations can also allow for some things to be learned without interference \citep{McClelland2020}.} By design, learning by optimizing the representation of a specific item cannot alter prior knowledge about other items.\par
Analogously, in this chapter we explore optimizing the task embedding of a novel task once the system has begun to perform that task. We will show that optimizing the task embedding alone will often allow near-perfect performance on a novel task, provided the model that is pre-trained on sufficiently many other tasks from the same distribution. This means that a new task can be learned without the possibility of interference with prior tasks, because only task-specific parameters are altered. This provides a new perspective on continual learning (though see \citep{Oswald2020} for some related observations). Rather than thinking about how a system can minimize interference when learning a sequence of tasks from \emph{tabula rasa}, we should perhaps ask how prior knowledge can allow learning without any interference at all. \par 
The important observation from our perspective is that a zero-shot guess at a task embedding provides a useful starting point for this optimization. In particular, we compare to a variety of other initializations, and show that the zero-shot guess provides faster learning, and lower cumulative error. This latter measure can be thought of as analogous to the notion of \emph{regret} in reinforcement learning, a measure of how sub-optimally the algorithm performs while learning to behave optimally. Starting from the output of a meta-mapping results in learning faster, and making fewer mistakes along the way. This may be part of the solution to why humans are able to learn faster and more accurately than deep learning models on novel tasks. It also has the potential to lead to much safer exploration in a new setting, as long as risks can be related to prior experience. \par   

\section{The polynomials domain}

%% TODO: add significance tests for these, maybe make regret barplot one panel of figure also plotting time to e.g. log loss = -3
We begin by demonstrating these results in the simple polynomial regression domain that we considered in Chapter \ref{chapter:zero_shot_via_homm}. We train the model as before, and then attempt to learn 800 held-out polynomials by optimizing a task representation for each one. We also compare to an untrained model, to show the importance of prior knowledge to the optimization procedure. \par
In the trained model, we compare four different initializations for the optimization. We first consider a small random initialization, as is often used for parameters of a deep model. We next consider initializing each task with the embedding of a random trained task, in case the distribution of these is helpful for later learning. We then compare to initializing with the centroid of all trained task representations, which is perhaps the most reasonable task-agnostic starting point \citep[c.f.][]{Lampinen2018a}. Finally, we compare to the estimate of the representation produced by meta-mapping from a prior task. \par 
As noted above, we make this comparison by optimizing the task embeddings for the new tasks. We do this without altering any other parameters in the model. It is not clear that this would be sufficient to produce good performance on a novel task, indeed we show below that in an untrained model it is not. However, if the model has sufficient experience on enough related tasks, this approach suffices. \par 
We used a similar distribution of tasks to our polynomial results presented in Chapter \ref{chapter:zero_shot_via_homm}, except that we did not include held-out meta-mappings. This eliminates the uncertainty introduced by having to learn a new mapping from examples, as well as applying the transformation, which allows for a more controlled comparison. To be precise, we trained the system on 60 base tasks plus the results of applying 20 meta-mappings to them. We additionally trained the system on 40 new base tasks, and held out the results of applying the 20 meta-mappings to them. We then optimized the task representations for these \(40 \times 20 = 800\) novel tasks, which the model never encountered during training. \par 

\subsection{Results}
In Fig. \ref{fig:timescales_polynomial_optimization_curves}, we show how the log-loss on the new tasks changes over epochs of learning on the new tasks (i.e. optimization of the task representations). We compare the output of the meta-mapping to a variety of sensible initializations, as well as an untrained network. We start by considering the untrained network --- while optimizing the task embedding alone is able to achieve performance well above chance, it is not able to capture the finer details of the task even after training for many epochs. This shows that prior knowledge of related tasks is key to learning with this method. \par
In the trained model, by contrast, all embedding initializations suffice to produce good performance eventually. However, the better starting point provided by a meta-mapping initialization results in both lower initial error, and faster learning early on, so much lower cumulative error. Other sensible initializations, such as the centroid of the representations of the trained tasks, perform much better than random initialization.\par
To demonstate this last point, in Fig. \ref{fig:timescales_polynomial_optimization_regret} we plot the average cumulative ``regret'' on the novel tasks for the different initializations. That is, we plot the integrated error over the course of learning. This measures how much loss the model must suffer in order to achieve perfect performance on the task. Starting from a meta-mapping results in almost an order of magnitude less cumulative error (mean \(= 24.58\), bootstrap 95\%-CI \([17.71, 32.08]\)) than the next best initialization (centroid of trained task representations, mean \(= 192.89\), bootstrap 95\%-CI \([151.98, 234.53]\)). That is, meta-mapping reduces the cumulative error by nearly an order of magnitude. \par
Thus we conclude that, at least in this simple setting, the zero-shot initialization is advantageous in reducing the time to reach near-optimal performance, and the cumulative regret (errors made along the way). \par 
\begin{figure}[p]
\centering
\includegraphics[width=0.8\textwidth]{5-timescales/figures/polynomial_optimization_curves.png}
\caption[Learning curves when optimizing the task embedding for new polynomials from various starting points.]{Learning curves when optimizing the task embedding for new polynomials from various starting points. The meta-mapping initialization provides a much better starting-point, and reaches near-optimal performance much faster. (Note that the y-axis is log-scale. Results are from 5 runs, individual runs are shown as light curves.)} \label{fig:timescales_polynomial_optimization_curves}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[width=0.8\textwidth]{5-timescales/figures/polynomial_optimization_cumulative_regret.png}
\caption[Cumulative loss when optimizing the task embedding for held-out polynomials from various starting points.]{Cumulative loss when optimizing the task embedding for held-out polynomials from various starting points. The meta-mapping initialization results in an order of magnitude less cumulative error over the course of learning.} \label{fig:timescales_polynomial_optimization_regret}
\end{figure}

\section{The visual concepts domain}

We next return to the visual concepts domain we considered in Chapter \ref{chapter:extending}. We trained the model with 16 meta-mappings (8 switch-color and 8 switch-shape), and with approximately 18 training tasks per meta-mapping, chosen as in the experiments where we varied numbers of training meta-mappings. However, we did not include any held-out meta-mappings. 

\subsection{Results}
In Fig. \ref{fig:timescales_polynomial_optimization_curves}, we show how average accuracy on the new tasks evolves over epochs of learning on the new tasks. Intriguingly, while small random initializations performed worse than centroids or arbitrary task embeddings in the polynomials domain, they perform substantially better in the visual concepts domain. However, meta-mapping output still has a substantial advantage, because it is achieving extremely high accuracy zero-shot. \par 
To explore this further, in Fig. \ref{fig:timescales_polynomial_optimization_regret} we again plot the average cumulative errors on the novel tasks for the different initializations. Initializing with a meta-mapping results in very low cumulative error (mean \(= 0.33\), bootstrap 95\%-CI \([0.10, 0.57]\)) compared to the next best approach (small random initalization, mean \(= 9.62\), bootstrap 95\%-CI \([6.63, 13.59]\)). This difference is significant in a mixed linear model (\(t(4) = 4.628\), \(p = 0.01\)). \par
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{5-timescales/figures/category_optimization_curves.png}
\caption[Learning curves when optimizing the task embedding for new visual concepts from various starting points.]{Learning curves when optimizing the task embedding for new visual concepts from various starting points. The meta-mapping initialization provides a much better starting-point, and reaches near-optimal performance faster. (Results are from 5 runs, individual runs are shown as light curves.)} \label{fig:timescales_category_optimization_curves}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{5-timescales/figures/category_optimization_cumulative_regret.png}
\caption[Cumulative errors when optimizing the task embedding for held-out visual concepts from various starting points.]{Cumulative errors when optimizing the task embedding for held-out visual concepts from various starting points. The meta-mapping initialization substantially reduces cumulative error over the course of learning. (Note that the vertical axis is log-scaled. Error bars are bootstrap 95\%-CIs across the 5 runs plotted in Fig. \ref{fig:timescales_category_optimization_curves}.)} \label{fig:timescales_category_optimization_regret}
\end{figure}

\section{Discussion}
In this chapter, we have shown that a zero-shot initialization reduces both the time to learn a novel task, and the mistakes made along the way. We have demonstrated this in both the polynomial regression and visual concept domains. This abilitis is important for multiple reasons. \par 
First, a frequent criticism of deep learning is the idea that it is ``data-hungry'' \citep[e.g.]{Lake2016, Marcus2018}. These critiques ignore the success of meta-learning, as we noted in a previous commentary \citep{Hansen2017}. However, as shown in this chapter, zero-shot adaptation provides another perspective on how learning in a novel task can be accelerated. While using deep learning from scratch can be data-hungry, starting from a good task representation output by a meta-mapping might allow deep learning to go on a diet. \par  
Second, starting from a good representation substantially reduces the errors made on the way to mastery. This can be an important goal in its own right, since making errors can be quite costly in settings like robotics, where errors may damage the robot or its surroundings, or even injure bystanders. This has spawned an increasing amount of recent work on ``safe exploration'' \citep[e.g.][]{Turchetta2016, Turchetta2019}. While we have not explored applications to safe exploration in detail, our results suggest that zero-shot adaptation to a novel task might allow for much safer exploration, by reducing the potential errors and allowing the system to explore more productively. This provides an exciting direction for future work. \par  
Finally, we have shown that this learning of novel tasks can occur without the possibility of interfering with prior knowledge. In fact, this learning actively requires that prior knowledge, as shown by the comparison to an untrained model. I suggest that this may better reflect human learning. I suggest that we exhibit \emph{less} interference in domains in which we have more experience. Continual learning schemes often exhibit the opposite pattern of effects. Thus, optimizing task representations in order to learn a new task without intereference, could be a useful way to model human learning. It may also offer a useful new approach to continual learning.\par 
However, the present experiments are limited. We have assumed in this chapter that task boundaries and identities are known, but of course there are other settings for continual learning \citep{Ven2018}. Another interesting future direction would be to combine our approach with approaches to these settings that try to infer the current task \citep[e.g][]{Nagabandi2019}. \par
At a higher level, we have assumed a sharp transition between fully-interleaved learning of many tasks, and learning of new tasks without interference by optimizing only the task embedding. However, it is possible that in some cases it would not be possible to optimize the task representation in order to produce optimal behavior on the new task (as it is in the untrained model). For example, this might occur when the model is introduced to a new task from a broader distribution than its training set, as perhaps is the case when a human first encounters a racket sport. To accomodate this, it would be worth exploring a mechanism that would optimize the task embedding to the extent possible, and afterward make minor adjustments to the weights of the model to resolve the remaining error. These later updates might benefit from other continual learning strategies \citep[e.g.][]{Kirckpatrick2016, Zenke2017}. Exploring these ideas further would be an interesting direction for future research.\par
