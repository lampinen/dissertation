\chapter{Comparing to human adaptation} \label{chapter:human}

In the previous chapter I proposed a framework for modeling human adaptation to new tasks, based on relationships between tasks. To evaluate the quality of the framework, it is necessary to compare its adaptation to human adaptation. It is worth stopping for a moment to consider how flexible humans actually are. We certainly experience short-term interference from switching tasks or goals \citep{Rogers1995}, or when we try to override a habitual response \citep{Stroop1935, MacLeod1991}. Over longer periods of time, our adaptation might be a response to learning in the new situation, rather than the type of zero-shot flexbiility that HoMM is intended to model. How flexibly are humans able to adapt to task changes in a short amount of time? \par
Unfortunately a complete evaluation of human flexibility is a large research program. In this chapter we present an evaluation of human adaptation in one setting, inspired by a motivating example used in the previous chapters. We implemented a simplified poker-like card game, and evaluated how well participants are able to switch to losing the game, after learning to win it. This is a challenging form of adaptation, which requires completely reversing a value function, and has been highlighted as a challenge for deep learning \citep{Lake2016}.\par
We compared human performance on the game to both a HoMM model and a task-description based language-generalization model. We trained both models on variations of 5 card-game tasks, including losing variations of 4 of these, but crucially holding out all losing variations of the task that the human subjects played. We then evaluated the ability of the models to adapt to the losing variation zero-shot, just as the humans did. In the HoMM model, this adaptation was based on applying a ``switch-to-losing'' meta-mapping to the learned variation of the game, whereas in the langauge-generalization model it was based on a novel instruction to lose the game that was systematically related to the training instructions for losing other games.\par 

\section{Experimental design}
We tried to design the game so that it would be easy for humans to learn, without relying on their prior knowledge of card games. The game we had participants play was a simplified variation of poker. They were dealt hands which consisted of two cards, each with a number (rank) between 1 and 4, and a color (suit) of red or black. The participants played against a computer opponent that was dealt a similar hand. The hands were ranked such that straight flushes (adjacent cards in the same suit) beat adjacent cards in a different suit, which beat non-adjacent cards (including pairs). Ties were broken by the highest card, or by suit if both cards were tied. \par
\begin{figure}
\centering
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=\textwidth]{3-human-adaptation/figures/pre_bet_screenshot.png}
\caption{Before betting.}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=\textwidth]{3-human-adaptation/figures/post_bet_screenshot.png}
\caption{Feedback.} \label{fig:human_betting_trial_feedback}
\end{subfigure}%
\caption{The card game experiment trials, as seen by participants.} \label{fig:human_betting_trial}
\end{figure}
On each trial, participants were dealt a hand and asked to make a bet of 0, 5, or 10 cents (see fig. \ref{fig:human_betting_trial}). If their hand beat the opponent's hand, they won the bet amount. If their hand lost, they lost it. If the hands were tied, they neither won nor lost money. \par
The experiment had several phases. First, participants were instructed in the rules and payment scheme for the experiment. Next, they were instructed on the rules of the game. After this, they were tested with four hand-comparison trials intended to probe their understanding of each of the rules of the game. If they failed more than one of these trials, they were not allowed to continue with the experiment. \par
Following this understanding check, subjects played a block of 32 hands (sampled to have a diversity of expected values), where they saw the results of their play, as in (fig. \ref{fig:human_betting_trial_feedback}). After this block, they played a similar block of 24 trials where they did not see the results of their play. The results were replaced with a brief grayed-out screen, and participants were payed the net expected value of their actions over the block (rounded to the nearest 10). This provides an evaluation phase with relatively less potential for learning. \par
Finally, participants were told that we wanted them to try to lose for the remaining trials, and that ``for the remainder of the experiment, if you bet and lose, you’ll gain the amount you bet, and if you bet and win, you’ll lose the amount you bet.''. They were then given an attention check to evaluate whether they had understood this instruction. Subjects who failed this attention check were excluded from the analysis. They then played another block of 24 trials where they were rewarded for losing instead of winning (i.e. the expected returns were reversed). As in the previous block, they did not see the results of their actions. They were finally asked a few demographic questions. See Appendix \ref{appendix:human} for detailed instructions \& methods.\par
Our main target comparison was performance in the two blocks without feedback -- were participants able to switch their behavior to lose at the game as well as they won at it?\par

\subsection{HoMM model}

We wanted to evaluate the HoMM model's ability to switch to losing based on a ``try-to-lose'' meta-mapping. To do so, we need other games (with winning and losing variations) to use as training examples of the meta-mapping. We therefore created 4 other card games, based on simplifications of other card games, like blackjack, as well as games where having matching cards is optimal. We created variations of these games that switched rather suit or rank was the most important attribute, and which suit was most valuable. We created losing variations of each of these. In total, there are 5 card games \(\times\) 3 binary attributes \(= 40\) basic tasks. See Appendix \ref{app:human:model_details} for full descriptions of all the games and variations.\par

We trained the model on 36 of these tasks, but held out the losing variations of all versions of the task that the human subjects played. We trained the model on three meta-mappings, corresponding to toggling the three binary attributes each game could have. We also trained the model to classify the basic game types (one vs. all) and each attributes. We then evaluated the ability of the model to adapt to the losing variation zero-shot, just as the humans did. This adaptation was based on instantiating the ``switch-to-losing'' meta-mapping with the 32 available examples, and applying it to the 4 other training tasks. We used the three other held-out variations of the game as a validation set to pick an optimal-stopping point for evaluating the model on the remaining hold-out (the game that the humans played). \par

In order to perform the basic tasks with the HoMM model, we had to make one minor alteration. Because in this task the model (or participant) only receives feedback on the action taken, this is not a simple regression problem with inputs and targets. It is more similar to a reinforcement learning setting, where the model takes actions and may or may not receive rewards in response (although there is no temporal component in our simplified tasks). We thus altered the way a task representation is constructed from examples in the model. Instead of using a dataset of (input, target) tuples, we used a dataset of (input, (action, reward)) tuples, where the action and reward were processed together to form a single embedding. The model was then trained with a masked loss, such that it only updated its predictions for actions it actually took, rather than other possibilities. (See Appendix \ref{app:human:model_details} for further details of the training.) \par

\subsection{Language model}

We also compared to a language-generalization baseline. As a reminder, this is an alternative approach to zero-shot task performance, where the model simply receives a natural language description of the task. The task descriptions were sequences of the form:\\\verb|[``game'', <game_type>, ``losers'', <losers-value>, [other attributes]]| encoded by a 2-layer LSTM network. The language model was trained on the same set of tasks as the HoMM model, and was optimally stopped by using the same validation set. (Again, see Appendix \ref{app:human:model_details} for further details.)


\section{Human performance}
First, how well were participants able to learn the game? Participants performance is reasonable, although they are far from optimal, and there is substantial individual variability (fig. \ref{fig:human_cards_basic_results}). In particular, participants are sometimes making intermediate bets, which an optimal agent would never do. Furthermore, all subjects thresholds (where they cross a betting probability of 0.5) are variable, some subjects are substantially over- or under-conservative. However, performance is actually more optimal than some basic statistics might suggest, see Appendix \ref{appendix:human:suboptimality}. \par 
After being asked to lose, participants also performed above chance, but far from optimally (fig. \ref{fig:human_cards_losing_results}). So how well were they able to adapt? 

\begin{figure}
\centering
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{3-human-adaptation/figures/testing_basic_bet_densities.png}
\caption{Bet density by expected value.}
\end{subfigure}%
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{3-human-adaptation/figures/testing_basic_subject_response_fits.png}
\caption{Probability of non-zero bet by expected value. The red dashed line is the optimal threshold, the grey curves are the individual subject fits.}
\end{subfigure}%
\caption[Human performance on the card game task, basic game evaluation block.]{Human performance on the card game task, basic game evaluation block. While participants are performing well above chance, they are far from optimal. They make intermediate value bets, and do not switch optimally between betting and not betting. There is also substantial inter-subject variability.} \label{fig:human_cards_basic_results}
\end{figure}
\begin{figure}
\centering
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{3-human-adaptation/figures/testing_losing_bet_densities.png}
\caption{Bet density by expected value.}
\end{subfigure}%
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{3-human-adaptation/figures/testing_losing_subject_response_fits.png}
\caption{Probability of non-zero bet by expected value. The red dashed line is the optimal threshold, the grey curves are the individual subject fits.}
\end{subfigure}%
\caption[Human performance on the card game task, losing evaluation block.]{Human performance on the card game task, losing evaluation block. There is again substantial inter-subject variability.} \label{fig:human_cards_losing_results}
\end{figure}


\section{Adaptation in humans and HoMM}
Human adaptation was quite good, in the sense that on average performance was almost identically preserved (fig. \ref{fig:human_cards_adaptation_results}). However, this average masks substantial individual variability. Because the figure plots \emph{expected} earnings, and hand values were closely matched, almost all the change from one point to the next is due to either stochasticity in the participants policies, or non-standard adaptation. (We cannot call this non-standard adaptation sub-optimal, since it sometimes results in \emph{improved} performance on the adapted task.) It would be interesting to explore in detail what factors underlie this variability, but here we focus on the comparison to the HoMM model and language-generalization model. \par  
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{3-human-adaptation/figures/human_adaptation.png}
\caption[Human adaptation in the cards experiment.]{Human adaptation in the cards experiment: the change in performance from the winning evaluation block to the losing evaluation block. Larger lines are the average, smaller are individual subjects. While there is substantial individual variability, average performance is preserved.} \label{fig:human_cards_adaptation_results}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{3-human-adaptation/figures/human_adaptation_vs_HoMM.png}
\caption{Comparing human adaptation to the HoMM and language models.}\label{fig:human_cards_homm_results}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{3-human-adaptation/figures/change_scores.png}
\caption[Comparing adaptation change scores between the humans and models.]{Comparing adaptation change scores (as \% of score on prior task) between humans and the models. While the HoMM model is slightly sub-optimal, it is not significantly different than humans. By contrast, the language-based model is near chance on the held-out tasks.} \label{fig:human_cards_homm_change_scores}
\end{figure}
In Fig. \ref{fig:human_cards_homm_results} we plot the adaptation of the models against the human results. Both models are performing near-optimally on the training tasks, but of course they have much more experience with these tasks than the humans do. However, the adaptation of the models is substantially different. Both models are worse at the losing versions of the tasks, but the HoMM model is only slightly worse, while the language-based model is degrading to near chance performance. \par
In order to make a more fair comparison between humans and the models, in Fig. \ref{fig:human_cards_homm_change_scores} we plot performance on the losing task, as a percentage of performance on the winning tasks. Note, however, that this metric is biased against the models, because of the ceiling effect -- their scores on the adapted tasks cannot be higher than their optimal scores on the original tasks, whereas the humans are only optimal on average because many of them are performing substantially better on the adapted tasks, for unclear reasons. While HoMM is adapting slightly sub-optimally, the difference between it and the human subjects is not statistically significant, either by a \(t\)-test (\(t(18.54) = -1.80\), \(p = 0.09\)), or a permutation test (\(p > 0.05\)). However, HoMM is significantly better than the language-based approach, either by a   \(t\)-test (\(t(5.37) = 9.33\), \(p < 0.001\)), or a permutation test (\(p < 0.005\)). \par

\section{Discussion}

In this chapter, I have compared the ability of humans, the HoMM model, and a language baseline to adapt to losing a simple card game. The HoMM model was trained only on variations of 4 simple card games, far fewer than the humans experienced. Despite this, the HoMM model was able to adapt well. The human subjects also seemed to be adapting well on average, although there was substantial variability between subjects. The model appears to be adapting slightly worse than the subjects (although the difference is not statistically significant), but without giving the model an equivalent amount of experience with games as varied as those the humans have played, it's difficult to draw a strong conlusion from these results. \par
Instead, I interpret the comparison to humans and the language model as suggesting that mechanisms like meta-mapping may offer a useful model of human adaptation. In particular, the alternative approach of generalizing based on language did not adapt nearly as well, although it suffers from the same objection that it does not have the degree of language experience that humans have. Ultimately, multiple mechanisms likely play a role in explaining adaptation in different people. Meta-mapping may be one important piece of the puzzle, but I do not want to imply that it is the only one. \par 
Furthermore, the limited number of tasks each model experienced, as well as the limited complexity of the tasks themselves, also limit the breadth of conclusions that can be drawn from the experiments in this chapter. In the next chapter, I apply the model to more complex tasks in RL and vision, that provide more challenging tests of adaptation, and support the idea that meta-mapping could be a broadly useful concept for deep learning models of cognition. \par  
