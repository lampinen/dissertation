\chapter{Supplemental material for Chapter \getrefnumber{chapter:extending}} \label{appendix:extending}

\section{RL tasks} \label{app:extending_grids_methods}

All implementation and analysis code can be found at \url{https://github.com/lampinen/HoMM_grids}.\par


\subsection{Correlation in generalization across tasks}
In Fig. \ref{fig:app_extending:RL_correlation_by_run}, we show the correlation in performance on the pick-up and push-off generalization tasks within each run (at different time points in learning). Points are only included if train performance is above the threshold used for selection --- \(3.8\) for the HoMM model, \(3.5\) for the language model. Stricter thresholds for the language model result in weaker (sometimes negative) correlations (not shown). \par 
\begin{figure}
\centering
\includegraphics[width=\textwidth]{4-extending/figures/grids_adaptation_correlation_loose_by_run.png}
\caption[Correlation of performance on the RL tasks, by run.]{Correlation of performance on the two RL tasks, broken down by run. The correlation is higher in the HoMM model, both within and across runs.} \label{fig:app_extending:RL_correlation_by_run}
\end{figure}


\subsection{HyperNetwork-based architecture}
In Fig. \ref{supp_fig:extending:RL:arch_cond_vs_hyper}, we show that the HyperNetwork-based architecture outperforms a task-network-concatenating architecture at meta-mapping on the RL tasks, as in the polynomials domain. \par 

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{4-extending/figures/grids_hyper_vs_tcnh.png}
\caption[In the RL domain, the HyperNetwork-based architecture performs better on meta-mappings than an architecture that simply concatenates a task representation to the input.]{In the RL domain, the HyperNetwork-based architecture performs better on meta-mappings than an architecture that simply concatenates a task representation to the input before passing it through a fixed MLP. We showed similar (though less dramatic) results for the polynomials domain in Supp. Fig. \ref{supp_fig:HoMM_arch_cond_vs_hyper}}\label{supp_fig:extending:RL:arch_cond_vs_hyper}
\end{figure}


\section{Categorization tasks} \label{app:extending_categorization_methods}
All implementation and analysis code can be found at \url{https://github.com/lampinen/categorization_HoMM}.\par
In Fig. \ref{fig:app_extending_cat_stims} we show all shapes (triangle, square, plus, circle, tee, inverseplus, emptysquare, emptytriangle), colors (blue, pink, purple, yellow, ocean, green, cyan, red), and sizes (16, 24, and 32 pixels) that we used in our experiments. All stimuli were rendered at random positions within a \(50 \times 50\) image (constrained so that the full shape remained within the frame), and at random angles within \(\pm20^{\circ}\) of their canonical orientation.\par

\begin{figure}[!htb]
\centering
\begin{subfigure}{0.24\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/categorization_stimuli/16_blue_triangle.png}
\end{subfigure}%
\begin{subfigure}{0.24\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/categorization_stimuli/24_pink_square.png}
\end{subfigure}%
\begin{subfigure}{0.24\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/categorization_stimuli/32_purple_plus.png}
\end{subfigure}%
\begin{subfigure}{0.24\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/categorization_stimuli/16_cyan_emptysquare.png}
\end{subfigure}\\
\begin{subfigure}{0.24\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/categorization_stimuli/24_ocean_tee.png}
\end{subfigure}%
\begin{subfigure}{0.24\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/categorization_stimuli/32_green_inverseplus.png}
\end{subfigure}%
\begin{subfigure}{0.24\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/categorization_stimuli/16_yellow_circle.png}
\end{subfigure}%
\begin{subfigure}{0.24\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/categorization_stimuli/24_red_emptytriangle.png}
\end{subfigure}%
\caption{Sample stimuli for categorization tasks, showing all shapes, colors, and sizes.} \label{fig:app_extending_cat_stims}
\end{figure}

\subsection{Language model architecture} \label{app:extending_categorization_lang_arch}

In the categorization experiments, we used a different task network architecture for the meta-mapping based architectures than for the language generalization architectures. Here, we justify that choice by showing that the model architecture we used for the meta-mapping approach results in worse language generalization, in Fig. \ref{fig:app_extending_cat_lang_arch}. In particular, the linear task network resulted in worse generalization performance (mean \(= 0.85\), bootstrap 95\%-CI [0.82, 0.88]) than the deep nonlinear task network (mean \(= 0.92\), bootstrap 95\%-CI [0.89, 0.94]). This difference was significant under a linear mixed-model (\(t(4) = 3.615\), \(p = 0.02\)), and under a permutation test. \par 

\begin{figure}
\includegraphics[width=0.8\textwidth]{4-extending/figures/language_model_architecture_justification.png}
\caption[Comparing language generalization with linear vs. deep-nonlinear task-networks.]{Comparing language generalization with a linear task network to a deep, nonlinear architecture. Although the linear task network worked best for the meta-mapping approaches (not shown), the nonlinear task network generalized better to new language instructions.} \label{fig:app_extending_cat_lang_arch}
\end{figure}

\subsection{More detailed result visualizations}
In Fig. \ref{fig:app_extending:concepts_generalization_density} we show the zero-shot generalization accuracy of the models across runs, at different training set sizes. At moderate sample sizes, the HoMM model results in a sharper peak at perfect accuracy --- i.e. more qualitatively ``getting it'' or ``not getting it.'' \par 
In Fig. \ref{fig:app_extending:concepts_all_runs} we show learning curves for meta-mapping performance across all runs. Performance is highly variable at small training-set sizes, especially on held-out meta-mappings, but becomes increasingly systematic as training set size increases.\par

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{4-extending/figures/concepts_adaptation_generalization_density.png}
\caption[Visual concept generalization densities.]{In the visual concepts domain, meta-mapping results in more qualitative ``getting it'' or ``not getting it'' behavior, in the middle ranges of dataset size. Here we plot the density of the zero-shot evaluation accuracy across runs for the HoMM model and language generalization. The HoMM model exhibits sharper peaking at one at moderate sample sizes, whereas the language generalization is more smeared out --- i.e. the HoMM model is either systematically getting everything correct or is making a large number of mistakes, whereas the language generalization is more stochastic. This qualitative, systematic difference in performance that HoMM exhibits is more like what would be expected from human cognition.}\label{fig:app_extending:concepts_generalization_density}
\end{figure}

\begin{figure}[p]
\centering
\begin{subfigure}{\textwidth}
\centering
\includegraphics[width=0.88\textwidth]{4-extending/figures/concepts_all_runs_train.png}
\caption{Trained meta-mappings.}\label{fig:app_extending:concepts_all_runs:train}
\end{subfigure}\\
\begin{subfigure}{\textwidth}
\centering
\includegraphics[width=0.88\textwidth]{4-extending/figures/concepts_all_runs_eval.png}
\caption{Held-out meta-mappings.}\label{fig:app_extending:concepts_all_runs:eval}
\end{subfigure}
\caption[Visual concept learning curves by run.]{Meta-mapping learning curves in the visual concepts domain broken down by number of training meta-mappings (rows), and by run (columns). The green lines are performance when the transformed task was encountered during training, the pink lines are performance on transformed tasks that were never encountered during training. Panel (\subref{fig:app_extending:concepts_all_runs:train}) shows the results for trained meta-mappings, and panel (\subref{fig:app_extending:concepts_all_runs:eval}) shows the results for held-out meta-mappings. With more training meta-mappings, HoMM both generalizes better when applying the trained meta-mappings to held-out examples (\subref{fig:app_extending:concepts_all_runs:train}), and when applying held-out meta-mappings (\subref{fig:app_extending:concepts_all_runs:eval}). However, even with smaller sample sizes, HoMM is achieving perfect generalization on the trained meta-mappings on many runs.} \label{fig:app_extending:concepts_all_runs}
\end{figure}

