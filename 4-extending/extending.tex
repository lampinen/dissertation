\chapter{Extending meta-mapping to more complex tasks} \label{chapter:extending}
In the previous chapters we have demonstrated the succes of meta-mapping in two simple domains. While these have allowed us to demonstrate the efficacy of the approach relative to other baselines, and compare its adaptation to that of humans, there are several reasons to extend beyond these to more complex tasks. \par 
First, the approach relies on several pieces which it is not obvious would scale to more complex settings, such as representing an entire task with a single vector, parameterizing the task network via a HyperNetwork conditioned on this vector, and learning meta-mappings from relatively few task examples. If any of these fails to extend to more complex settings, it could limit the applications of our approach. \par
Second, there are limitations to toy experiments. While toy experiments can provide carefully controlled demonstrations of an idea, we have shown in other work that more systematic generalization can emerge when agents are placed in more realistic settings \citep{Hill2019a}. This may impact both the meta-mapping approach and the language baseline, so it is important to evaluate the effects of richer environments on both. This will help inform us as to whether our approach will be useful in more complex settings. (Unfortunately, creating truly realistic environments and training agents in them requires complex implementations and substantial computational resources. Thus, in this chapter we demonstrate our results in environments of moderate complexity, and leave the extension to even richer environments to future work.)\par 
To address these motivations, in this chapter we present experiments on extending our ideas to two important settings: reinforcement learning and classification from raw pixel inputs. These settings are important both because they are dominant paradigms for applying deep learning, and because they have deep connections to cognitive modelling \citep[e.g.][]{Yamins2014, Kriegeskorte2015, Momennejad2017}. \par

\section{Reinforcement learning}

Reinforcement learning is an interesting (and challenging) application for meta-mapping for several reasons. First, reinforcement learning has deep roots in neuroscience, and appears to relate to some computations in the brain \citep{Sutton2017, Niv2009, Odoherty2003, Dabney2020}. Second, reinforcement learning has achieved impressive human-level performance on complex tasks such as Atari games \citep{Mnih2015}, Go \citep{Silver2016, Silver2017}, and complex video games like Dota 2 \citep{OpenAI2019}, and Starcraft II \citep{Vinyals2019}. This motivates it as an important place to explore more human-like intelligence. Third, there has been a rich vein of research on adaptation in reinforcement learning from language-conditioned models \citep{Hermann2017} to the observation that model-based methods or successor representations can allow for adaptation to environment changes \citep{Daw2014, Momennejad2017}. However, these latter methods assume that a new reward function is given, which requires a substantial portion of the adaptation problem already be solved. Thus, there is substantial room to ask whether meta-mapping can provide good performance in RL tasks, and good motivation for a language baseline. \par 
\subsection{Tasks}
To address these challenges we created a set of RL tasks based on raw visual input with a relatively simple action space. Refer to Fig. \ref{fig:extending_grid_task_views} throughout this section for images of the visual input the agent would receive. The tasks take place in a \(6 \times 6\) room with an additional impassable barrier of \(1\) square on each side. The squares are upsampled at a resolution of 7 pixels per square to provide the raw visual input to the agent. In addition, the agent receives egocentric input, since we have shown in other work that this is beneficial to generalization \citep{Hill2019a}. That is, the agent's view is always centered on itself, and the world moves around it as the agent moves. The agent has four actions available to it, corresponding to moving in the four cardinal directions. If it makes an invalid action, such as trying to move past the edge of the board, the state does not change. The view window is sufficiently larger than the world so that the agent can see the entire world, no matter where it is.\par 
The tasks the agent must perform relate to objects which are placed in this space. The objects can appear in 10 different colors. In any given task, the world will have two colors of objects in it. Each color of objects only appears with one other color, so there are in total 5 possible colors that can appear. In any given task, one of the present colors is ``good,'' and the other is ``bad.'' On some tasks, the good and bad colors in a pair are switched.\par
There are two types of tasks, a ``pick-up'' task, and a ``push-off'' task. The tasks are visually distinguishable to the agent, because the shape of the objects used for them are different. In the pick-up task, the agent is rewarded for picking up the good-colored objects by moving to their grid locations, and is negatively rewarded for picking up the bad-colored objects. In the push-off task, the agent is able to push an adjacent object by moving toward it, if there is no other object behind it. The agent is rewarded for pushing the good-colored objects off the edges of the board, and negatively rewarded for pushing the bad colored objects off. The two types of tasks (``pick-up'' and ``push-off'') are visually distinguishable to the agent, because the shape of the objects used for them are different. However, which color is good or bad is not visually discernable, and must be inferred from the example (state, (action, reward)) tuples used to construct the task representation.\par 
There are in total 2 task types \(\times\) 5 color pairs \(\times\) binary whether the good and bad colors are switched \(= 20\) tasks in total. We trained the system on 18 of these, holding out the switched color combination of (``red'', ``blue'') in both task types. That is, during training the agent always is positively rewarded for interacting with red objects and negative rewarded for interacting with blue objects across both task types. We train the system on the ``switch-good-and-bad-colors'' meta-mapping using the remaining three color pairs in the two task types, and then evaluate its ability to perform the held-out tasks zero-shot based on this mapping. Note that this is a quite difficult challenge for a model-free system, since any rewards it receives during training on similar-colored tasks are the opposite of these evaluation rewards.\par
\begin{figure}[!htb]
\centering
\begin{tikzpicture}[auto]%, scale=0.8, every node/.style={scale=0.8}]
\draw[boundingbox, draw=gray, fill=white] (-9.3, 2.65) rectangle (0.3, -2.3);
\node[gray, align=center] at (-4.6, 2.35) {Pick-up task};
\node at (-7, 0) (pu1) {\includegraphics[width=4cm]{4-extending/figures/grid_tasks/pick_up_3.png}};
\node at (-2, 0) (pu2) {\includegraphics[width=4cm]{4-extending/figures/grid_tasks/pick_up_4.png}};
\node at (-4.5, 0) {\huge \(\bm \downarrow\)};
%\draw[arrow, very thick, gray] (pu1) -- node[black] {\huge``\(\bm \downarrow\)''} (pu2);

\begin{scope}[shift={(0, -5.5)}]
\draw[boundingbox, draw=gray, fill=white] (-9.3, 2.65) rectangle (0.3, -2.3);
\node[gray, align=center] at (-4.6, 2.35) {Push-off task};
\node at (-7, 0) (pu1) {\includegraphics[width=4cm]{4-extending/figures/grid_tasks/pusher_9.png}};
\node at (-2, 0) (pu2) {\includegraphics[width=4cm]{4-extending/figures/grid_tasks/pusher_10.png}};
\node at (-4.5, 0) {\huge \(\bm \leftarrow\)};
%\draw[arrow, very thick, gray] (pu1) -- node[black] {\huge``\(\bm \downarrow\)''} (pu2);
\end{scope}
\end{tikzpicture}
\caption[Illustrative state transitions from the RL grid experiments.]{Illustrative state, action, state transitions from the RL grid experiments. In the pick-up task example (top), the agent moves downward and picks up the green object. In the push-off task example (bottom), the agent moves left and pushes the red object. Views are the visual input the agent would receive. The agent is the white triangle, note that it is always at the center of the view, because of the egocentric perspective.} \label{fig:extending_grid_task_views}
\end{figure}

\subsection{Model}
%% TODO: detailed parameters
To accomodate this setting, we essentially combined the DQN architecture \citep{Mnih2015} with our previous approaches. That is, the input to the model was raw pixels, which were passed through several convolutional layers to produce state embeddings. This visual processing was shared across all tasks. As in the card game tasks discussed in previous chapters, we used (state, action, reward) tuples as input to the meta-network \(\mathcal{M}\). More precisely, we processed actions and rewards through several layers to produce an ``action-outcome'' embedding, and then concatenated that to the visual embedding as input to \(\mathcal{M}\). As before, \(\mathcal{M}\) produced a task embedding, which was passed through a HyperNetwork \(\mathcal{H}\) to parameterize the task network \(F\). The task network took state embeddings output by the convolutional network, and processed these to produce to produce output embeddings, which were linearly decoded to Q-values for the different actions. When meta-mapping, the input task embeddings replaced the input state embeddings to \(\mathcal{M}\) and \(F\), and the output task embeddings replaced the action-outcome inputs to \(\mathcal{M}\). See Appendix \ref{app:extending_grids_methods} for further details of the architecture and training. \par 
Perhaps because of the difficulty of the generalization problem in this setting, we found that two additional model modifications were useful. First, rather than constructing the task embedding completely from scratch each time, we kept persistent task embeddings cached, and used a random convex-combination of the output of \(\mathcal{M}\) and the cached embedding to perform the task. We added an additional \(\ell_2\) loss between the cached and transient embedding that attempted to match each to the other. Having partially persistent embeddings made it easier for the system to overcome the initial confusion about the fact that objects were sometimes positively rewarding and sometimes negatively rewarding, and made it easier for it to discover the overall structure of the tasks. \par
Second, we found that incorporating weight normalization \citep{Salimans2016} in the task network increased the stability of the training process. In the simpler settings of the cards tasks, neither of these modifications was necessary. It is likely that the temporally extended nature of these tasks makes the interference between the conflicting tasks worse. However, it is possible that with appropriate hyperparameters, and enough training and time, the model could overcome this and learn without persistent representations or weight normalization in this setting as well. \par 

\subsection{Results}

\begin{figure}
\centering
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/grids_adaptation_results.png}
\caption{Adaptation performance at best-validation epochs.}\label{fig:HoMM_RL_results:accuracy}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/grids_adaptation_correlation_loose.png}
\caption{Correlation of performance on the two tasks.}\label{fig:HoMM_RL_results:correlation}
\end{subfigure}%

\caption[Results on the RL tasks.]{Results on the RL tasks. (\subref{fig:HoMM_RL_results:accuracy}) Performance on the held-out tasks via a meta-mapping and via language generalization. Despite the challenging nature of the adaptation (as evidenced by the language-generalization performance), HoMM is performing quite well. (\subref{fig:HoMM_RL_results:correlation}) Correlation of performance on the two hold-out tasks, across runs and time points where performance on the training tasks is high. The correlation is much stronger in the HoMM model than in the language-generalization model, that is, the HoMM model is behaving more systematically in the sense that it is generalizing similarly on both tasks. (Results from 5 runs, error-bars in panel \subref{fig:HoMM_RL_results:accuracy} are bootstrap 95\%-CIs across runs.)} \label{fig:HoMM_RL_results}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{4-extending/figures/grids_language_learning_curves.png}
\caption[Language generalization model learning curves on the RL tasks.]{Average performance of the language generalization model over training on the RL tasks. The model exhibits intriguing but transient generalization early in learning, before it has understood the full structure of the tasks (especially the more difficult and sequential push-off task). However, this quickly decays to below-chance generalization as the model masters the training tasks. This early generalization is not included in the main results since the train accuracy at this time is below the threshold of having adequately learned the tasks.} \label{fig:HoMM_RL_language_learning_curves}
\end{figure}

In Fig. \ref{fig:HoMM_RL_results} we show the results. We optimally stop the model for each task by requiring the training accuracy to be above a threshold (95\% for the HoMM model, but 87.5\% for the language model, because stricter thresholds resulted in worse language results), and using the other task as a validation set --- that is, we evaluate the model on one task when the model performs well on the other task as well as the training tasks. The HoMM model substantially outperforms the language model, achieving 88.0\% of optimal rewards (mean, bootstrap 95\%-CI [75.0-99.0]) on the held-out pick-up task, and 71.7\% (mean, bootstrap 95\%-CI [42.0, 94.6]) on the held-out push-off task. By contrast, the language model is showing very little adaptation, with respective performance of -92.8\% (mean, bootstrap 95\%-CI [-96.3, -88.4]) and -79.7\% (mean, bootstrap 95\%-CI [-92.8, -59.1]) on the two tasks. This difference between the models is significant in a mixed linear regression controlling for task type and a random effect of run (\(t (20.6) = -19.515\), \(p < 1\cdot10^{-14}\)).\footnote{Degrees of freedom calculated by the Satterthwaite approximation.}\par

The HoMM model also exhibits significantly stronger correlations between its performance on the two tasks, both within runs at different time-points and across runs (Fig. \ref{fig:HoMM_RL_results:correlation}, Supp. Fig. \ref{fig:app_extending:RL_correlation_by_run}). Specifically, the HoMM model has a correlation of \(r=0.82\) between performance on the two tasks, while the language model only has a correlation \(r=0.10\), and this difference is significant in a mixed linear model predicting push-off performance from pick-up performance, controlling for task type, epoch, and the random effect of run (main effect of HoMM \(t(451.3) = 4.76\), \(p < 1\cdot 10^{-5}\), interaction of HoMM with pick-up performance \(t(452.0) = 3.43\), \(p < 1 \cdot 10^{-3}\)). At a surface level, this means that it is easier to select a good stopping point for the HoMM model --- even though the language model is achieving less bad (though still at or below chance) performance at some points in some runs, the lack of correlation between the results on the different tasks means there is no fair way to stop training the model at that point. More fundamentally, this suggests that the meta-mapping approach is exhibiting more systematic generalization, in the sense that it is either generalizing well on both tasks, or not generalizing well on both. Again, this is more like what would be expected from human cognition.

Intriguingly, the language model does transiently exhibit slightly positive generalization very early in learning (Fig. \ref{fig:HoMM_RL_language_learning_curves}) --- however, as the model beings to master the training tasks, the generalization quickly decays to substantially below chance. The early generalization is not included in the main results in Fig. \ref{fig:HoMM_RL_results} because the train set performance is below even the more generous threshold we set for the language model. Even if it were included, this early performance is significantly worse than the HoMM model's results. \par



\section{Visual concepts}

There is a long history of cognitive research on how people learn concepts or categories \citep{Bourne1970, Medin1978, Kruschke1992, Goodman2008}. This work has focused almost entirely on how people can learn a concept from examples, and more recent work in the area has focused on areas like active learning by choosing the examples to test \citep{Markant2014, Markant2015}. However, humans can also understand concepts without any examples at all. If I teach you that ``blickets'' are red triangles by examples, and then I tell you that ``zipfs are blue blickets,'' you will instantly be able to recognize a zipf without ever having seen an example (Fig. \ref{fig:extending_concept_example}). This type of zero-shot performance can be understood as applying a ``switch-red-to-blue'' meta-mapping to the ``blicket'' classification function. This motivates applying our approach to the domain of concepts. \par 

\subsection{Tasks}
%% TODO: detailed methods, rework this figure
\begin{figure}
\centering
\begin{tikzpicture}[auto]%, scale=0.8, every node/.style={scale=0.8}]
% blickets
\node at (-4.8, 2.2) {\LARGE ``Blickets''};
\draw[decoration={calligraphic brace, amplitude=0.5cm},decorate,line width=1mm] (-3, 0.3) -- (-3, 4);
\node at (-2, 3.1) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/32_red_triangle_0.png}};
\node at (-2, 1.2) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/24_red_triangle_0.png}};

% not blickets
\node at (-5.2, -2.1) {\LARGE ``Not blickets''};
\draw[decoration={calligraphic brace, amplitude=0.5cm},decorate,line width=1mm] (-3, -4) -- (-3, -0.3);
\node at (-2, -3.1) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/32_red_circle_0.png}};
\node at (-2, -1.2) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/24_yellow_triangle_0.png}};
\node at (3, 3.5) {\LARGE ``Zipfs = blue blickets''};
\node at (1.6, 0) {\LARGE ``Zipfs?''};
\draw[decoration={calligraphic brace, amplitude=0.5cm},decorate,line width=1mm] (3.2, -2.9) -- (3.2, 2.9);
\node at (4.2, 1.9) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/24_red_triangle_1.png}};
\node at (4.2, 0) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/32_blue_triangle_1.png}};
\node at (4.2, -1.9) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/32_blue_inverseplus_1.png}};
\end{tikzpicture}
\caption[An example of zero-shot visual concept understanding that can be captured by a meta-mapping.]{An example of zero-shot visual concept understanding that can be captured by a meta-mapping. We can understand what zipfs are if we learn about blickets, and about how zipfs relate to blickets.} \label{fig:extending_concept_example}

\end{figure}

We constructed stimuli by selecting from 8 shapes (triangle, square, plus, circle, a t shape, an outline of a square, an outline of a triangle, and 4 small squares forming a larger square) and 8 colors (red, green, blue, yellow, purple, pink, cyan, and ocean). We rendered these stimuli at 3 sizes (16, 24, and 32 pixels) at a random position and rotation within a \(50 \times 50\) pixel image, to produce stimuli like those shown in figure \ref{fig:extending_concept_example}. See Appendix \ref{app:extending_categorization_methods} for renderings of each shape, color, and size. \par
We defined the basic task mappings as binary classifications of the images (i.e. functions from images to \(\{0, 1\}\)). We gave the system all the uni-dimensional concepts as training examples of concepts (i.e. one-vs-all classification of each shape, color, and size), so that it would be able to recognize all the basic attributes. We also constructed a set of composite tasks based on conjuctions, disjunctions, and exclusive-disjunctions (XOR) of these basic attributes. For each concept, we chose the examples so that the datasets were balanced (that is, there was an 50\% chance that each item in the dataset was a member of the category), both during training and evaluation. We only included negative examples that were one change away from being a member of the category. These careful contrasts may be beneficial during training -- recent work has shown contrasting examples to be useful for causing neural networks to extract more general concepts \citep{Hill2019}. They also make the evaluation more challenging, and increase the ability to dscriminate partial understanding of the concept from complete understanding. We trained the system on a subset of the concepts, and on meta-mappings that switched one shape for another, or switched one color for another. We evaluated the system on its ability to apply these meta-mappings to basic tasks it was trained on in order to perform the held-out basic tasks. Because there are many meta-mappings available in this setting, we were able to hold out one shape meta-mapping and one color meta-mapping for evaluation, and we will also show results on adaptation based on these held-out mappings. \par  

\subsection{Model}
We used a 4-layer convolutional network for input embedding, and a linear task network \(F\) (although for the language comparison we used a 3-hidden-layer network, which resulted in improved performance in that condition). We found that constructing task representations from language was more effective here than constructing them from examples. \par
We also found that in this setting the optimal architectures for the task network differed for language-based generalization and meta-mapping-based generalization. In particular, we show in Appendix \ref{app:extending_categorization_lang_arch} that, while a linear task network worked better for the meta-mapping model, a nonlinear and deeper one generalized better from language. This is likely because the linear map provides a useful inductive bias for meta-mapping. This raises the possibility that adding linear skip-connections to nonlinear task networks which might allow for better meta-mapping performance as well as better task representations. It is also possible that using examples and language together to infer tasks would improve task representations further. Both of these provide exciting directions for future investigations. \par 

\subsection{Results}

\subsubsection{Comparing different sources of task representations}
We found that constructing task representations from language worked better than construting task representations from examples in this setting. In Fig. \ref{fig:extending_concepts_adaptation}, we show that language generalization (mean \(= 0.92\), bootstrap 95\%-CI \([0.89, 0.94]\)) appears better than that obtained from meta-mapping task representations constructed from examples (mean \(= 0.90\), bootstrap 95\%-CI \([0.89, 0.91]\)). However, this comparison is not statistically significant under a mixed model (\(t(8) = -1.5\), \(p = 0.18\)). However, meta-mapping with language-based task representations performs better (mean \(= 0.95\), bootstrap 95\%-CI \([0.94, 0.97]\), mixed-model \(t(8) = -2.9\), \(p = 0.02\)). Thus it appears that language may be a better way of constructing task representations in this setting, but meta-mapping a prior task still results in better zero-shot generalization than language alone. \par

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{4-extending/figures/concepts_adaptation.png}
\caption[Accuracy on held-out visual concepts.]{Accuracy on held-out visual concepts based on meta-mapping (either from examples or language), or language generalization. While language generalization appears non-significantly better than meta-mapping from examples, transforming language-based task representations with meta-mapping performs significantly better.} \label{fig:extending_concepts_adaptation} 
\end{figure}

Why does language result in better task representations in this setting? Of course, language conveys the basic concept more cleanly than examples can, but this was also true in other settings where language did not seem as advantageous. While we cannot completely answer this question, we note several points. First, we gave the system substantially more training tasks here than in the prior settings, and this larger amount of data may be necessary to keep the language system from overfitting. Second, several exciting lines of work are converging on the role of language in shaping our concept representations. Recent work has shown that more ``nameable'' concepts are easier to understand \citep{Lupyan2020}, and that language may be beneficial to visual concept learning in machine learning \citep{Mu2019}. Thus, these results may be reflective of broader issues about how concepts are formed. \par

\subsubsection{Evaluating the effects of training set size}
\begin{figure}
\centering
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/concepts_adaptation_sweeping.png}
\caption{Meta-mapping performance.}\label{fig:extending_concepts_sweeping_results:accuracy}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/concepts_adaptation_proportion_perfect.png}
\caption{Held-out meta-mappings.}\label{fig:extending_concepts_sweeping_results:perfect}
\end{subfigure}
\caption[Results of applying HoMM to visual concepts at different training set sizes.]{Results of applying HoMM to visual concepts at different training set sizes. Results are shown after training the model on various numbers of training meta-mappings (or the equivalent set of training concepts). HoMM and language generalization are well above chance (50\% with our balanced evaluation sets). (\subref{fig:extending_concepts_sweeping_results:accuracy}) The system is able to generalize trained meta-mappings to perform new tasks zero-shot, and does so somewhat better than language when the datasets are of moderate size. It is also able to generalize to adaptation based on held-out meta-mappings, once it experiences sufficiently many training meta-mappings. (\subref{fig:extending_concepts_sweeping_results:perfect}) The proportion of runs in which each model attained \(> 99\)\% accuracy on the transfored concepts. The HoMM model more frequently shows extremely systematic generalization at moderate sample sizes. (Results are from 10 runs of each model with each training set size. Errorbars are bootstrap 95\%-CIs across runs.)} \label{fig:extending_concepts_sweeping_results}
\end{figure}

We next evaluated the language-based HoMM model and language generalization with held-out meta-mappings, at varying sample sizes. Specifically, we trained the model with either 4 meta-mappings total (2 switch color, and 2 switch-shape), or 8, 16, 24, or 32. With each mapping, we included 6 training examples of the mapping (one for each composite pairing of rule type and other attribute). We also included 6 other pairs for evaluation, where the source concept was trained, but the target was held-out for evaluation. That is, the number of basic concepts the system encounters during training is roughly 18 per meta-mappings trained (roughly because it can be reduced if the meta-mappings have overlapping examples), and the number of evaluation concepts is roughly 6 per meta-mapping. We also included a set of training and evaluation basic tasks for a held-out meta-mapping of each type. We are thus able to evaluate how increasing training affects the generalization of the model within a meta-mapping, and its ability to generalize to a held-out meta-mapping. \par

In Fig. \ref{fig:extending_concepts_sweeping_results:accuracy}, we show the results. The advantage of the HoMM model is most prominent at moderate dataset sizes --- at very small training set sizes, its generalization results are comparable to (or worse than) the language model. At very high training set sizes, both models are performing well in this domain. \par

Even in this more complex task setting, once the model has seen a relatively small set of meta-mappings (32), it is able to generalize quite well to held-out meta-mappings from their language description. Although the average performance from held-out meta-mappings is not perfect, it is perfect in a sizable proportion of the runs. (See Supp. Fig. \ref{fig:app_extending:concepts_all_runs} for learning curves for each run and sample size.) \par

We would like to note an interesting difference in the patterns of generalization across runs exhibited by the models, as shown in Fig. \ref{fig:extending_concepts_sweeping_results:perfect}. While both models are varying across runs at small training set sizes, and both are generalizing near-perfectly at large sizes, for moderate sample sizes there are more runs in which the HoMM model is exhibiting near-perfectly systematic generalization. In Supp. Fig. \ref{fig:app_extending:concepts_generalization_density} we show this in more detail with density plots of the generalization performance across runs, which show that HoMM results in more sharply peaked performance. This suggests that HoMM is exhibiting a behavior of qualitatively either understanding or not understanding, while the language generalization approach exhibits more graded generalization. Thus the HoMM model may better capture the qualitative, systematic generalization that adult humans tend to exhibit.


\section{Discussion}

In this chapter, we have shown that HoMM can perform well in more complex domains, while still requiring only a relatively small set of training data. In the RL domain, HoMM generalized a single meta-mapping well from only 16 example tasks (18 trained tasks total). In the visual concepts domain, HoMM was able to generalize trained meta-mappings perfectly on every run once a sufficient number of training meta-mappings were provided, and was able to generalize to held-out meta-mappings quite well (although generalization to these improved beyond the point that trained meta-mapping generalization was at ceiling). \par 

The results of the language generalization model in this chapter are particularly striking. In the RL tasks, it generalized much worse than chance (once the training tasks were learned), and thus much worse than it did in the previous chapter. By contrast, in the visual concepts setting, it performed competitively with the meta-mapping approach, and thus much better than in the previous chapter. What is the cause of this discrepancy? \par 

There are a few possible factors underlying these results. First, because of the structure of the task spaces, the number of training tasks in the visual concepts space is much larger. The language model seems to need more training tasks than HoMM to begin to generalize well. It's also worth reflecting at this point on the results of \citet{Hill2019a}, in which we showed that more realistic environments improve language model generalization. The realism of the environments is matchedi between models in this setting, and I tried to incorporate features that we showed improve generalization (such as egocentric perspective on the RL tasks). However, it will be important to evaluate both types of models in more realistic settings in the future. \par

However, there is another factor that may be driving the differences between the results in this chapter. The training tasks in the RL setting more directly contradict the evaluation tasks, and there is no task that is close to the evaluation tasks. By contrast, in the visual concepts domain the sytem will have encountered many training concepts that overlap with the held-out evaluation in one attribute. That is, the HoMM model may generalize better to tasks farther outside the space of its experience than the language model does. \par  

In fact, we observed in both domains that the HoMM model was exhibiting intriguing signatures of generalizing more systematically than the language model. In the RL domain, generalization on the two held-out tasks was more tightly correlated in the HoMM model than in the language model. In the visual concepts domain, the HoMM model was resulting in perfect performance on a larger proportion of the runs at moderate sample sizes. Both of these results are suggestive of the more systematic generalization (or generalization failure) that humans tend to exhibit. \par

In summary, the HoMM model is promising in more realistic and complex settings that are more representative of the challenges that humans (and modern AI systems) face. It does not seem to require substantially larger sets of training tasks in these settings, and may even generalize more systematically than the language model. 



