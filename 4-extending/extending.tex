\chapter{Extending meta-mapping to more complex tasks} \label{chapter:extending}
In the previous chapters we have demonstrated the succes of meta-mapping in two simple domains. While these have allowed us to demonstrate the efficacy of the approach relative to other baselines, and compare its adaptation to that of humans, there are several reasons to extend beyond these to more complex tasks. \par 
First, the approach relies on several pieces which it is not obvious would scale to more complex settings, such as representing an entire task with a single vector, parameterizing the task network via a HyperNetwork conditioned on this vector, and learning meta-mappings from relatively few task examples. If any of these fails to extend to more complex settings, it could limit the applications of our approach. \par
Second, there are limitations to toy experiments. While toy experiments can provide carefully controlled demonstrations of an idea, we have shown in other work that more systematic generalization can emerge when agents are placed in more realistic settings \citep{Hill2019a}. This may impact both the meta-mapping approach and the language baseline, so it is important to evaluate the effects of richer environments on both. This will help inform us as to whether our approach will be useful in more complex settings. (Unfortunately, creating truly realistic environments and training agents in them requires complex implementations and substantial computational resources. Thus, in this chapter we demonstrate our results in environments of moderate complexity, and leave the extension to even richer environments to future work.)\par 
To address these motivations, in this chapter we present experiments on extending our ideas to two important settings: reinforcement learning and classification from raw pixel inputs. These settings are important both because they are dominant paradigms for applying deep learning, and because they have deep connections to cognitive modelling \citep[e.g.][]{Yamins2014, Kriegeskorte2015, Momennejad2017}. \par

\section{Reinforcement learning}

Reinforcement learning is an interesting (and challenging) application for meta-mapping for several reasons. First, reinforcement learning has deep roots in neuroscience, and appears to relate to some computations in the brain \citep{Sutton2017, Niv2009, Odoherty2003, Dabney2020}. Second, reinforcement learning has achieved impressive human-level performance on complex tasks such as Atari games \citep{Mnih2015}, Go \citep{Silver2016, Silver2017}, and complex video games like Dota 2 \citep{OpenAI2019}, and Starcraft II \citep{Vinyals2019}. This motivates it as an important place to explore more human-like intelligence. Third, there has been a rich vein of research on adaptation in reinforcement learning from language-conditioned models \citep{Hermann2017} to the observation that model-based methods or successor representations can allow for adaptation to environment changes \citep{Daw2014, Momennejad2017}. However, these latter methods assume that a new reward function is given, which requires a substantial portion of the adaptation problem already be solved. Thus, there is substantial room to ask whether meta-mapping can provide good performance in RL tasks, and good motivation for a language baseline. \par 
\subsection{Tasks}
To address these challenges we created a set of RL tasks based on raw visual input with a relatively simple action space. Refer to Fig. \ref{fig:extending_grid_task_views} throughout this section for images of the visual input the agent would receive. The tasks take place in a \(6 \times 6\) room with an additional impassable barrier of \(1\) square on each side. The squares are upsampled at a resolution of 7 pixels per square to provide the raw visual input to the agent. In addition, the agent receives egocentric input, since we have shown in other work that this is beneficial to generalization \citep{Hill2019a}. That is, the agent's view is always centered on itself, and the world moves around it as the agent moves. The agent has four actions available to it, corresponding to moving in the four cardinal directions. If it makes an invalid action, such as trying to move past the edge of the board, the state does not change. The view window is sufficiently larger than the world so that the agent can see the entire world, no matter where it is.\par 
The tasks the agent must perform relate to objects which are placed in this space. The objects can appear in 10 different colors. In any given task, the world will have two colors of objects in it. Each color of objects only appears with one other color, so there are in total 5 possible colors that can appear. In any given task, one of the present colors is ``good,'' and the other is ``bad.'' On some tasks, the good and bad colors in a pair are switched.\par
There are two types of tasks, a ``pick-up'' task, and a ``pusher'' task. The tasks are visually distinguishable to the agent, because the shape of the objects used for them are different. In the pick-up task, the agent is rewarded for picking up the good-colored objects by moving to their grid locations, and is negatively rewarded for picking up the bad-colored objects. In the pusher task, the agent is able to push an adjacent object by moving toward it, if there is no other object behind it. The agent is rewarded for pushing the good-colored objects off the edges of the board, and negatively rewarded for pushing the bad colored objects off. \par 
There are in total 2 task types \(\times\) 5 color pairs \(\times\) binary whether the good and bad colors are switched \(= 20\) tasks in total. We trained the system on 16 of these, holding out the switched color combinations of (``red'', ``blue''), and (``forest'', ``orange'') in both task types. That is, during training the agent always is positively rewarded for interacting with red objects and negative rewarded for interacting with blue objects across both task types, and similarly for forest and orange. We train the system on the ``switch-good-and-bad-colors'' meta-mapping using the remaining three color pairs in the two task types, and then evaluate its ability to perform the held-out tasks zero-shot based on this mapping. Note that this is a quite difficult challenge for a model-free system, since any rewards it receives during training on similar-colored tasks are the opposite of these evaluation rewards.\par
\begin{figure}
\centering
\begin{subfigure}[b]{0.5\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{4-extending/figures/pick_up_0.png}
\caption{Sample pick-up task view.}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{4-extending/figures/pusher_0.png}
\caption{Sample pusher task view.} 
\end{subfigure}%
\caption{The RL grid experiments, as seen by the agent. The agent is the white triangle, note that it is always at the center of the view, because of the egocentric perspective.} \label{fig:extending_grid_task_views}
\end{figure}

\subsection{Model}
%% TODO: detailed parameters
To accomodate this setting, we essentially combined the DQN architecture \citep{Mnih2015} with our previous approaches. That is, the input to the model was raw pixels, which were passed through several convolutional layers to produce state embeddings. This visual processing was shared across all tasks. As in the card game tasks discussed in previous chapters, we used (state, action, reward) tuples as input to the meta-network \(\mathcal{M}\). More precisely, we processed actions and rewards through several layers to produce an ``action-outcome'' embedding, and then concatenated that to the visual embedding as input to \(\mathcal{M}\). As before, \(\mathcal{M}\) produced a task embedding, which was passed through a HyperNetwork \(\mathcal{H}\) to parameterize the task network \(F\). The task network took state embeddings output by the convolutional network, and processed these to produce to produce output embeddings, which were linearly decoded to Q-values for the different actions. When meta-mapping, the input task embeddings replaced the input state embeddings to \(\mathcal{M}\) and \(F\), and the output task embeddings replaced the action-outcome inputs to \(\mathcal{M}\). See Appendix \ref{app:extending_grids_methods} for further details of the architecture and training. \par 
We made one additional change to the architecture that proved essential to learning -- rather than constructing the task embedding completely from scratch each time, we kept persistent task embeddings cached, and used a random convex-combination of the output of \(\mathcal{M}\) and the cached embedding to perform the task. We added an additional \(\ell_2\) loss between the cached and transient embedding that attempted to match each to the other. Having partially persistent embeddings made it easier for the system to overcome the initial confusion about the fact that objects were sometimes positively rewarding and sometimes negatively rewarding, and made it easier for it to discover the overall structure of the tasks. In the simpler settings of the cards tasks, this was not necessary. It is likely that the temporally extended nature of these tasks makes the interference between the conflicting tasks worse. However, with enough training and time, the model could likely overcome this and learn without persistent representations in this setting as well. \par 

\subsection{Results}


\section{Concepts}

There is a long history of cognitive research on how people learn concepts or categories \citep{Bourne1970, Medin1978, Kruschke1992, Goodman2008}. This work has focused almost entirely on how people can learn a concept from examples, and more recent work in the area has focused on areas like active learning by choosing the examples to test \citep{Markant2014, Markant2015}. However, humans can also understand concepts without any examples at all. If I teach you that ``blickets'' are red triangles by examples, and then I tell you that ``zipfs are blue blickets,'' you will instantly be able to recognize a zipf without ever having seen an example (Fig. \ref{fig:extending_concept_example}). This type of zero-shot performance can be understood as applying a ``switch-red-to-blue'' meta-mapping to the ``blicket'' classification function. This motivates applying our approach to the domain of concepts. \par 

\subsection{Tasks}
%% TODO: detailed methods, rework this figure
\begin{figure}
\centering
\begin{tikzpicture}[auto]%, scale=0.8, every node/.style={scale=0.8}]
% blickets
\node at (-4.8, 2.2) {\LARGE ``Blickets''};
\draw[decoration={calligraphic brace, amplitude=0.5cm},decorate,line width=1mm] (-3, 0.3) -- (-3, 4);
\node at (-2, 3.1) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/32_red_triangle_0.png}};
\node at (-2, 1.2) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/24_red_triangle_0.png}};

% not blickets
\node at (-5.2, -2.1) {\LARGE ``Not blickets''};
\draw[decoration={calligraphic brace, amplitude=0.5cm},decorate,line width=1mm] (-3, -4) -- (-3, -0.3);
\node at (-2, -3.1) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/32_red_circle_0.png}};
\node at (-2, -1.2) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/24_yellow_triangle_0.png}};
\node at (3, 3.5) {\LARGE ``Zipfs = blue blickets''};
\node at (1.6, 0) {\LARGE ``Zipfs?''};
\draw[decoration={calligraphic brace, amplitude=0.5cm},decorate,line width=1mm] (3.2, -2.9) -- (3.2, 2.9);
\node at (4.2, 1.9) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/24_red_triangle_1.png}};
\node at (4.2, 0) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/32_blue_triangle_1.png}};
\node at (4.2, -1.9) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/32_blue_inverseplus_1.png}};
\end{tikzpicture}
\caption{An example of zero-shot concept understanding that can be captured by a meta-mapping. We can understand what zipfs are if we learn about blickets, and about how zipfs relate to blickets.} \label{fig:extending_concept_example}

\end{figure}

We constructed stimuli by selecting from 8 shapes (triangle, square, plus, circle, a t shape, an outline of a square, an outline of a triangle, and 4 small squares forming a larger square) and 8 colors (red, green, blue, yellow, purple, pink, cyan, and ocean). We rendered these stimuli at 3 sizes (16, 24, and 32 pixels) at a random position and rotation within a \(50 \times 50\) pixel image, to produce stimuli like those shown in figure \ref{fig:extending_concept_example}. See Appendix \ref{app:extending_categorization_methods} for renderings of each shape, color, and size. \par
We defined the basic task mappings as binary classifications of the images (i.e. functions from images to \(\{0, 1\}\)). We gave the system all the uni-dimensional concepts as training examples of concepts (i.e. one-vs-all classification of each shape and color), so that it would be able to recognize all the basic attributes. We also constructed a set of composite tasks based on conjuctions, disjunctions, and exclusive-disjunctions (XOR) of shape and color attributes. We trained the system on a subset of these, and on meta-mappings that switched one shape for another, or switched one color for another. We evaluated the system on its ability to apply these meta-mappings to trained tasks in order to perform held-out tasks. \par  

\subsection{Model}
We used a 4 layer convolutional network for input embedding, and a linear task network \(F\) (although for the language comparison we used a 3-hidden-layer network, which resulted in improved performance in that condition). 

{\color{red} We found that constructing task representations from language was more effective here than constructing them from examples. We show in Fig. TODO that }


\subsection{Results}

