\chapter{Extending meta-mapping to more complex tasks} \label{chapter:extending}
In the previous chapters we have demonstrated the success of meta-mapping in two simple domains. While those experiments allowed us to demonstrate the efficacy of the approach relative to other baselines, and compare its adaptation to that of humans, there are several reasons to extend beyond them to more complex tasks. \par 
First, the approach relies on several pieces that it is not obvious would scale to more complex settings. For example, representing an entire task with a single vector could be limiting. Similarly, parameterizing the task network via a HyperNetwork conditioned on a task representation could be challenging as the computations become more complex, as could learning meta-mappings from relatively few task examples. If any of these fails to extend to more complex settings, it could limit the applications of our approach. \par
A second motivation for exploring more complex settings is the limitations inherent to toy experiments. While toy experiments can provide carefully controlled demonstrations of an idea, we have shown in other work that more systematic generalization can emerge when agents are placed in more realistic settings \citep{Hill2019a}. This may impact both the meta-mapping approach and the language baseline, so it is important to evaluate the effects of richer environments on both. This will help inform us as to whether our approach will be useful in more complex settings. \par
Unfortunately, creating truly realistic environments, and training agents in them, requires complex implementations and substantial computational resources. Thus, in this chapter we demonstrate our results in environments of moderate complexity, and leave the extension to even richer environments for future work.\par 
In particular, we present experiments on extending our ideas to two important settings: reinforcement learning and classification from raw pixel inputs. These settings are important both because they are dominant paradigms for applying deep learning, and because they have deep connections to cognitive modeling and neuroscience \citep[e.g.][]{Yamins2014, Kriegeskorte2015, Momennejad2017}. \par

\section{Reinforcement learning} \label{sec:extending:rl}

\begin{figure}[!htb]
\centering
\begin{tikzpicture}[auto]%, scale=0.8, every node/.style={scale=0.8}]
\draw[boundingbox, draw=gray, fill=white] (-9.3, 2.65) rectangle (0.3, -2.3);
\node[gray, align=center] at (-4.6, 2.35) {Pick-up task};
\node at (-7, 0) (pu1) {\includegraphics[width=4cm]{4-extending/figures/grid_tasks/pick_up_3.png}};
\node at (-2, 0) (pu2) {\includegraphics[width=4cm]{4-extending/figures/grid_tasks/pick_up_4.png}};
\node at (-4.5, 0) {\huge \(\bm \downarrow\)};
%\draw[arrow, very thick, gray] (pu1) -- node[black] {\huge``\(\bm \downarrow\)''} (pu2);

\begin{scope}[shift={(0, -5.5)}]
\draw[boundingbox, draw=gray, fill=white] (-9.3, 2.65) rectangle (0.3, -2.3);
\node[gray, align=center] at (-4.6, 2.35) {Push-off task};
\node at (-7, 0) (pu1) {\includegraphics[width=4cm]{4-extending/figures/grid_tasks/pusher_9.png}};
\node at (-2, 0) (pu2) {\includegraphics[width=4cm]{4-extending/figures/grid_tasks/pusher_10.png}};
\node at (-4.5, 0) {\huge \(\bm \leftarrow\)};
%\draw[arrow, very thick, gray] (pu1) -- node[black] {\huge``\(\bm \downarrow\)''} (pu2);
\end{scope}
\end{tikzpicture}
\caption[Illustrative state transitions from the RL grid experiments.]{Illustrative state, action, state transitions from the RL grid experiments. In the pick-up task example (top), the agent moves downward and picks up the green object. In the push-off task example (bottom), the agent moves left and pushes the red object. Views are the visual input the agent would receive. The agent is the white triangle, note that it is always at the center of the view, because of the egocentric perspective.} \label{fig:extending_grid_task_views}
\end{figure}

Reinforcement learning is an interesting (and challenging) application for meta-mapping for several reasons. First, reinforcement learning has deep roots in neuroscience, and various RL-related computations appear to explain some aspects of neural activity \citep{Sutton2017, Niv2009, Odoherty2003, Dabney2020}.\par 
Second, reinforcement learning has achieved impressive human-level, performance on complex tasks such as Atari games \citep{Mnih2015}, Go \citep{Silver2016, Silver2017}, and complex video games like Dota 2 \citep{OpenAI2019}, and Starcraft II \citep{Vinyals2019}. This motivates it as an important place to explore more human-like intelligence. \par 
Third, there has been a rich vein of research on adaptation in reinforcement learning, from using language-conditioned models \citep{Hermann2017} to the observation that model-based methods or successor representations can allow for adaptation to environment changes \citep{Daw2014, Momennejad2017}. However, these latter methods assume that a new reward function is given, which requires a substantial portion of the adaptation problem already be solved. Thus, there is substantial room to ask whether meta-mapping can provide good performance in RL tasks, and good motivation for a language-based approach as a comparison. \par 
\subsection{Tasks}
To address these challenges we created a set of RL tasks based on raw visual input, with a relatively simple action space. Refer to Fig. \ref{fig:extending_grid_task_views} throughout this section for images of the visual input the agent would receive. The tasks take place in a \(6 \times 6\) room with an additional impassable barrier of \(1\) square on each side. The squares are upsampled at a resolution of 7 pixels per square to provide the raw visual input to the agent. In addition, the agent receives egocentric input, since we have shown in other work that this is beneficial to generalization \citep{Hill2019a}. That is, the agent's view is always centered on itself, and the world moves around it as the agent moves. The agent has four actions available to it, corresponding to moving in the four cardinal directions. If it makes an invalid action, such as trying to move past the edge of the board, the state does not change. The view window is sufficiently large so that the agent can see the entire world, no matter where it is.\par 
The tasks the agent must perform relate to objects which are placed in this space. The objects can appear in 10 different colors. In any given task, the world will have two colors of objects in it. Each color of objects only appears with one other color, so there are in total 5 possible pairs of colors that can appear. In any given task, one of the present colors is ``good,'' and the other is ``bad.'' On some tasks, the good and bad colors in a pair are switched.\par
There are two types of tasks, a ``pick-up'' task, and a ``push-off'' task. In the pick-up task, the agent is rewarded for picking up the good-colored objects by moving to their grid locations, and is negatively rewarded for picking up the bad-colored objects. In the push-off task, the agent is able to push an adjacent object by moving toward it, if there is no other object behind it. The agent is rewarded for pushing the good-colored objects off the edges of the board, and negatively rewarded for pushing the bad colored objects off. The two types of tasks (``pick-up'' and ``push-off'') are visually distinguishable to the agent, because the shape of the objects used for them are different. However, which color is good or bad is not visually discernable, and must be inferred from the example (state, (action, reward)) tuples used to construct the task representation.\par 
There are in total 2 task types \(\times\) 5 color pairs \(\times\) binary whether the good and bad colors are switched \(= 20\) tasks in total. We trained the system on 18 of these, holding out the switched color combination of (``red'', ``blue'') in both task types. That is, during training the agent is always positively rewarded for interacting with red objects and negatively rewarded for interacting with blue objects, across both task types.\par
We trained the system on the ``switch-good-and-bad-colors'' meta-mapping using the remaining three color pairs in the two task types, and then evaluate its ability to perform the held-out tasks zero-shot based on this mapping. Note that this is a quite difficult challenge for a model-free system, since any rewards it receives during training on similar-colored tasks are the opposite of these evaluation rewards.\par

\subsection{Model}
%% TODO: detailed parameters
To accomodate this setting, we essentially combined the DQN architecture \citep{Mnih2015} with our previous approaches. That is, the input to the model was raw pixels, which were passed through several convolutional layers to produce state embeddings. This visual processing was shared across all tasks. As in the card game tasks discussed in the previous chapter, we used (state, action, reward) tuples as input to the meta-network \(\mathcal{M}\). More precisely, we processed actions and rewards through several layers to produce an ``action-outcome'' embedding, and then concatenated that to the visual embedding as input to \(\mathcal{M}\). As before, \(\mathcal{M}\) produced a task embedding, which was passed through a HyperNetwork \(\mathcal{H}\) to parameterize the task network \(F\). The task network took state embeddings output by the convolutional network, and processed these to produce to produce output embeddings. The output embeddings were processed by a linear layer (shared across tasks) to produce Q-values for the different actions. When meta-mapping, the input task embeddings replaced the input state embeddings to \(\mathcal{M}\) and \(F\), and the output task embeddings replaced the action-outcome inputs to \(\mathcal{M}\). (See Appendix \ref{app:extending_grids_methods} for further details of the architecture and training, and see Supp. Fig. \ref{supp_fig:extending:RL:arch_cond_vs_hyper} for a demonstration that the HyperNetwork-based HoMM architecture outperforms a simpler task-concatenated one, as in the polynomials domain.)\par 
Perhaps because of the difficulty of the generalization problem in this setting, we found that two additional model modifications were useful. First, rather than constructing the task embedding completely from scratch each time, we kept persistent task embeddings cached, and used a random convex combination of the output of \(\mathcal{M}\) and the cached embedding to perform the task. We added an additional \(\ell_2\) loss between the cached and transient embedding that attempted to match each to the other. Having partially persistent embeddings made it easier for the system to overcome the initial conflicting gradients caused by the fact that objects were sometimes positively rewarding and sometimes negatively rewarding, and thus made it easier for the model to discover the overall structure of the tasks. \par
Second, we found that incorporating weight normalization \citep{Salimans2016} in the task network increased the stability of the training process. In the simpler settings of the cards tasks, neither of these modifications were necessary. It is likely that the temporally extended nature of these tasks makes the interference between the conflicting tasks worse. However, it is possible that with appropriate hyperparameters, and enough training and time, the model could overcome this and learn without persistent representations or weight normalization in this setting as well. \par 

\subsection{Results}

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{4-extending/figures/grids_adaptation_results.png}
\caption[Performance on the held-out RL tasks.]{Performance on the held-out RL tasks via a meta-mapping and via language generalization. Despite the challenging nature of the adaptation (as evidenced by the language-generalization performance), HoMM is performing quite well. (Results from 5 runs, error-bars are bootstrap 95\%-CIs across runs.)} \label{fig:HoMM_RL_results}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{4-extending/figures/grids_adaptation_correlation_loose.png}
\caption[Correlation of performance on the held-out RL tasks.]{Correlation of performance on the held-out RL tasks, across runs and time points where performance on the training tasks is high. The correlation is much stronger in the HoMM model than in the language-generalization model, that is, the HoMM model is behaving more systematically in the sense that it is generalizing similarly on both tasks. (Results from 5 runs, lines are linear model fits within model type.)} \label{fig:HoMM_RL_results:correlation}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{4-extending/figures/grids_language_learning_curves.png}
\caption[Language generalization model learning curves on the RL tasks.]{Average performance of the language generalization model over training on the RL tasks. The model exhibits intriguing, but transient, generalization early in learning, before it has understood the full structure of the tasks (especially the more difficult and sequential push-off task). However, this quickly decays to below-chance generalization as the model masters the training tasks. This early generalization is not included in the main results since the train accuracy at this time is below the threshold of having adequately learned the tasks.} \label{fig:HoMM_RL_language_learning_curves}
\end{figure}

\begin{figure}[htb]
\centering
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{4-extending/figures/grids_behavioral_uncertainty.png}
\caption{Mean step counts.}\label{fig:extending:RL:behavioral_uncertainty:main}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{4-extending/figures/grids_behavioral_uncertainty_by_score_diff.png}
\caption{Differences in steps vs. differences in rewards.}\label{fig:extending:RL:behavioral_uncertainty:deltas}
\end{subfigure}
\caption[The HoMM model exhibits behavioral uncertainty in meta-mapping generalization on the RL tasks.]{The HoMM model exhibits behavioral uncertainty in meta-mapping generalization on the RL tasks, measured by the steps taken to complete each episode. (\subref{fig:extending:RL:behavioral_uncertainty:main}) The HoMM model takes more steps to complete episodes from the held-out tasks via a meta-mapping than to complete episodes from tasks used to train the meta-mapping. That is, it appears to be more uncertain about its behavior on the generalization tasks. (\subref{fig:extending:RL:behavioral_uncertainty:deltas}) The behavioral uncertainty effect is not solely driven by the model performing more poorly overall; even on the runs where it performs well, it is almost always taking longer to complete the episodes from the tasks it has never seen before. To show this, we plot the difference in average steps vs. difference in average rewards between train and eval. Note that the step difference is almost always positive (evaluation tasks are slower), even where rewards are comparable. (Panel \subref{fig:extending:RL:behavioral_uncertainty:main}: means and bootstrap 95\%-CIs across 5 runs. Panel \subref{fig:extending:RL:behavioral_uncertainty:deltas}: each point is one game type within one run.)} \label{fig:extending:RL:behavioral_uncertainty}
\end{figure}


In Fig. \ref{fig:HoMM_RL_results} we show the results. We optimally stop the model for each task by requiring the training accuracy to be above a threshold (95\% for the HoMM model, but 87.5\% for the language model, because stricter thresholds resulted in worse language results), and using the other task as a validation set --- that is, we evaluate the model on one task when the model performs well on the other task as well as the training tasks. The HoMM model substantially outperforms the language model, achieving 88.0\% of optimal rewards (mean, bootstrap 95\%-CI [75.0-99.0]) on the held-out pick-up task, and 71.7\% (mean, bootstrap 95\%-CI [42.0, 94.6]) on the held-out push-off task. By contrast, the language model is showing very little adaptation, with respective performance of -92.8\% (mean, bootstrap 95\%-CI [-96.3, -88.4]) and -79.7\% (mean, bootstrap 95\%-CI [-92.8, -59.1]) on the two tasks. This difference between the models is significant in a mixed linear regression controlling for task type and a random effect of run (\(t (20.6) = -19.515\), \(p < 1\cdot10^{-14}\)).\footnote{Degrees of freedom calculated by the Satterthwaite approximation.}\par

The HoMM model also exhibits significantly stronger correlations between its performance on the two tasks, both within runs at different time-points and across runs (Fig. \ref{fig:HoMM_RL_results:correlation}, Supp. Fig. \ref{fig:app_extending:RL_correlation_by_run}). Specifically, the HoMM model has a correlation of \(r=0.82\) between performance on the two tasks, while the language model only has a correlation \(r=0.10\), and this difference is significant in a mixed linear model predicting push-off performance from pick-up performance, controlling for task type, epoch, and the random effect of run (main effect of HoMM \(t(451.3) = 4.76\), \(p < 1\cdot 10^{-5}\), interaction of HoMM with pick-up performance \(t(452.0) = 3.43\), \(p < 1 \cdot 10^{-3}\)). At a surface level, this means that it is easier to select a good stopping point for the HoMM model --- even though the language model is achieving less bad (though still at or below chance) performance at some points in some runs, the lack of correlation between the results on the different tasks means there is no fair way to stop training the model at that point. More fundamentally, this suggests that the meta-mapping approach is exhibiting more systematic generalization, in the sense that it is either generalizing well on both tasks, or not generalizing well on both. Again, this is more like what would be expected from human cognition.

Intriguingly, the language model does transiently exhibit slightly positive generalization very early in learning (Fig. \ref{fig:HoMM_RL_language_learning_curves}) --- however, as the model beings to master the training tasks, the generalization quickly decays to substantially below chance. The early generalization is not included in the main results in Fig. \ref{fig:HoMM_RL_results} because the train set performance is below even the more generous threshold we set for the language model. Even if it were included, this early performance is significantly worse than the HoMM model's results. \par

It is also interesting to explore the behavior of the HoMM model after meta-mapping. In Fig. \ref{fig:extending:RL:behavioral_uncertainty} we show intriguing behavioral uncertainty in generalization, where the model exhibits more uncertainty (takes longer to solve the task) on novel tasks, even if it performs well. In the figure, we compare mean steps on the held-out task versions (performed via a meta-mapping) to the mean steps on the trained versions (performed via a meta-mapping, for a fair comparison). In Fig. \ref{fig:extending:RL:behavioral_uncertainty:deltas} we show that even when the model achieves similar rewards on the held-out tasks, it is doing so more slowly. Selected recordings of behavior can be found at: \url{https://github.com/lampinen/homm_grids/tree/master/recordings} --- these show the range of behavior across the different tasks and runs. On the held-out tasks, the model often transiently exhibits uncertain behavior (e.g. running in circles) before correctly executing the task behavior. (Note that our use of a softmax policy can both contribute to this behavior, and allow an escape from it!) I have also anecdotally observed similarly longer episode lengths in RL generalization while working on the \citet{Hill2019a} project. Exploring uncertainty in RL generalization, both with meta-mapping and with other paradigms, would be an interesting direction for future work.




\section{Visual concepts} \label{sec:extending:concepts}

There is a long history of cognitive research on how people learn concepts or categories \citep{Bourne1970, Medin1978, Kruschke1992, Goodman2008}. This work has focused almost entirely on how people can learn a concept from examples, and more recent work in the area has focused on areas like active learning by choosing which examples to test \citep{Markant2014, Markant2015}. However, humans can also understand concepts without any examples at all. If I teach you that ``blickets'' are red triangles by examples, and then I tell you that ``zipfs are blue blickets,'' you will instantly be able to recognize a zipf without ever having seen an example (Fig. \ref{fig:extending_concept_example}). This type of zero-shot performance can be understood as applying a ``switch-red-to-blue'' meta-mapping to the ``blicket'' classification function. This motivates applying our approach to the domain of concepts. \par 

\subsection{Tasks}
%% TODO: detailed methods, rework this figure
\begin{figure}
\centering
\begin{tikzpicture}[auto]%, scale=0.8, every node/.style={scale=0.8}]
% blickets
\node at (-4.8, 2.2) {\LARGE ``Blickets''};
\draw[decoration={calligraphic brace, amplitude=0.5cm},decorate,line width=1mm] (-3, 0.3) -- (-3, 4);
\node at (-2, 3.1) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/32_red_triangle_0.png}};
\node at (-2, 1.2) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/24_red_triangle_0.png}};

% not blickets
\node at (-5.2, -2.1) {\LARGE ``Not blickets''};
\draw[decoration={calligraphic brace, amplitude=0.5cm},decorate,line width=1mm] (-3, -4) -- (-3, -0.3);
\node at (-2, -3.1) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/32_red_circle_0.png}};
\node at (-2, -1.2) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/24_yellow_triangle_0.png}};
\node at (3, 3.5) {\LARGE ``Zipfs = blue blickets''};
\node at (1.6, 0) {\LARGE ``Zipfs?''};
\draw[decoration={calligraphic brace, amplitude=0.5cm},decorate,line width=1mm] (3.2, -2.9) -- (3.2, 2.9);
\node at (4.2, 1.9) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/24_red_triangle_1.png}};
\node at (4.2, 0) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/32_blue_triangle_1.png}};
\node at (4.2, -1.9) {\includegraphics[width=1.8cm]{4-extending/figures/categorization_stimuli/32_blue_inverseplus_1.png}};
\end{tikzpicture}
\caption[An example of zero-shot visual concept understanding that can be captured by a meta-mapping.]{An example of zero-shot visual concept understanding that can be captured by a meta-mapping. We can understand what zipfs are if we learn about blickets, and about how zipfs relate to blickets.} \label{fig:extending_concept_example}

\end{figure}

We constructed stimuli by selecting from 8 shapes (triangle, square, plus, circle, a t shape, an outline of a square, an outline of a triangle, and 4 small squares forming a larger square) and 8 colors (red, green, blue, yellow, purple, pink, cyan, and ocean). We rendered these stimuli at 3 sizes (16, 24, and 32 pixels) at a random position and rotation within a \(50 \times 50\) pixel image, to produce stimuli like those shown in figure \ref{fig:extending_concept_example}. See Supplemental Fig. \ref{fig:app_extending_cat_stims} for renderings of each shape, color, and size. \par
We defined the basic task mappings as binary classifications of the images (i.e. functions from images to \(\{0, 1\}\)). We gave the system all the uni-dimensional concepts as training examples of concepts (i.e. one-vs-all classification of each shape, color, and size), so that it would be able to recognize all the basic attributes. We also constructed a set of composite tasks based on conjuctions, disjunctions, and exclusive-disjunctions (XOR) of these basic attributes. \par
For each concept, we chose the examples so that the datasets were balanced (that is, there was an 50\% chance that each item in the dataset was a member of the category), both during training and evaluation. We only included negative examples that were one change away from being a member of the category. These careful contrasts may be beneficial during training -- recent work has shown contrasting examples to be useful for causing neural networks to extract more general concepts \citep{Hill2019}. They also make the evaluation tasks more challenging, and therefore increase our ability to dscriminate partial understanding of the concept from complete understanding. \par
We trained the system on a subset of the concepts, and on meta-mappings that switched one shape for another, or switched one color for another. We evaluated the system on its ability to apply these meta-mappings to basic tasks it was trained on in order to perform the held-out basic tasks. Because there are many meta-mappings available in this setting, we were able to hold out one shape meta-mapping and one color meta-mapping for evaluation, and we will also show results on adaptation based on these held-out mappings. \par  

\subsection{Model}
We used a 4-layer convolutional network for input embedding, and a linear task network \(F\), followed by a 2-layer output network (shared across tasks). We found that constructing task representations from language was more effective here than constructing them from examples --- see below. \par
We also found that in this setting the optimal architectures for the task network differed for language-based generalization and meta-mapping-based generalization. In particular, we show in Appendix \ref{app:extending_categorization_lang_arch} that, while a linear task network worked better for the meta-mapping model, a nonlinear and deeper one generalized better from language. This is likely because the linear map provides a useful inductive bias for meta-mapping. This raises the possibility that adding linear skip-connections to nonlinear task networks which might allow for better meta-mapping performance as well as better task representations. It is also possible that using examples and language together to infer tasks would improve task representations further. Both of these provide exciting directions for future investigations. \par 

\subsection{Results}

\subsubsection{Comparing different sources of task representations}
We found that constructing task representations from language worked better than construting task representations from examples in this setting. In Fig. \ref{fig:extending_concepts_adaptation}, we show that language generalization (mean \(= 0.92\), bootstrap 95\%-CI \([0.89, 0.94]\)) appears better than that obtained from meta-mapping task representations constructed from examples (mean \(= 0.90\), bootstrap 95\%-CI \([0.89, 0.91]\)). However, this comparison is not statistically significant under a mixed model (\(t(8) = -1.5\), \(p = 0.18\)). However, meta-mapping with language-based task representations performs better (mean \(= 0.95\), bootstrap 95\%-CI \([0.94, 0.97]\), mixed-model \(t(8) = -2.9\), \(p = 0.02\)). Thus it appears that language may be a better way of constructing task representations in this setting, but meta-mapping a prior task still results in better zero-shot generalization than language alone. \par

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{4-extending/figures/concepts_adaptation.png}
\caption[Accuracy on held-out visual concepts.]{Accuracy on held-out visual concepts based on meta-mapping (either from examples or language), or language generalization. While language generalization appears non-significantly better than meta-mapping from examples, transforming language-based task representations with meta-mapping performs significantly better.} \label{fig:extending_concepts_adaptation} 
\end{figure}

Why does language result in better task representations in this setting? Of course, language conveys the basic concept more cleanly than examples can, but this was also true in other settings, where language did not seem as advantageous. First, several exciting lines of work are converging on the role of language in shaping our concept representations. Recent work has shown that more ``nameable'' concepts are easier to understand \citep{Lupyan2020}, and that language may be beneficial to visual concept learning in machine learning \citep{Mu2019}. Thus, these results may be reflective of broader issues about how concepts are formed. But perhaps more importantly, some concepts can be difficult to discriminate from a small set of examples (such as XOR vs. OR). In order to provide a fair evaluation, we would have to sample the examples much more carefully to ensure that every set we used was unambiguous. We adopted the simpler approach of simply constructing task representations from language for the following experiments. \par

\subsubsection{Evaluating the effects of training set size}
\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{4-extending/figures/concepts_adaptation_sweeping.png}
\caption[Applying HoMM to visual concepts after training on different numbers of meta-mappings.]{Applying HoMM to visual concepts after training on different numbers of meta-mappings (or the equivalent set of training concepts). HoMM and language generalization are well above chance (50\% with our balanced evaluation sets, not shown). HoMM is able to generalize trained meta-mappings to perform new tasks zero-shot, and performs comparably to language generalization. (Results are from 10 runs of each model with each training set size. Errorbars are bootstrap 95\%-CIs across runs.)} \label{fig:extending_concepts_sweeping_results}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{4-extending/figures/concepts_adaptation_sweeping_eval.png}
\caption[Evaluating HoMM on held-out meta-mappings at different training set sizes.]{Evaluation HoMM on held-out meta-mappings visual concepts at different training set sizes. Results are shown after training the model on various numbers of training meta-mappings. The HoMM model is able to generalize to adaptation based on held-out meta-mappings, once it experiences sufficiently many training meta-mappings. (Results are from 10 runs of each model with each training set size. Errorbars are bootstrap 95\%-CIs across runs.)} \label{fig:extending_concepts_sweeping_results_eval}
\end{figure}

We next evaluated the language-based HoMM model and language generalization with held-out meta-mappings, at varying sample sizes. Specifically, we trained the model with either 4 meta-mappings total (2 switch color, and 2 switch-shape), or 8, 16, 24, or 32. With each mapping, we included 6 training examples of the mapping (one for each pairing of composite rule type and other attribute). We also included 6 other pairs for evaluation, where the source concept was trained, but the target was held-out for evaluation. That is, the number of basic concepts the system encounters during training is roughly 18 per meta-mappings trained (roughly because it can be reduced if the meta-mappings have overlapping examples), and the number of evaluation concepts is roughly 6 per meta-mapping. However, this sampling of the tasks also has a drawback, see below. \par
We also included a set of training and evaluation basic tasks for two held-out meta-mappings, one ``switch shape'' and one ``switch color.'' We are thus able to evaluate how increasing training affects both the generalization of the model within a meta-mapping, and its ability to generalize to a held-out meta-mapping. \par

In Fig. \ref{fig:extending_concepts_sweeping_results}, we show the results for trained meta-mappings. The language model and the HoMM model perform quite comparably in this domain. The HoMM model may show a slight advantage at moderate training set sizes, but the magnitude of the difference is smaller than that in the previous experiment. It is possible that this is due to the sampling of basic concepts we chose. By ensuring that each meta-mapping would be supported by a set of examples that span all possible relations and types, we ensured that there is a training task that is very similar to each evaluation task, which may have made it easier for the language model to interpolate. By contrast, in the results shown in Fig. \ref{fig:extending_concepts_adaptation}, we sampled tasks randomly, which means some tasks would have required more extrapolation. See below for further discussion.\par

Once the HoMM model has seen a relatively small set of meta-mappings (32), it is able to generalize quite well to held-out meta-mappings from their language description, as shown in Fig. \ref{fig:extending_concepts_sweeping_results_eval}. Although the average performance from held-out meta-mappings is not perfect, it is perfect in a sizable proportion of the runs (Fig. \ref{fig:extending_concepts_perfect:eval}). See Supp. Fig. \ref{fig:app_extending:concepts_all_runs} for learning curves for each run and sample size. \par

%We would like to note an interesting difference in the patterns of generalization across runs exhibited by the models, as shown in Fig. \ref{fig:extending_concepts_perfect}. While both models are varying across runs at small training set sizes, and both are generalizing near-perfectly at large sizes, for moderate sample sizes there are more runs in which the HoMM model is exhibiting near-perfectly systematic generalization. This may suggest that HoMM is exhibiting a behavior of qualitatively either understanding or not understanding, while the language generalization approach exhibits more graded generalization. Thus the HoMM model may better capture the qualitative, systematic generalization that adult humans tend to exhibit.

\begin{figure}[htb]
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/concepts_adaptation_proportion_perfect.png}
\caption{Trained meta-mappings.}\label{fig:extending_concepts_perfect:train}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{4-extending/figures/concepts_adaptation_proportion_perfect_eval.png}
\caption{Held-out meta-mappings.}\label{fig:extending_concepts_perfect:eval}
\end{subfigure}
\caption[In the visual concepts domain, the proportion of runs in which each model attained \(> 99\)\% accuracy on the transformed concepts.]{In the visual concepts domain, the proportion of runs in which each model attained \(> 99\)\% accuracy on the transformed concepts. (\subref{fig:extending_concepts_perfect:train}) Trained meta-mappings. The HoMM model more frequently shows extremely systematic generalization at moderate sample sizes. (\subref{fig:extending_concepts_perfect:eval}) Held-out meta-mappings. At the largest sample sizes we considered, the HoMM model is able to adapt near-perfectly to new meta-mappings on many runs. Note that even at this largest sample size, the system is generalizing from only 32 trained meta-mappings.}\label{fig:extending_concepts_perfect}

\end{figure}



\section{Discussion}
%% TODO: mention behavioral uncertainty?

In this chapter, we have shown that HoMM can perform well in more complex domains than we had previously explored, while still requiring only a relatively small set of training tasks. In the RL domain, HoMM generalized a single meta-mapping well from only 16 example tasks (18 trained tasks total). In the visual concepts domain, HoMM was able to generalize trained meta-mappings perfectly on every run once a sufficient number of training meta-mappings were provided, and was able to generalize to held-out meta-mappings quite well. Generalization on the held-out meta-mappings continued to improve beyond the point that trained meta-mapping generalization was at ceiling. \par 

The results of the language generalization model in this chapter are particularly striking. In the RL tasks, it generalized much worse than chance (once the training tasks were learned), and thus much worse than it did in the previous chapter. By contrast, in the visual concepts setting, it performed competitively with the meta-mapping approach, and thus much better than in the previous experiments. What is the cause of this discrepancy? \par 

There are a few possible factors underlying these results. First, because of the structure of the task spaces, the number of training tasks in the visual concepts domain is much larger. The language model seems to need more training tasks than HoMM to begin to generalize well. However, as noted above, there is another factor that may be driving the differences between the results in this chapter. The training tasks in the RL setting more directly contradict the evaluation tasks, and there is no task that is close to the evaluation tasks. By contrast, in the visual concepts domain the sytem will have encountered many training concepts that overlap with the held-out evaluation in one attribute. Indeed, when we evaluated the effects of sample size, our task sampling scheme ensured that there would be a training concept closely matched to every evaluation concept. It is possible that the HoMM model generalizes better to tasks farther outside the space of its experience than the language model does. \par  

In fact, we observed in the RL domain that the HoMM model was exhibiting an intriguing signature of generalizing more systematically than the language model. Generalization on the two held-out tasks was more tightly correlated in the HoMM model than in the language model. %In the visual concepts domain, the HoMM model was resulting in perfect performance on a larger proportion of the runs at moderate sample sizes.
%Both of these results are
This result is suggestive of the more systematic generalization (or systematic failure to generalize appropriately) that humans tend to exhibit. \par

It's also worth reflecting at this point on the results of \citet{Hill2019a}, in which we showed that more realistic environments improve language model generalization. The realism of the environments is matchedi between models in this setting, and I tried to incorporate features that we showed improve generalization (such as egocentric perspective on the RL tasks). However, it will be important to evaluate both types of models in more realistic settings in the future. Ideally, both classes of models would perform even better in more realistic settings. \par

In summary, the HoMM model is promising in more realistic and complex settings that are more representative of the challenges that humans (and modern AI systems) face. It does not seem to require unreasonably large sets of training tasks in these settings, and may even adapt more systematically than the language-based model. But what can we do with this adaptation? In the next chapter, we explore one key idea: zero-shot adaptation provides a good starting-point for later learning. 



