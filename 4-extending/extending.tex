\chapter{Extending meta-mapping to more complex tasks} \label{chapter:extending}
In the previous chapters we have demonstrated the succes of meta-mapping in two simple domains. While these have allowed us to demonstrate the efficacy of the approach relative to other baselines, and compare its adaptation to that of humans, there are several reasons to extend beyond these to more complex tasks. \par 
First, the approach relies on several pieces which it is not obvious would scale to more complex settings, such as representing an entire task with a single vector, parameterizing the task network via a HyperNetwork conditioned on this vector, and learning meta-mappings from relatively few task examples. If any of these fails to extend to more complex settings, it could limit the applications of our approach. \par
Second, there are limitations to toy experiments. While toy experiments can provide carefully controlled demonstrations of an idea, we have shown in other work that more systematic generalization can emerge when agents are placed in more realistic settings \citep{Hill2019a}. This may impact both the meta-mapping approach and the language baseline, so it is important to evaluate the effects of richer environments on both. This will help inform us as to whether our approach will be useful in more complex settings. (Unfortunately, creating truly realistic environments and training agents in them requires complex implementations and substantial computational resources. Thus, in this chapter we demonstrate our results in environments of moderate complexity, and leave the extension to even richer environments to future work.)\par 
To address these motivations, in this chapter we present experiments on extending our ideas to two important settings: reinforcement learning and classification from raw pixel inputs. These settings are important both because they are dominant paradigms for applying deep learning, and because they have deep connections to cognitive modelling \citep[e.g.][]{Yamins2014, Kriegeskorte2015, Momennejad2017}. \par

\section{Reinforcement learning}

Reinforcement learning is an interesting (and challenging) application for meta-mapping for several reasons. First, reinforcement learning has deep roots in neuroscience, and appears to relate to some computations in the brain \citep{Sutton2017, Niv2009, Odoherty2003}. Second, reinforcement learning has achieved impressive human-level performance on complex tasks such as Atari games \citep{Mni2015}, Go \citep{Silver2016, Silver2017}, and complex video games like Dota 2 \citep{OpenAI2019}, and Starcraft II \citep{Vinyals2019}. This motivates it as an important place to explore more human-like intelligence. Third, there has been a rich vein of research on adaptation in reinforcement learning from language-conditioned models \citep{Hermann2017} to the observation that model-based methods or successor representations can allow for adaptation to environment changes \citep{Daw2014, Momennejad2017}. However, these latter methods assume that a new reward function is given, which requires a substantial portion of the adaptation problem already be solved. Thus, there is substantial room to ask whether meta-mapping can provide good performance in RL tasks, and good motivation for a language baseline. \par 

