@article{Brown2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2005.14165v3},
author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
eprint = {arXiv:2005.14165v3},
file = {:home/andrew/Documents/grad/Papers/2005.14165.pdf:pdf},
journal = {arXiv preprint},
title = {{Language Models are Few-Shot Learners}},
year = {2020}
}
@article{Vankov2019,
author = {Vankov, Ivan I and Bowers, Jeffrey S and Vankov, Ivan I},
file = {:home/andrew/Documents/grad/Papers/rstb.2019.0309.pdf:pdf},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
number = {1791},
title = {{Training neural networks to encode symbols enables combinatorial generalization}},
volume = {375},
year = {2019}
}
@article{Zhuang2020,
author = {Zhuang, Chengxu and Yan, Siming and Nayebi, Aran and Schrimpf, Martin and Frank, Michael C and Dicarlo, James J and Yamins, Daniel L K},
file = {:home/andrew/Documents/grad/Papers/2020.06.16.155556v1.full.pdf:pdf},
journal = {bioRxiv preprint},
title = {{Unsupervised Neural Network Models of the Ventral Visual Stream}},
year = {2020}
}
@article{Rae2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1911.05507v1},
author = {Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Hillier, Chloe and Lillicrap, Timothy P},
eprint = {arXiv:1911.05507v1},
file = {:home/andrew/Documents/grad/Papers/1911.05507.pdf:pdf},
journal = {arXiv preprint},
pages = {1--19},
title = {{Compressive transformers for long range sequence modelling}},
year = {2019}
}
@article{Andrea2006,
author = {Andrea, A and Sherin, Bruce L},
doi = {10.1080/0950069980201002},
file = {:home/andrew/Documents/grad/Papers/What changes in conceptual change.pdf:pdf},
number = {1998},
title = {{What changes in conceptual change?}},
volume = {0693},
year = {2006}
}
@article{Pierrot2019,
author = {Pierrot, Thomas and Sigaud, Olivier and Kas, David and Ligner, Guillaume and Perrin, Nicolas and Beguir, Karim and Reed, Scott},
file = {:home/andrew/Documents/grad/Papers/9608-learning-compositional-neural-programs-with-recursive-tree-search-and-planning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Learning Compositional Neural Programs with Recursive Tree Search and Planning}},
year = {2019}
}
@book{dunlosky2008metacognition,
author = {Dunlosky, John and Metcalfe, Janet},
publisher = {Sage Publications},
title = {{Metacognition}},
year = {2008}
}
@article{McClelland1985,
author = {McClelland, James L},
file = {:home/andrew/Documents/grad/Papers/McClelland85.pdf:pdf},
journal = {Cognition},
pages = {113--146},
title = {{Putting knowledge in its place : A scheme for programming parallel processing structures on the fly}},
volume = {146},
year = {1985}
}
@inproceedings{Lee2020a,
author = {Lee, Hae Beom and Nam, Taewook and Yang, Eunho and Hwang, Sung Ju},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/meta{\_}dropout{\_}learning{\_}to{\_}perturb{\_}latent{\_}features{\_}for{\_}generalization.pdf:pdf},
pages = {1--15},
title = {{Meta-dropout: learning to perturb latent features for generalization}},
year = {2020}
}
@inproceedings{Keysers2020,
author = {Keysers, Daniel and Sch{\"{a}}rli, Nathanael and Scales, Nathan and Buisman, Hylke and Furrer, Daniel and Kashubin, Sergii and Momchev, Nikola and Sinopalnikov, Danila and Stafiniak, Lukasz and Tihon, Tibor and Tsarkov, Dmitry and Wang, Xiao and Zee, Marc Van and Bousquet, Olivier},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/measuring{\_}compositional{\_}generalization{\_}a{\_}comprehensive{\_}method{\_}on{\_}realistic{\_}data.pdf:pdf},
number = {Section 2},
pages = {1--38},
title = {{Measuring compositional generalization: a comprehensive method on realistic data}},
year = {2020}
}
@inproceedings{Tadros2020,
author = {Tadros, Timothy and Ramyaa, Ramyaa and Krishnan, Giri P and Bazhenov, Maxim},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/biologically{\_}inspired{\_}sleep{\_}algorithm{\_}for{\_}increased{\_}generalization{\_}and{\_}adversarial{\_}robustness{\_}in{\_}deep{\_}neural{\_}networks.pdf:pdf},
number = {2013},
pages = {1--23},
title = {{Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks}},
year = {2020}
}
@inproceedings{Gulcehre2020,
author = {Gulcehre, Caglar and Paine, Tom Le and Shahriari, Bobak and Denil, Misha and Hoffman, Matt and Soyer, Hubert and Tanburn, Richard and Kapturowski, Steven and Rabinowitz, Neil and Williams, Duncan and Barth-maron, Gabriel and Wang, Ziyu and Freitas, Nando De and Team, Worlds},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/making{\_}efficient{\_}use{\_}of{\_}demonstrations{\_}to{\_}solve{\_}hard{\_}exploration{\_}problems.pdf:pdf},
pages = {1--20},
title = {{Making efficient use of demonstrations to solve hard exploration problems}},
year = {2020}
}
@inproceedings{Kirsch2020,
author = {Kirsch, Louis and Steenkiste, Sjoerd Van},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/improving{\_}generalization{\_}in{\_}meta{\_}reinforcement{\_}learning{\_}using{\_}learned{\_}objectives.pdf:pdf},
pages = {1--21},
title = {{Improving generalization in meta reinforcement learning using learned objectives}},
volume = {2},
year = {2020}
}
@inproceedings{Khandelwal2020,
author = {Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/generalization{\_}through{\_}memorization{\_}nearest{\_}neighbor{\_}language{\_}models.pdf:pdf},
title = {{Generalization through memorization: nearest neighbor language models}},
year = {2020}
}
@inproceedings{Lee2020,
author = {Lee, Kimin and Lee, Kibok and Shin, Jinwoo and Lee, Honglak},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/network{\_}randomization{\_}a{\_}simple{\_}technique{\_}for{\_}generalization{\_}in{\_}deep{\_}reinforcement{\_}learning.pdf:pdf},
pages = {1--22},
title = {{Network randomization: a simple technique for generalization in deep reinforcement learning}},
year = {2020}
}
@inproceedings{Raghu2020,
author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/rapid{\_}learning{\_}or{\_}feature{\_}reuse{\_}towards{\_}understanding{\_}the{\_}effectiveness{\_}of{\_}maml.pdf:pdf},
pages = {1--21},
title = {{Rapid learning or feature reuse? Towards understanding the effectiveness of MAML}},
year = {2020}
}
@article{Holtzman2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2004.10151v1},
author = {Holtzman, Ari and Thomason, Jesse and Chai, Joyce},
eprint = {arXiv:2004.10151v1},
file = {:home/andrew/Documents/grad/Papers/2004.10151.pdf:pdf},
journal = {arXiv preprint},
title = {{Experience Grounds Language}},
year = {2020}
}
@inproceedings{Wang2020,
author = {Wang, Duo and Jamnik, Mateja and Lio, Pietro},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/abstract{\_}diagrammatic{\_}reasoning{\_}with{\_}multiplex{\_}graph{\_}networks.pdf:pdf},
number = {2015},
pages = {1--20},
title = {{Abstract diagrammatic reasoning with multiplex graph networks}},
year = {2020}
}
@inproceedings{Dhillon2020,
author = {Dhillon, Guneet S and Chaudhari, Pratik and Ravichandran, Avinash and Soatto, Stefano},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/a{\_}baseline{\_}for{\_}few{\_}shot{\_}image{\_}classification.pdf:pdf},
pages = {1--20},
title = {{A baseline for few-shot image classification}},
year = {2020}
}
@article{Lupyan2020a,
author = {Lupyan, Gary and Abdel, Rasha and Boroditsky, Lera and Clark, Andy},
file = {:home/andrew/Documents/grad/Papers/Effects{\_}of{\_}language{\_}on{\_}perception.pdf:pdf},
journal = {PsyArXiv preprint},
keywords = {categorization,language,perception,top-down effects,vision},
pages = {1--24},
title = {{Effects of Language on Visual Perception}},
year = {2020}
}
@article{Merel2019,
author = {Merel, Josh and Botvinick, Matthew and Wayne, Greg},
doi = {10.1038/s41467-019-13239-6},
file = {:home/andrew/Documents/grad/Papers/s41467-019-13239-6.pdf:pdf},
issn = {2041-1723},
journal = {Nature Communications},
pages = {1--12},
publisher = {Springer US},
title = {{Hierarchical motor control in mammals and machines}},
url = {http://dx.doi.org/10.1038/s41467-019-13239-6},
year = {2019}
}
@article{Abrahamse2016,
author = {Abrahamse, Elger and Braem, Senne and Notebaert, Wim and Verguts, Tom},
file = {:home/andrew/Documents/grad/Papers/2016-22436-001.pdf:pdf},
journal = {Psychological Bulletin},
keywords = {attentional control,cognitive control,conflict adaptation,response inhibition,task switching},
number = {7},
pages = {693--728},
title = {{Grounding Cognitive Control in Associative Learning}},
volume = {142},
year = {2016}
}
@article{Zaheer2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.06114v3},
author = {Zaheer, Manzil and Kottur, Satwik and Ravanbhakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander J},
eprint = {arXiv:1703.06114v3},
file = {:home/andrew/Documents/grad/Papers/1703.06114.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Deep Sets}},
year = {2017}
}
@article{Tanenhaus1987,
author = {Tanenhaus, Michael K and Lucas, Margery M},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-0010027787900102-main.pdf:pdf},
journal = {Cognition},
pages = {213--234},
title = {{Context effects in lexical processing}},
volume = {25},
year = {1987}
}
@article{Xu2020a,
author = {Xu, Yaoda},
file = {:home/andrew/Documents/grad/Papers/2020.03.12.989376v1.full.pdf:pdf},
journal = {bioRxiv preprint},
keywords = {2019,analyzing the fmri,and mv-p collecting and,author contributions,cnn,designing the fmri experiments,fmri,here were from two,prior publications,representational similarity,the fmri data used,vaziri-pashkam,vaziri-pashkam et al,visual object representation,with mv-p and yx,xu},
title = {{Limited correspondence in visual representation between the human brain and convolutional neural networks}},
url = {https://doi.org/10.1101/2020.03.12.989376},
year = {2020}
}
@article{Chollet2019,
abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
archivePrefix = {arXiv},
arxivId = {1911.01547},
author = {Chollet, Fran{\c{c}}ois},
eprint = {1911.01547},
file = {:home/andrew/Documents/grad/Papers/1911.01547.pdf:pdf},
journal = {arXiv preprint},
pages = {1--64},
title = {{On the Measure of Intelligence}},
url = {http://arxiv.org/abs/1911.01547},
year = {2019}
}
@article{Andreas2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1711.00482v1},
author = {Andreas, Jacob and Klein, Dan and Levine, Sergey},
eprint = {arXiv:1711.00482v1},
file = {:home/andrew/Documents/grad/Papers/1711.00482.pdf:pdf},
journal = {arXiv preprint},
number = {Figure 1},
title = {{Learning with Latent Language}},
year = {2017}
}
@incollection{sep-compositionality,
author = {Szab{\'{o}}, Zolt{\'{a}}n Gendler},
booktitle = {The Stanford Encyclopedia of Philosophy},
edition = {Summer 2017},
editor = {Zalta, Edward N},
howpublished = {$\backslash$url{\{}https://plato.stanford.edu/archives/sum2017/entries/compositionality/{\}}},
publisher = {Metaphysics Research Lab, Stanford University},
title = {{Compositionality}},
year = {2017}
}
@misc{Sutton2019,
author = {Sutton, Richard},
booktitle = {Incomplete Ideas (blog)},
title = {{The bitter lesson}},
urldate = {2020-03-06},
year = {2019}
}
@article{Wang2019b,
abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
archivePrefix = {arXiv},
arxivId = {1905.00537},
author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
eprint = {1905.00537},
file = {:home/andrew/Documents/grad/Papers/1905.00537.pdf:pdf},
journal = {arXiv preprint},
number = {July},
pages = {1--29},
title = {{SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems}},
url = {http://arxiv.org/abs/1905.00537},
volume = {2019},
year = {2019}
}
@article{Caruana1997,
author = {Caruana, Rich},
doi = {10.1111/j.1468-0319.1995.tb00042.x},
file = {:home/andrew/Documents/grad/Papers/Caruana1997{\_}Article{\_}MultitaskLearning.pdf:pdf},
issn = {14680319},
journal = {Machine Learning},
keywords = {backpropagation,generalization,inductive transfer,k-nearest neighbor,kernel,multitask learning,parallel transfer,regression,supervised learning},
number = {q},
title = {{Multitask learning}},
volume = {28},
year = {1997}
}
@article{Raffel2019,
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.},
archivePrefix = {arXiv},
arxivId = {1910.10683},
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
eprint = {1910.10683},
file = {:home/andrew/Documents/grad/Papers/1910.10683.pdf:pdf},
journal = {arXiv preprint, arXiv:1910.10683},
pages = {1--53},
title = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
url = {http://arxiv.org/abs/1910.10683},
year = {2019}
}
@article{Dujmovic2020,
abstract = {Deep convolutional neural networks (DCNNs) are frequently described as promising models of human and primate vision. An obvious challenge to this claim is the existence of adversarial images that fool DCNNs but are uninterpretable to humans. However, recent research has suggested that there may be similarities in how humans and DCNNs interpret these seemingly nonsense images. In this study, we reanalysed data from a high-profile paper and conducted four experiments controlling for different ways in which these images can be generated and selected. We show that agreement between humans and DCNNs is much weaker and more variable than previously reported, and that the weak agreement is contingent on the choice of adversarial images and the design of the experiment. Indeed, it is easy to generate images with no agreement. We conclude that adversarial images still challenge the claim that DCNNs constitute promising models of human and primate vision.},
author = {Dujmovic, Marin and Malhotra, Gaurav and Bowers, Jeffrey},
doi = {10.1101/2020.02.25.964361},
file = {:home/andrew/Documents/grad/Papers/2020.02.25.964361v1.full.pdf:pdf},
isbn = {10.1101/2020.02.2},
journal = {arXiv preprint},
keywords = {adversarial images,deep neural networks,human vision},
title = {{What do adversarial images tell us about human vision?}},
url = {https://doi.org/10.1101/2020.02.25.964361},
year = {2020}
}
@article{Jarrett2009,
abstract = {In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hard-wired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63{\%} recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6{\%}) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 ({\textgreater} 65{\%}), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53{\%}). {\textcopyright}2009 IEEE.},
author = {Jarrett, Kevin and Kavukcuoglu, Koray and Ranzato, Marc'Aurelio and LeCun, Yann},
doi = {10.1109/ICCV.2009.5459469},
file = {:home/andrew/Documents/grad/Papers/jarrett-iccv-09.pdf:pdf},
isbn = {9781424444205},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2146--2153},
title = {{What is the best multi-stage architecture for object recognition?}},
year = {2009}
}
@article{McClelland2009,
abstract = {I consider the role of cognitive modeling in cognitive science. Modeling, and the computers that enable it, are central to the field, but the role of modeling is often misunderstood. Models are not intended to capture fully the processes they attempt to elucidate. Rather, they are explorations of ideas about the nature of cognitive processes. In these explorations, simplification is essential-through simplification, the implications of the central ideas become more transparent. This is not to say that simplification has no downsides; it does, and these are discussed. I then consider several contemporary frameworks for cognitive modeling, stressing the idea that each framework is useful in its own particular ways. Increases in computer power (by a factor of about 4 million) since 1958 have enabled new modeling paradigms to emerge, but these also depend on new ways of thinking. Will new paradigms emerge again with the next 1,000-fold increase? {\textcopyright} 2009 Cognitive Science Society, Inc.},
author = {McClelland, James L.},
doi = {10.1111/j.1756-8765.2008.01003.x},
file = {:home/andrew/Documents/grad/Papers/j.1756-8765.2008.01003.x.pdf:pdf},
issn = {17568757},
journal = {Topics in Cognitive Science},
keywords = {Bayesian approaches,Cognitive architectures,Computer simulation,Connectionist models,Dynamical systems,Hybrid models,Modeling frameworks,Symbolic models of cognition},
number = {1},
pages = {11--38},
title = {{The Place of Modeling in Cognitive Science}},
volume = {1},
year = {2009}
}
@article{Gontijo-Lopes2020,
abstract = {Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of either distribution shift or augmentation diversity. Inspired by these, we seek to quantify how data augmentation improves model generalization. To this end, we introduce interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.},
archivePrefix = {arXiv},
arxivId = {2002.08973},
author = {Gontijo-Lopes, Raphael and Smullin, Sylvia J. and Cubuk, Ekin D. and Dyer, Ethan},
eprint = {2002.08973},
file = {:home/andrew/Documents/grad/Papers/2002.08973.pdf:pdf},
journal = {arXiv preprint},
title = {{Affinity and Diversity: Quantifying Mechanisms of Data Augmentation}},
url = {http://arxiv.org/abs/2002.08973},
year = {2020}
}
@article{Zadrozny1992,
author = {Zadrozny, Wlodek},
doi = {10.3115/992066.992109},
file = {:home/andrew/Documents/grad/Papers/C92-1042.pdf:pdf},
journal = {Proceedings of the 14th conference on Computational Linguistics},
title = {{On compositional semantics}},
volume = {1},
year = {1992}
}
@article{Cohen1990,
abstract = {Traditional views of automaticity are in need of revision. For example, automaticity often has been treated as an all-or-none phenomenon, and traditional theories have held that automatic processes are independent of attention. Yet recent empirical data suggest that automatic processes are continuous, and furthermore are subject to attentional control. A model of attention is presented to address these issues. Within a parallel distributed processing framework, it is proposed that the attributes of automaticity depend on the strength of a processing pathway and that strength increases with training. With the Stroop effect as an example, automatic processes are shown to be continuous and to emerge gradually with practice. Specifically, a computational model of the Stroop task simulates the time course of processing as well as the effects of learning. This was accomplished by combining the cascade mechanism described by McClelland (1979) with the backpropagation learning algorithm (Rumelhart, Hinton, {\&} Williams, 1986). The model can simulate performance in the standard Stroop task, as well as aspects of performance in variants of this task that manipulate stimulus-onset asynchrony, response set, and degree of practice. The model presented is contrasted against other models, and its relation to many of the central issues in the literature on attention, automaticity, and interference is discussed.},
author = {Cohen, Jonathan D. and Dunbar, Kevin and McClelland, James L.},
doi = {10.1037/0033-295X.97.3.332},
file = {:home/andrew/Downloads/1990-27437-001.pdf:pdf},
issn = {0033295X},
journal = {Psychological Review},
number = {3},
pages = {332--361},
pmid = {2200075},
title = {{On the control of automatic processes: A parallel distributed processing account of the stroop effect}},
volume = {97},
year = {1990}
}
@article{Monsell2003,
abstract = {Everyday life requires frequent shifts between cognitive tasks. Research reviewed in this article probes the control processes that reconfigure mental resources for a change of task by requiring subjects to switch frequently among a small set of simple tasks. Subjects' responses are substantially slower and, usually, more error-prone immediately after a task switch. This 'switch cost' is reduced, but not eliminated, by an opportunity for preparation. It seems to result from both transient and long-term carry-over of 'task-set' activation and inhibition as well as time consumed by task-set reconfiguration processes. Neuroimaging studies of task switching have revealed extra activation in numerous brain regions when subjects prepare to change tasks and when they perform a changed task, but we cannot yet separate 'controlling' from 'controlled' regions.},
author = {Monsell, Stephen},
doi = {10.1016/S1364-6613(03)00028-7},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S1364661303000287-main.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {3},
pages = {134--140},
pmid = {12639695},
title = {{Task switching}},
volume = {7},
year = {2003}
}
@article{McClelland2014,
abstract = {In a seminal 1977 article, Rumelhart argued that perception required the simultaneous use of multiple sources of information, allowing perceivers to optimally interpret sensory information at many levels of representation in real time as information arrives. Building on Rumelhart's arguments, we present the Interactive Activation hypothesis-the idea that the mechanism used in perception and comprehension to achieve these feats exploits an interactive activation process implemented through the bidirectional propagation of activation among simple processing units. We then examine the interactive activation model of letter and word perception and the TRACE model of speech perception, as early attempts to explore this hypothesis, and review the experimental evidence relevant to their assumptions and predictions. We consider how well these models address the computational challenge posed by the problem of perception, and we consider how consistent they are with evidence from behavioral experiments. We examine empirical and theoretical controversies surrounding the idea of interactive processing, including a controversy that swirls around the relationship between interactive computation and optimal Bayesian inference. Some of the implementation details of early versions of interactive activation models caused deviation from optimality and from aspects of human performance data. More recent versions of these models, however, overcome these deficiencies. Among these is a model called the multinomial interactive activation model, which explicitly links interactive activation and Bayesian computations. We also review evidence from neurophysiological and neuroimaging studies supporting the view that interactive processing is a characteristic of the perceptual processing machinery in the brain. In sum, we argue that a computational analysis, as well as behavioral and neuroscience evidence, all support the Interactive Activation hypothesis. The evidence suggests that contemporary versions of models based on the idea of interactive activation continue to provide a basis for efforts to achieve a fuller understanding of the process of perception. {\textcopyright} 2014 Cognitive Science Society, Inc.},
author = {McClelland, James L. and Mirman, Daniel and Bolger, Donald J. and Khaitan, Pranav},
doi = {10.1111/cogs.12146},
file = {:home/andrew/Documents/grad/Papers/McCEtAl14IAinPercept+Cog.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Connectionist models,Interactive activation,Neural networks,Optimal perceptual inference,Parallel distributed processing,Perception},
number = {6},
pages = {1139--1189},
title = {{Interactive activation and mutual constraint satisfaction in perception and cognition}},
volume = {38},
year = {2014}
}
@book{fodor2001mind,
author = {Fodor, Jerry A},
publisher = {MIT press},
title = {{The mind doesn't work that way: The scope and limits of computational psychology}},
year = {2001}
}
@book{fodor1975language,
author = {Fodor, Jerry A},
publisher = {Harvard university press},
title = {{The language of thought}},
volume = {5},
year = {1975}
}
@book{fodor2008lot2,
author = {Fodor, Jerry A},
publisher = {Oxford University Press on Demand},
title = {{LOT 2: The language of thought revisited}},
year = {2008}
}
@book{fodor1983modularity,
author = {Fodor, Jerry A},
publisher = {MIT press},
title = {{The modularity of mind}},
year = {1983}
}
@article{Vapnik1971,
author = {Vapnik, Vladimir and Chervonenkis, A.Y.},
file = {:home/andrew/Documents/grad/Papers/1116025.pdf:pdf},
journal = {Theory of Probability and its Applications},
number = {2},
pages = {264--280},
title = {{On the uniform convergence of relative frequencies of events to their probabilities}},
volume = {16},
year = {1971}
}
@article{Potts2019,
abstract = {Pater's (2019) target article builds a persuasive case for establishing stronger ties between theoretical linguistics and connectionism (deep learning). This commentary extends his arguments to semantics, focusing in particular on issues of learning, compositionality, and lexical meaning.},
author = {Potts, Christopher},
doi = {10.1353/lan.2019.0019},
file = {:home/andrew/Documents/grad/Papers/pater-commentary-by-potts.pdf:pdf},
issn = {15350665},
journal = {Language},
keywords = {Compositionality,Connectionism,Deep learning,Lexical semantics,Machine learning,Semantics},
number = {1},
pages = {e115--e124},
title = {{A case for deep learning in semantics: Response to pater}},
volume = {95},
year = {2019}
}
@article{Frank2008,
abstract = {Does speaking a language without number words change the way speakers of that language perceive exact quantities? The Pirah{\~{a}} are an Amazonian tribe who have been previously studied for their limited numerical system [Gordon, P. (2004). Numerical cognition without words: Evidence from Amazonia. Science 306, 496-499]. We show that the Pirah{\~{a}} have no linguistic method whatsoever for expressing exact quantity, not even "one." Despite this lack, when retested on the matching tasks used by Gordon, Pirah{\~{a}} speakers were able to perform exact matches with large numbers of objects perfectly but, as previously reported, they were inaccurate on matching tasks involving memory. These results suggest that language for exact number is a cultural invention rather than a linguistic universal, and that number words do not change our underlying representations of number but instead are a cognitive technology for keeping track of the cardinality of large sets across time, space, and changes in modality. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Frank, Michael C. and Everett, Daniel L. and Fedorenko, Evelina and Gibson, Edward},
doi = {10.1016/j.cognition.2008.04.007},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S0010027708001042-main.pdf:pdf},
issn = {00100277},
journal = {Cognition},
keywords = {Cross-cultural research,Indigenous peoples,Language and thought,Number},
number = {3},
pages = {819--824},
title = {{Number as a cognitive technology: Evidence from Pirah{\~{a}} language and cognition}},
volume = {108},
year = {2008}
}
@article{Frank2004,
abstract = {The Pirah , an Amazonian hunter-gatherer tribe, lack words a for numbers and are unable to complete simple matching tasks when the tasks require memory for exact quantities (Gordon, 2004; Frank et al., in press). Here we show that American par- ticipants perform similarly to the Pirah when asked to execute a the same kinds of matching tasks under verbal interference. These results provide support for the hypothesis that number words act as a cognitive technology: a method for quickly and efficiently storing information via abstraction. We review a variety of other evidence supporting this proposal from the domains of color, navigation, and theory of mind.},
author = {Frank, Michael C and Fedorenko, Evelina and Gibson, Edward},
doi = {10.1038/nprot.2010.66},
file = {:home/andrew/Documents/grad/Papers/qt7nm3w9k1.pdf:pdf},
issn = {17542189},
journal = {Methods},
keywords = {guage thought,lan,numerical cognition,pirah,verbal interference,whorf hypothesis},
number = {6},
pages = {439--444},
title = {{Language as a cognitive technology: English-speakers match like Pirah{\~{a}} when you don't let them count}},
url = {http://csjarchive.cogsci.rpi.edu/Proceedings/2008/pdfs/p439.pdf},
volume = {5},
year = {2004}
}
@book{Gleitman1970,
author = {Gleitman, Lilah R and Gleitman, Henry},
title = {{Phrase and paraphrase: Some innovative uses of language}},
year = {1970}
}
@article{Lake2019a,
abstract = {People learn in fast and flexible ways that have not been emulated by machines. Once a person learns a new verb "dax," he or she can effortlessly understand how to "dax twice," "walk and dax," or "dax vigorously." There have been striking recent improvements in machine learning for natural language processing, yet the best algorithms require vast amounts of experience and struggle to generalize new concepts in compositional ways. To better understand these distinctively human abilities, we study the compositional skills of people through language-like instruction learning tasks. Our results show that people can learn and use novel functional concepts from very few examples (few-shot learning), successfully applying familiar functions to novel inputs. People can also compose concepts in complex ways that go beyond the provided demonstrations. Two additional experiments examined the assumptions and inductive biases that people make when solving these tasks, revealing three biases: mutual exclusivity, one-to-one mappings, and iconic concatenation. We discuss the implications for cognitive modeling and the potential for building machines with more human-like language learning capabilities.},
archivePrefix = {arXiv},
arxivId = {1901.04587},
author = {Lake, Brenden M. and Linzen, Tal and Baroni, Marco},
eprint = {1901.04587},
file = {:home/andrew/Documents/grad/Papers/1901.04587.pdf:pdf},
journal = {arXiv preprint},
keywords = {compositionality,concept learning,ing,neural networks,word learn-},
title = {{Human few-shot learning of compositional instructions}},
url = {http://arxiv.org/abs/1901.04587},
year = {2019}
}
@article{Hasson2020,
author = {Hasson, Uri and Nastase, Samuel A and Goldstein, Ariel},
file = {:home/andrew/Documents/grad/Papers/764258v2.full.pdf:pdf},
journal = {Neuron},
keywords = {evolution,experimental design,learning,neural networks},
number = {3},
pages = {416--434},
title = {{Direct fit to nature: an evolutionary perspective on biological (and artificial) neural networks}},
volume = {105},
year = {2020}
}
@article{Mu2019,
abstract = {Language is designed to convey useful information about the world, thus serving as a scaffold for efficient human learning. How can we let language guide representation learning in machine learning models? We explore this question in the setting of few-shot visual classification, proposing models which learn to perform visual classification while jointly predicting natural language task descriptions at train time. At test time, with no language available, we find that these language-influenced visual representations are more generalizable, compared to meta-learning baselines and approaches that explicitly use language as a bottleneck for classification.},
archivePrefix = {arXiv},
arxivId = {1911.02683},
author = {Mu, Jesse and Liang, Percy and Goodman, Noah},
eprint = {1911.02683},
file = {:home/andrew/Documents/grad/Papers/1911.02683.pdf:pdf},
journal = {Visually Grounded Interaction and Language Workshop, NeurIPS},
title = {{Shaping visual representations with language for few-shot classification}},
url = {http://arxiv.org/abs/1911.02683},
year = {2019}
}
@article{McClelland2019,
abstract = {Language is central to human intelligence. We review recent breakthroughs in machine language processing and consider what remains to be achieved. Recent approaches rely on domain general principles of learning and representation captured in artificial neural networks. Most current models, however, focus too closely on language itself. In humans, language is part of a larger system for acquiring, representing, and communicating about objects and situations in the physical and social world, and future machine language models should emulate such a system. We describe existing machine models linking language to concrete situations, and point toward extensions to address more abstract cases. Human language processing exploits complementary learning systems, including a deep neural network-like learning system that learns gradually as machine systems do, as well as a fast-learning system that supports learning new information quickly. Adding such a system to machine language models will be an important further step toward truly human-like language understanding.},
archivePrefix = {arXiv},
arxivId = {1912.05877},
author = {McClelland, James L. and Hill, Felix and Rudolph, Maja and Baldridge, Jason and Sch{\"{u}}tze, Hinrich},
eprint = {1912.05877},
file = {:home/andrew/Documents/grad/Papers/1912.05877.pdf:pdf},
journal = {arXiv preprint},
pages = {1--8},
title = {{Extending machine language models toward human-level language understanding}},
url = {http://arxiv.org/abs/1912.05877},
year = {2019}
}
@article{McClelland2020,
author = {McClelland, James L. and McNaughton, Bruce L. and Lampinen, Andrew K.},
file = {:home/andrew/Documents/grad/Papers/2020.01.17.909804v1.full.pdf:pdf},
journal = {Proceedings of the Royal Society B},
pages = {1--34},
title = {{Integration of new information in memory: new insights from a complementary learning systems perspective}},
year = {2020}
}
@article{Goodman2008,
abstract = {This article proposes a new model of human concept learning that provides a rational analysis of learning feature-based concepts. This model is built upon Bayesian inference for a grammatically structured hypothesis space - a concept language of logical rules. This article compares the model predictions to human generalization judgments in several well-known category learning experiments, and finds good agreement for both average and individual participant generalizations. This article further investigates judgments for a broad set of 7-feature concepts - a more natural setting in several ways - and again finds that the model explains human performance.},
author = {Goodman, Noah A. and Tenenbaum, Joshua B. and Feldman, Jacob and Griffiths, Thomas L.},
doi = {10.1080/03640210701802071},
file = {:home/andrew/Documents/grad/Papers/03640210701802071.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Bayesian induction,Categorization,Concept learning,Language of thought,Probabilistic grammar,Rules},
number = {1},
pages = {108--154},
title = {{A rational analysis of rule-based concept learning}},
volume = {32},
year = {2008}
}
@article{Medin1978,
abstract = {Most theories dealing with ill-defined concepts assume that performance is based on category level information or a mixture of category level and specific item information. A context theory of classification is described in which judgments are assumed to derive exclusively from stored exemplar information. The main idea is that a probe item acts as a retrieval cue to access information associated with stimuli similar to the probe. The predictions of the context theory are contrasted with those of a class of theories (including prototype theory) that assume that the information entering into judgments can be derived from an additive combination of information from component cue dimensions. Across 4 experiments with 128 paid Ss, using both geometric forms and schematic faces as stimuli, the context theory consistently gave a better account of the data. The relation of context theory to other theories and phenomena associated with ill-defined concepts is discussed in detail. (42 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1978 American Psychological Association.},
author = {Medin, Douglas L. and Schaffer, Marguerite M.},
doi = {10.1037/0033-295X.85.3.207},
file = {:home/andrew/Documents/grad/Papers/154ef2e19d4733bfac30f04ed708f01b42d1.pdf:pdf},
issn = {0033295X},
journal = {Psychological Review},
keywords = {test of context theory vs other theories of ill-defined concepts, classification learning, 17-30 yr olds},
number = {3},
pages = {207--238},
title = {{Context theory of classification learning}},
volume = {85},
year = {1978}
}
@article{Dabney2020,
abstract = {Twenty years ago, a link was discovered between the neurotransmitter dopamine and 11 the computational framework of reinforcement learning. Since then, it has become well 12 established that dopamine release reflects a reward prediction error, a surprise signal that 13 drives learning of reward predictions and shapes future behavior. According to the now 14 canonical theory, reward predictions are represented as a single scalar quantity, which 15 supports learning about the expectation, or mean, of stochastic outcomes. In the present 16 work, we propose a novel account of dopamine-based reinforcement learning, reporting 17 experimental results which point to a significant modification of the standard reward 18 prediction error theory. Inspired by recent artificial intelligence research on distributional 19 reinforcement learning, we hypothesized that the brain represents possible future rewards 20 not as a single mean, but instead as a probability distribution, effectively representing 21 multiple future outcomes simultaneously and in parallel. This idea leads immediately to 22 a set of empirical predictions, which we tested using single-unit recordings from mouse 23 ventral tegmental area. Our findings provide strong evidence for a neural realization of 24},
author = {Dabney, Will and Kurth-nelson, Zeb and Uchida, Naoshige and Starkweather, Clara Kwon and Hassabis, Demis and Munos, R{\'{e}}mi and Botvinick, Matthew},
doi = {10.1038/s41586-019-1924-6},
file = {:home/andrew/Documents/grad/Papers/s41586-019-1924-6.pdf:pdf},
journal = {Nature},
number = {January},
pages = {1--32},
title = {{A distributional code for value in dopamine-based reinforcement learning}},
year = {2020}
}
@article{ODoherty2003,
abstract = {Temporal difference learning has been proposed as a model for Pavlovian conditioning, in which an animal learns to predict delivery of reward following presentation of a conditioned stimulus (CS). A key component of this model is a prediction error signal, which, before learning, responds at the time of presentation of reward but, after learning, shifts its response to the time of onset of the CS. In order to test for regions manifesting this signal profile, subjects were scanned using event-related fMRI while undergoing appetitive conditioning with a pleasant taste reward. Regression analyses revealed that responses in ventral striatum and orbitofrontal cortex were significantly correlated with this error signal, suggesting that, during appetitive conditioning, computations described by temporal difference learning are expressed in the human brain.},
author = {O'Doherty, John P. and Dayan, Peter and Friston, Karl and Critchley, Hugo and Dolan, Raymond J.},
doi = {10.1016/S0896-6273(03)00169-7},
file = {:home/andrew/Documents/grad/Papers/PIIS0896627303001697.pdf:pdf},
issn = {08966273},
journal = {Neuron},
number = {2},
pages = {329--337},
pmid = {12718865},
title = {{Temporal difference models and reward-related learning in the human brain}},
volume = {38},
year = {2003}
}
@article{Niv2009,
abstract = {diss},
author = {Niv, Yael},
file = {:home/andrew/Documents/grad/Papers/Niv2009.pdf:pdf},
journal = {Journal of Mathematical Psychology},
title = {{Reinforcement learning in the brain}},
year = {2009}
}
@article{OpenAI2019,
abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
archivePrefix = {arXiv},
arxivId = {1912.06680},
author = {OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\c{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'{o}}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique Pond{\'{e}} de Oliveira and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
eprint = {1912.06680},
file = {:home/andrew/Documents/grad/Papers/1912.06680.pdf:pdf},
journal = {arXiv preprint},
title = {{Dota 2 with Large Scale Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1912.06680},
year = {2019}
}
@article{Vinyals2019,
abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1â€“3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"{e}}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'{e}}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"{u}}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
doi = {10.1038/s41586-019-1724-z},
file = {:home/andrew/Documents/grad/Papers/Vinyals{\_}et{\_}al-2019-Nature.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7782},
pages = {350--354},
pmid = {31666705},
publisher = {Springer US},
title = {{Grandmaster level in StarCraft II using multi-agent reinforcement learning}},
url = {http://dx.doi.org/10.1038/s41586-019-1724-z},
volume = {575},
year = {2019}
}
@article{Daw2014,
abstract = {Despite many debates in the first half of the twentieth century, it is now largely a truism that humans and other animals build models of their environments and use them for prediction and control. However, model-based (MB) reasoning presents severe computational challenges. Alternative, computationally simpler, model-free (MF) schemes have been suggested in the reinforcement learning literature, and have afforded influential accounts of behavioural and neural data. Here, we study the realization of MB calculations, and the ways that this might be woven together with MF values and evaluation methods. There are as yet mostly only hints in the literature as to the resulting tapestry, so we offer more preview than review.},
author = {Daw, Nathaniel D. and Dayan, Peter},
doi = {10.1098/rstb.2013.0478},
file = {:home/andrew/Documents/grad/Papers/rstb.2013.0478.pdf:pdf},
isbn = {0000000334761},
issn = {14712970},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
keywords = {Model-based reasoning,Model-free reasoning,Monte Carlo tree search,Orbitofrontal cortex,Reinforcement learning,Striatum},
number = {1655},
title = {{The algorithmic anatomy of model-based evaluation}},
volume = {369},
year = {2014}
}
@article{Turchetta2016,
abstract = {In classical reinforcement learning agents accept arbitrary short term loss for long term gain when exploring their environment. This is infeasible for safety critical applications such as robotics, where even a single unsafe action may cause system failure or harm the environment. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an a priori unknown safety constraint that depends on states and actions and satisfies certain regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm, SAFEMDP, for this task and prove that it completely explores the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.},
archivePrefix = {arXiv},
arxivId = {1606.04753},
author = {Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
eprint = {1606.04753},
file = {:home/andrew/Documents/grad/Papers/6358-safe-exploration-in-finite-markov-decision-processes-with-gaussian-processes.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {4312--4320},
title = {{Safe exploration in finite Markov decision processes with Gaussian processes}},
year = {2016}
}
@article{Turchetta2019,
abstract = {In Interactive Machine Learning (IML), we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods, e.g., Bayesian optimization and active learning, have been successful in applications, on real-world systems they must provably avoid unsafe decisions. To this end, safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient, as it explores decisions that are not relevant for the original IML objective. In this paper, we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result, we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP), which have been analyzed separately before. Our method outperforms other algorithms empirically.},
archivePrefix = {arXiv},
arxivId = {1910.13726},
author = {Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
eprint = {1910.13726},
file = {:home/andrew/Documents/grad/Papers/1910.13726.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Safe Exploration for Interactive Machine Learning}},
url = {http://arxiv.org/abs/1910.13726},
year = {2019}
}
@book{Sutton2017,
author = {Sutton, Richard S and Barto, Andrew G},
file = {:home/andrew/Documents/grad/Papers/bookdraft2017nov5.pdf:pdf},
title = {{Reinforcement learning: An introduction}},
year = {2017}
}
@article{Fodor2001,
author = {Fodor, Jerry A.},
doi = {10.1111/1468-0017.00153},
file = {:home/andrew/Documents/grad/Papers/Fodor-2001-Mind{\_}{\&}{\_}Language.pdf:pdf},
issn = {02681064},
journal = {Mind and Language},
number = {1},
pages = {1--15},
title = {{Language, thought and compositionality}},
volume = {16},
year = {2001}
}
@incollection{Goldberg2015,
author = {Goldberg, Adele E.},
booktitle = {Routledge Handbook of Semantics},
file = {:home/andrew/Documents/grad/Papers/Compositionality-Routledge.pdf:pdf},
pages = {419--433},
title = {{Compositionality}},
year = {2015}
}
@article{Lupyan2020,
author = {Lupyan, Gary and Zettersten, Martin},
file = {:home/andrew/Documents/grad/Papers/lupyan{\_}zettersten{\_}nameability.pdf:pdf},
journal = {PsyArXiv preprint},
title = {{Does vocabulary help structure the mind?}},
year = {2020}
}
@article{Clune2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.10985v1},
author = {Clune, Jeff},
eprint = {arXiv:1905.10985v1},
file = {:home/andrew/Documents/grad/Papers/1905.10985.pdf:pdf},
journal = {arXiv preprint},
title = {{AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence}},
year = {2019}
}
@article{Velez2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.07241v3},
author = {Velez, Roby and Clune, Jeff},
eprint = {arXiv:1705.07241v3},
file = {:home/andrew/Documents/grad/Papers/1705.07241.pdf:pdf},
journal = {PLoS ONE},
title = {{Diffusion-based neuromodulation can eliminate catastrophic forgetting in simple neural networks}},
volume = {12},
year = {2017}
}
@article{Miconi2019,
author = {Miconi, Thomas and Rawal, Aditya and Clune, Jeff and Stanley, Kenneth O},
file = {:home/andrew/Documents/grad/Papers/backpropamine{\_}training{\_}self{\_}modifying{\_}neural{\_}networks{\_}with{\_}differentiable{\_}neuromodulated{\_}plasticity.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--15},
title = {{Backpropamine: Training self-modifying neural networks with differentiable neuromodulated plasticity}},
year = {2019}
}
@article{Nagabandi2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1812.07671v2},
author = {Nagabandi, Anusha and Finn, Chelsea and Levine, Sergey},
eprint = {arXiv:1812.07671v2},
file = {:home/andrew/Documents/grad/Papers/1812.07671.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--15},
title = {{Deep online learning via meta-learning: Continual adaptation for model-based RL}},
year = {2019}
}
@article{Ven2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1904.07734v1},
author = {Ven, Gido M Van De and Tolias, Andreas S},
eprint = {arXiv:1904.07734v1},
file = {:home/andrew/Documents/grad/Papers/1904.07734.pdf:pdf},
journal = {NeurIPS Continual Learning Workshop},
pages = {1--18},
title = {{Three scenarios for continual learning}},
year = {2018}
}
@article{Oswald2020,
archivePrefix = {arXiv},
arxivId = {arXiv:1906.00695v2},
author = {Oswald, Johannes Von and Henning, Christian and Sacramento, Jo{\~{a}}o and Grewe, Benjamin F},
eprint = {arXiv:1906.00695v2},
file = {:home/andrew/Documents/grad/Papers/1906.00695.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--25},
title = {{Continual learning with hypernetworks}},
year = {2020}
}
@article{Stroop1935,
abstract = {In this study pairs of conflicting stimuli, both being inherent aspects of the same symbols, were presented simultaneously (a name of one color printed in the ink of another color--a word stimulus and a color stimulus). The difference in time for reading the words printed in colors and the same words printed in black is the measure of the interference of color stimuli upon reading words. The difference in the time for naming the colors in which the words are printed and the same colors printed in squares (or swastikas) is the measure of the interference of conflicting word stimuli upon naming colors. The interference of conflicting color stimuli upon the time for reading 100 words (each word naming a color unlike the ink-color of its print) caused an increase of 2.3 seconds or 5.6{\%} over the normal time for reading the same words printed in black. This increase is not reliable. But the interference of conflicting word stimuli upon the time for naming 100 colors (each color being the print of a word which names another color) caused an increase of 47.0 seconds or 74.3{\%} of the normal time for naming colors printed in squares. Tests on the permanency of the interference of conflicting word stimuli are also described. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1935 American Psychological Association.},
author = {Stroop, J. R.},
doi = {10.1037/h0054651},
file = {:home/andrew/Downloads/1936-01863-001.pdf:pdf},
issn = {00221015},
journal = {Journal of Experimental Psychology},
keywords = {ATTENTION, MEMORY AND THOUGHT,INTERFERENCE, VERBAL REACTION},
number = {6},
pages = {643--662},
title = {{Studies of interference in serial verbal reactions}},
volume = {18},
year = {1935}
}
@article{MacLeod1991,
abstract = {The literature on interference in the Stroop Color-Word Task, covering over 50 years some 400 studies, is organized and reviewed. In so doing, a set of 18 reliable empirical findings is isolated that must be captured by any successful theory of the Stroop effect. Existing theoretical positions are summarized and evaluated in, view of this critical evidence and the 2 major candidate theories-relative speed of processing and automaticity of reading-are found to be wanting, it is concluded that recent theories placing the explanatory weight on parallel processing of the irrelevant and the relevant dimensions are likely to be more successful than are earlier theories attempting to locate a single bottleneck in attention.},
author = {MacLeod, Colin M.},
doi = {10.1037/0033-2909.109.2.163},
file = {:home/andrew/Documents/grad/Papers/MacLeod{\_}1991{\_}Half.pdf:pdf},
issn = {00332909},
journal = {Psychological Bulletin},
number = {2},
pages = {163--203},
pmid = {2034749},
title = {{Half a century of reseach on the stroop effect: An integrative review}},
volume = {109},
year = {1991}
}
@article{Jarvstad2013,
abstract = {Classical studies suggest that high-level cognitive decisions (e.g., choosing between financial options) are suboptimal. In contrast, low-level decisions (e.g., choosing where to put your feet on a rocky ridge) appear near-optimal: the perception-cognition gap. Moreover, in classical tasks, people appear to put too much weight on unlikely events. In contrast, when people can learn through experience, they appear to put too little weight on unlikely events: the description-experience gap. We eliminated confounding factors and, contrary to what is commonly believed, found results suggesting that (i) the perception-cognition gap is illusory and due to differences in the way performance is assessed; (ii) the description- experience gap arises from the assumption that objective probabilities match subjective ones; (iii) people's ability to make decisions is better than the classical literature suggests; and (iv) differences between decision-makers are more important for predicting peoples' choices than differences between choice tasks.},
author = {Jarvstad, Andreas and Hahn, Ulrike and Rushton, Simon K. and Warren, Paul A.},
doi = {10.1073/pnas.1300239110},
file = {:home/andrew/Documents/grad/Papers/16271.full.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {40},
pages = {16271--16276},
title = {{Perceptuo-motor, cognitive, and description-based decision-making seem equally good}},
volume = {110},
year = {2013}
}
@article{Rogers1995,
abstract = {In an investigation of task-set reconfiguration, participants switched between 2 tasks on every 2nd trial in 5 experiments and on every 4th trial in a final experiment. The tasks were to classify either the digit member of a pair of characters as even/odd or the letter member as consonant/vowel. As the response-stimulus interval increased up to 0.6 s, the substantial cost to performance of this predictable task-switch fell: Participants could partially reconfigure in advance of the stimulus. However, even with 1.2 s available for preparation, a large asymptotic reaction time (RT) cost remained, but only on the 1st trial of the new task. This is attributed to a component of reconfiguration triggered exogenously, i. e., only by a task-relevant stimulus. That stimuli evoke associated task-sets also explains why RT and switch costs increased when the stimulus included a character associated with the currently irrelevant task. {\textcopyright} 1995 American Psychological Association.},
author = {Rogers, Robert D. and Monsell, Stephen},
doi = {10.1037/0096-3445.124.2.207},
file = {:home/andrew/Documents/grad/Papers/Costs{\_}of{\_}a{\_}Predictable{\_}Switch{\_}Between{\_}Simple{\_}Cogni.pdf:pdf},
issn = {00963445},
journal = {Journal of Experimental Psychology: General},
number = {2},
pages = {207--231},
title = {{Costs of a Predictable Switch Between Simple Cognitive Tasks}},
volume = {124},
year = {1995}
}
@article{Such2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1912.07768v1},
author = {Such, Felipe Petroski and Rawal, Aditya and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
eprint = {arXiv:1912.07768v1},
file = {:home/andrew/Documents/grad/Papers/1912.07768.pdf:pdf},
journal = {arXiv preprint},
pages = {1--26},
title = {{Generative teaching networks: accelerating neural architecture search by learning to generate synthetic training data}},
year = {2019}
}
@article{Salimans2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.07868v3},
author = {Salimans, Tim and Kingma, Diederik P},
eprint = {arXiv:1602.07868v3},
file = {:home/andrew/Documents/grad/Papers/1602.07868.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks}},
year = {2016}
}
@article{Mollica2020,
author = {Mollica, Francis and Diachek, Evgeniia and Mineroff, Zachary and Kean, Hope and Siegelman, Matthew and Piantadosi, Steven T and Futrell, Richard},
file = {:home/andrew/Documents/grad/Papers/436204v2.full.pdf:pdf},
journal = {Neurobiology of Language},
pages = {1--28},
title = {{Composition is the core driver of the language-selective network}},
year = {2020}
}
@article{Xu2020,
author = {Xu, Yilun and Zhao, Shengjia and Song, Jiaming and Stewart, Russell and Ermon, Stefano},
file = {:home/andrew/Documents/grad/Papers/a{\_}theory{\_}of{\_}usable{\_}information{\_}under{\_}computational{\_}constraints.pdf:pdf},
journal = {International Conference on Learning Representations (ICLR)},
pages = {1--22},
title = {{A theory of usable information under computational constraints}},
year = {2020}
}
@article{Veeriah2019,
author = {Veeriah, Vivek and Hessel, Matteo and Xu, Zhongwen and Lewis, Richard and Rajendran, Janarthanan and Oh, Junhyuk and van Hasselt, Hado and Silver, David and Singh, Satinder},
file = {:home/andrew/Documents/grad/Papers/9129-discovery-of-useful-questions-as-auxiliary-tasks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {1--12},
title = {{Discovery of Useful Questions as Auxiliary Tasks}},
year = {2019}
}
@article{Madarasz2019,
author = {Madarasz, Tamas J and Behrens, Timothy E},
file = {:home/andrew/Documents/grad/Papers/9104-better-transfer-learning-with-inferred-successor-maps.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
title = {{Better transfer learning with inferred successor maps}},
year = {2019}
}
@article{Wang2019a,
author = {Wang, Boyu and Cai, Ming Bo and Mendez, Jorge A and Eaton, Eric},
file = {:home/andrew/Documents/grad/Papers/9249-transfer-learning-via-minimizing-the-performance-gap-between-domains.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
title = {{Transfer Learning via Minimizing the Performance Gap Between Domains}},
year = {2019}
}
@article{Yu2019,
author = {Yu, Hyeonwoo and Lee, Beomhee},
file = {:home/andrew/Documents/grad/Papers/8300-zero-shot-learning-via-simultaneous-generating-and-learning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
title = {{Zero-shot Learning via Simultaneous Generating and Learning}},
year = {2019}
}
@article{Ni2019,
author = {Ni, Jian and Zhang, Shanghang and Xie, Haiyong},
file = {:home/andrew/Documents/grad/Papers/8846-dual-adversarial-semantics-consistent-network-for-generalized-zero-shot-learning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {1--12},
title = {{Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot Learning}},
year = {2019}
}
@article{Davis2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1912.05752v2},
author = {Davis, Ernest},
eprint = {arXiv:1912.05752v2},
file = {:home/andrew/Documents/grad/Papers/1912.05752.pdf:pdf},
journal = {arXiv preprint},
pages = {1--7},
title = {{The Use of Deep Learning for Symbolic Integration: A Review of (Lample and Charton , 2019)}},
year = {2019}
}
@article{Lample2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1912.01412v1},
author = {Lample, Guillaume and Charton, Francois},
eprint = {arXiv:1912.01412v1},
file = {:home/andrew/Documents/grad/Papers/1912.01412.pdf:pdf},
journal = {arXiv preprint},
pages = {1--24},
title = {{Deep learning for symbolic mathematics}},
year = {2019}
}
@article{Perez2018,
abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
archivePrefix = {arXiv},
arxivId = {1709.07871},
author = {Perez, Ethan and Strub, Florian and {De Vries}, Harm and Dumoulin, Vincent and Courville, Aaron},
eprint = {1709.07871},
file = {:home/andrew/Documents/grad/Papers/1709.07871.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
pages = {3942--3951},
title = {{FiLM: Visual reasoning with a general conditioning layer}},
year = {2018}
}
@article{Ramanujan2019,
abstract = {Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever training the weight values. Hidden in a randomly weighted Wide ResNet-50 we show that there is a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 trained on ImageNet. Not only do these "untrained subnetworks" exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an "untrained subnetwork" approaches a network with learned weights in accuracy.},
archivePrefix = {arXiv},
arxivId = {1911.13299},
author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
eprint = {1911.13299},
file = {:home/andrew/Documents/grad/Papers/1911.13299.pdf:pdf},
journal = {arXiv preprint},
title = {{What's Hidden in a Randomly Weighted Neural Network?}},
url = {http://arxiv.org/abs/1911.13299},
year = {2019}
}
@article{Hooker2019,
abstract = {Neural network pruning techniques have demonstrated it is possible to remove the majority of weights in a network with surprisingly little degradation to test set accuracy. However, this measure of performance conceals significant differences in how different classes and images are impacted by pruning. We find that certain examples, which we term pruning identified exemplars (PIEs), and classes are systematically more impacted by the introduction of sparsity. Removing PIE images from the test-set greatly improves top-1 accuracy for both pruned and non-pruned models. These hard-to-generalize-to images tend to be mislabelled, of lower image quality, depict multiple objects or require fine-grained classification. These findings shed light on previously unknown trade-offs, and suggest that a high degree of caution should be exercised before pruning is used in sensitive domains.},
archivePrefix = {arXiv},
arxivId = {1911.05248},
author = {Hooker, Sara and Courville, Aaron and Dauphin, Yann and Frome, Andrea},
eprint = {1911.05248},
file = {:home/andrew/Documents/grad/Papers/1911.05248.pdf:pdf},
journal = {arXiv preprint},
pages = {1--19},
title = {{Selective Brain Damage: Measuring the Disparate Impact of Model Pruning}},
url = {http://arxiv.org/abs/1911.05248},
year = {2019}
}
@article{Hung2019,
abstract = {Humans prolifically engage in mental time travel. We dwell on past actions and experience satisfaction or regret. More than storytelling, these recollections change how we act in the future and endow us with a computationally important ability to link actions and consequences across spans of time, which helps address the problem of long-term credit assignment: the question of how to evaluate the utility of actions within a long-duration behavioral sequence. Existing approaches to credit assignment in AI cannot solve tasks with long delays between actions and consequences. Here, we introduce a paradigm where agents use recall of specific memories to credit past actions, allowing them to solve problems that are intractable for existing algorithms. This paradigm broadens the scope of problems that can be investigated in AI and offers a mechanistic account of behaviors that may inspire models in neuroscience, psychology, and behavioral economics.},
author = {Hung, Chia Chun and Lillicrap, Timothy and Abramson, Josh and Wu, Yan and Mirza, Mehdi and Carnevale, Federico and Ahuja, Arun and Wayne, Greg},
doi = {10.1038/s41467-019-13073-w},
file = {:home/andrew/Documents/grad/Papers/s41467-019-13073-w.pdf:pdf},
issn = {20411723},
journal = {Nature Communications},
number = {1},
pages = {1--12},
publisher = {Springer US},
title = {{Optimizing agent behavior over long time scales by transporting value}},
url = {http://dx.doi.org/10.1038/s41467-019-13073-w},
volume = {10},
year = {2019}
}
@article{Levinson1996,
abstract = {This review describes some recent, unexpected findings concerning variation in spatial language across cultures, and places them in the context of the general anthropology of space on the one hand, and theories of spatial cognition in the cognitive sciences on the other. There has been much concern with the symbolism of space in anthropological writings, but little on concepts of space in practical activities. This neglect of everyday spatial notions may be due to unwitting ethnocentrism, the assumption in Western thinking generally that notions of space are universally of a single kind. Recent work shows that systems of spatial reckoning and description can in fact be quite divergent across cultures, linguistic differences correlating with distinct cognitive tendencies. This unexpected cultural variation raises interesting questions concerning the relation between cultural and linguistic concepts and the biological foundations of cognition. It argues for more sophisticated models relating culture and cognition than we currently have available.},
author = {Levinson, Stephen C.},
doi = {10.1146/annurev.anthro.25.1.353},
file = {:home/andrew/Documents/grad/Papers/annurev.anthro.25.1.353.pdf:pdf},
issn = {0084-6570},
journal = {Annual Review of Anthropology},
keywords = {cognition and language,linguistic relativity,space},
number = {1},
pages = {353--382},
title = {{Language and Space}},
volume = {25},
year = {1996}
}
@article{Norouzi2014,
abstract = {Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional n-way classification framing of image understanding, particularly in terms of the promise for zero-shot learning â€“ the ability to correctly annotate images of previously unseen object categories. In this paper, we propose a simple method for constructing an image embedding system from any existing n-way image classifier and a semantic word embedding model, which contains the n class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional training. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.},
archivePrefix = {arXiv},
arxivId = {1312.5650},
author = {Norouzi, Mohammad and Mikolov, Tomas and Bengio, Samy and Singer, Yoram and Shlens, Jonathon and Frome, Andrea and Corrado, Greg S. and Dean, Jeffrey},
eprint = {1312.5650},
file = {:home/andrew/Documents/grad/Papers/1312.5650.pdf:pdf},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
pages = {1--9},
title = {{Zero-shot learning by convex combination of semantic embeddings}},
year = {2014}
}
@article{Changpinyo2016,
abstract = {Given semantic descriptions of object classes, zeroshot learning aims to accurately recognize objects of the unseen classes, from which no examples are available at the training stage, by associating them to the seen classes, from which labeled examples are provided. We propose to tackle this problem from the perspective of manifold learning. Our main idea is to align the semantic space that is derived from external information to the model space that concerns itself with recognizing visual features. To this end, we introduce a set of 'phantom' object classes whose coordinates live in both the semantic space and the model space. Serving as bases in a dictionary, they can be optimized from labeled data such that the synthesized real object classifiers achieve optimal discriminative performance. We demonstrate superior accuracy of our approach over the state of the art on four benchmark datasets for zero-shot learning, including the full ImageNet Fall 2011 dataset with more than 20,000 unseen classes.},
archivePrefix = {arXiv},
arxivId = {1603.00550},
author = {Changpinyo, Soravit and Chao, Wei Lun and Gong, Boqing and Sha, Fei},
doi = {10.1109/CVPR.2016.575},
eprint = {1603.00550},
file = {:home/andrew/Documents/grad/Papers/1603.00550.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {5327--5336},
title = {{Synthesized classifiers for zero-shot learning}},
volume = {2016-December},
year = {2016}
}
@article{Rao2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1910.14481v1},
author = {Rao, Dushyant and Visin, Francesco and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
eprint = {arXiv:1910.14481v1},
file = {:home/andrew/Documents/grad/Papers/1910.14481.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Continual Unsupervised Representation Learning}},
year = {2019}
}
@article{Parisi2019,
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
archivePrefix = {arXiv},
arxivId = {1802.07569},
author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
doi = {10.1016/j.neunet.2019.01.012},
eprint = {1802.07569},
file = {:home/andrew/Documents/grad/Papers/1802.07569.pdf:pdf},
isbn = {9781450362078},
issn = {18792782},
journal = {Neural Networks},
keywords = {Catastrophic forgetting,Continual learning,Developmental systems,Lifelong learning,Memory consolidation},
pages = {54--71},
title = {{Continual lifelong learning with neural networks: A review}},
volume = {113},
year = {2019}
}
@article{Henderson2018,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
eprint = {1709.06560},
file = {:home/andrew/Documents/grad/Papers/1709.06560.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
pages = {3207--3214},
title = {{Deep reinforcement learning that matters}},
year = {2018}
}
@article{Oh2017a,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.05064v2},
author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet},
eprint = {arXiv:1706.05064v2},
file = {:home/andrew/Documents/grad/Papers/1706.05064.pdf:pdf},
journal = {Proceedings of the 34th International Conference on Machine Learning},
title = {{Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning}},
year = {2017}
}
@article{Pal2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1903.01092v1},
author = {Pal, Arghya and Balasubramanian, Vineeth N},
eprint = {arXiv:1903.01092v1},
file = {:home/andrew/Documents/grad/Papers/1903.01092.pdf:pdf},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Zero-Shot Task Transfer}},
year = {2019}
}
@article{Yao2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.05301v1},
author = {Yao, Huaxiu and Wei, Ying and Huang, Junzhou and Li, Zhenhui},
eprint = {arXiv:1905.05301v1},
file = {:home/andrew/Documents/grad/Papers/1905.05301.pdf:pdf},
journal = {Proceedings of the 36th International Conference on Machine Learning},
title = {{Hierarchically Structured Meta-learning}},
year = {2019}
}
@article{Platanios2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1808.08493v1},
author = {Platanios, Emmanouil Antonios and Sachan, Mrinmaya and Neubig, Graham and Mitchell, Tom M},
eprint = {arXiv:1808.08493v1},
file = {:home/andrew/Documents/grad/Papers/1808.08493.pdf:pdf},
journal = {arXiv preprint},
title = {{Contextual Parameter Generation for Universal Neural Machine Translation}},
year = {2017}
}
@article{Socher2013,
author = {Socher, Richard and Ganjoo, Milind and Manning, Christopher D. and Ng, Andrew Y.},
file = {:home/andrew/Documents/grad/Papers/5027-zero-shot-learning-through-cross-modal-transfer.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Zero-shot learning through cross-modal transfer}},
year = {2013}
}
@article{Xian2018,
abstract = {Due to the importance of zero-shot learning, i.e. classifying images where there is a lack of labeled training data, the number of proposed approaches has recently increased steadily. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits of publicly available datasets used for this task. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Moreover, we propose a new zero-shot learning dataset, the Animals with Attributes 2 (AWA2) dataset which we make publicly available both in terms of image features and the images themselves. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss in detail the limitations of the current status of the area which can be taken as a basis for advancing it.},
archivePrefix = {arXiv},
arxivId = {arXiv:1707.00600v3},
author = {Xian, Yongqin and Lampert, Christoph H. and Schiele, Bernt and Akata, Zeynep},
doi = {10.1109/TPAMI.2018.2857768},
eprint = {arXiv:1707.00600v3},
file = {:home/andrew/Documents/grad/Papers/1707.00600.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Generalized Zero-shot Learning,Image classification,Transductive Learning,Weakly-Supervised Learning},
pages = {1--14},
title = {{Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly}},
year = {2018}
}
@article{Gregor2016,
abstract = {In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.},
archivePrefix = {arXiv},
arxivId = {1611.07507},
author = {Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan},
eprint = {1611.07507},
file = {:home/andrew/Documents/grad/Papers/1611.07507.pdf:pdf},
journal = {arXiv preprint},
pages = {1--15},
title = {{Variational Intrinsic Control}},
url = {http://arxiv.org/abs/1611.07507},
year = {2016}
}
@article{Romera-Paredes2015,
abstract = {{\textcopyright} Copyright 2015 by International Machine Learning Society (IMLS). All rights reserved. Zero-shot learning consists in learning how to recognise new concepts by just having a description of them. Many sophisticated approaches have been proposed to address the challenges this problem comprises. In this paper we describe a zero-shot learning approach that can be implemented in just one line of code, yet it is able to outperform state of the art approaches on standard datasets. The approach is based on a more general framework which models the relationships between features, attributes, and classes as a two linear layers network, where the weights of the top layer are not learned but are given by the environment. We further provide a learning bound on the generalisation error of this kind of approaches, by casting them as domain adaptation methods. In experiments carried out on three standard real datasets, we found that our approach is able to perform significantly better than the state of art on all of them, obtaining a ratio of improvement up to 17{\%}.},
author = {Romera-Paredes, Bernardino and Torr, Philip H.S.},
file = {:home/andrew/Documents/grad/Papers/romera-paredes15.pdf:pdf},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
pages = {2142--2151},
title = {{An embarrassingly simple approach to zero-shot learning}},
volume = {3},
year = {2015}
}
@article{Santoro2019,
abstract = {Brette contends that the neural coding metaphor is an invalid basis for theories of what the brain does. Here, we argue that it is an insufficient guide for building an artificial intelligence that learns to accomplish short- and long-term goals in a complex, changing environment.},
archivePrefix = {arXiv},
arxivId = {1904.10396},
author = {Santoro, Adam and Hill, Felix and Barrett, David and Raposo, David and Botvinick, Matthew and Lillicrap, Timothy},
eprint = {1904.10396},
file = {:home/andrew/Documents/grad/Papers/1904.10396.pdf:pdf},
journal = {Behavioral and Brain Sciences},
pages = {1--4},
title = {{Is coding a relevant metaphor for building AI? A commentary on "Is coding a relevant metaphor for the brain?"}},
url = {http://arxiv.org/abs/1904.10396},
year = {2019}
}
@article{Brette2019,
author = {Brette, Romain},
file = {:home/andrew/Documents/grad/Papers/is{\_}coding{\_}a{\_}relevant{\_}metaphor{\_}for{\_}the{\_}brain.pdf:pdf},
journal = {Behavioral and Brain Sciences},
keywords = {aesthetic enjoyment,aesthetic virtues,art schema,emotions,genre effects,interplays of positive and,meaning making,mixed,negative emotions},
pages = {1--44},
title = {{Is coding a relevant metaphor for the brain?}},
year = {2019}
}
@article{Zhou2019,
abstract = {How similar is the human mind to the sophisticated machine-learning systems that mirror its performance? Models of object categorization based on convolutional neural networks (CNNs) have achieved human-level benchmarks in assigning known labels to novel images. These advances promise to support transformative technologies such as autonomous vehicles and machine diagnosis; beyond this, they also serve as candidate models for the visual system itself -- not only in their output but perhaps even in their underlying mechanisms and principles. However, unlike human vision, CNNs can be "fooled" by adversarial examples -- carefully crafted images that appear as nonsense patterns to humans but are recognized as familiar objects by machines, or that appear as one object to humans and a different object to machines. This seemingly extreme divergence between human and machine classification challenges the promise of these new advances, both as applied image-recognition systems and also as models of the human mind. Surprisingly, however, little work has empirically investigated human classification of such adversarial stimuli: Does human and machine performance fundamentally diverge? Or could humans decipher such images and predict the machine's preferred labels? Here, we show that human and machine classification of adversarial stimuli are robustly related: In eight experiments on five prominent and diverse adversarial imagesets, human subjects reliably identified the machine's chosen label over relevant foils. This pattern persisted for images with strong antecedent identities, and even for images described as "totally unrecognizable to human eyes". We suggest that human intuition may be a more reliable guide to machine (mis)classification than has typically been imagined, and we explore the consequences of this result for minds and machines alike.},
author = {Zhou, Zhenglong and Firestone, Chaz},
doi = {10.1038/s41467-019-08931-6},
file = {:home/andrew/Documents/grad/Papers/s41467-019-08931-6.pdf:pdf},
issn = {20411723},
journal = {Nature Communications},
number = {1},
publisher = {Springer US},
title = {{Humans can decipher adversarial images}},
url = {http://dx.doi.org/10.1038/s41467-019-08931-6},
volume = {10},
year = {2019}
}
@article{Siegelman1992,
author = {Siegelman, Hava T and Sontag, Eduardo D},
file = {:home/andrew/Documents/grad/Papers/1992{\_}Siegelmann{\_}COLT.pdf:pdf},
journal = {Proceedings of the fifth annual workshop on computational learning theory},
title = {{On the computational power of neural nets}},
year = {1992}
}
@article{Burgess2016,
abstract = {We consider the task of one-shot learning of visual categories. In this paper we explore a Bayesian procedure for updating a pretrained convnet to classify a novel image category for which data is limited. We decompose this convnet into a fixed feature extractor and softmax classifier. We assume that the target weights for the new task come from the same distribution as the pretrained softmax weights, which we model as a multivariate Gaussian. By using this as a prior for the new weights, we demonstrate competitive performance with state-of-the-art methods whilst also being consistent with 'normal' methods for training deep networks on large data.},
archivePrefix = {arXiv},
arxivId = {1707.05562},
author = {Burgess, Jordan and Lloyd, James Robert and Ghahramani, Zoubin},
eprint = {1707.05562},
file = {:home/andrew/Documents/grad/Papers/1707.05562.pdf:pdf},
journal = {Workshop on Bayesian Deep Learning, NIPS 2016},
pages = {3--5},
title = {{One-Shot Learning in Discriminative Neural Networks}},
url = {http://arxiv.org/abs/1707.05562},
year = {2016}
}
@article{Grant2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1801.08930v1},
author = {Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas and Sciences, Computer},
eprint = {arXiv:1801.08930v1},
file = {:home/andrew/Documents/grad/Papers/1801.08930.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--13},
title = {{Recasting gradient-based meta-learning as hierarchical Bayes}},
year = {2018}
}
@article{Barak2013,
abstract = {Intelligent behavior requires integrating several sources of information in a meaningful fashion- be it context with stimulus or shape with color and size. This requires the underlying neural mechanism to respond in a different manner to similar inputs (discrimination), while maintaining a consistent response for noisy variations of the same input (generalization). We show that neurons that mix information sources via random connectivity can form an easy to read representation of input combinations. Using analytical and numerical tools, we show that the coding level or sparseness of these neurons' activity controls a trade-off between generalization and discrimination, with the optimal level depending on the task at hand. In all realistic situations that we analyzed, the optimal fraction of inputs to which a neuron responds is close to 0.1. Finally, we predict a relation between a measurable property of the neural representation and task performance. {\textcopyright} 2013 the authors.},
author = {Barak, Omri and Rigotti, Mattia and Fusi, Stefano},
doi = {10.1523/JNEUROSCI.2753-12.2013},
file = {:home/andrew/Documents/grad/Papers/3844.full.pdf:pdf},
issn = {02706474},
journal = {Journal of Neuroscience},
number = {9},
pages = {3844--3856},
title = {{The sparseness of mixed selectivity neurons controls the generalization-discrimination trade-off}},
volume = {33},
year = {2013}
}
@article{Erhan2010,
author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
file = {:home/andrew/Documents/grad/Papers/erhan10a.pdf:pdf},
issn = {00219258},
journal = {Journal of Machine Learning Research},
title = {{Why does unsupervised pre-training help deep learning?}},
year = {2010}
}
@article{Baranes2013,
abstract = {We introduce the Self-Adaptive Goal Generation Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsically motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters. We present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: (1) learning the inverse kinematics in a highly-redundant robotic arm, (2) learning omnidirectional locomotion with motor primitives in a quadruped robot, and (3) an arm learning to control a fishing rod with a flexible wire. We show that (1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; (2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; (3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.4862v1},
author = {Baranes, Adrien and Oudeyer, Pierre Yves},
doi = {10.1016/j.robot.2012.05.008},
eprint = {arXiv:1301.4862v1},
file = {:home/andrew/Documents/grad/Papers/1301.4862.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Active learning,Autonomous motor learning,Competence based intrinsic motivation,Curiosity-driven task space exploration,Developmental robotics,Goal babbling,Inverse models,Motor development},
number = {1},
pages = {49--73},
title = {{Active learning of inverse models with intrinsically motivated goal exploration in robots}},
volume = {61},
year = {2013}
}
@article{Ravi2017,
abstract = {Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a classifier has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity classifiers requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametriza-tion of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.},
author = {Ravi, Sachin and Larochelle, Hugo},
file = {:home/andrew/Documents/grad/Papers/191257b538cdf09db8808fe926c1ffb2f51db178.pdf:pdf},
journal = {Proceedings of the 5th International Conference on Learning Representations (ICLR 2017)},
pages = {1--11},
title = {{Optimization as a Model for Few-Shot Learning}},
year = {2017}
}
@article{Jaderberg2019,
author = {Jaderberg, Max and Czarnecki, Wojciech M and Dunning, Iain and Marris, Luke and Lever, Guy and Casta{\~{n}}eda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C and Morcos, Ari S and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
file = {:home/andrew/Documents/grad/Papers/859.full.pdf:pdf},
journal = {Science},
number = {May},
pages = {859--865},
title = {{Human-level performance in 3D multiplayer games with population-based reinforcement learning}},
volume = {364},
year = {2019}
}
@article{Graves2017,
abstract = {We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.},
archivePrefix = {arXiv},
arxivId = {arXiv:1704.03003v1},
author = {Graves, Alex and Bellemare, Marc G. and Menick, Jacob and Munos, R{\'{e}}mi and Kavukcuoglu, Koray},
eprint = {arXiv:1704.03003v1},
file = {:home/andrew/Documents/grad/Papers/1704.03003.pdf:pdf},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {2120--2129},
title = {{Automated curriculum learning for neural networks}},
volume = {3},
year = {2017}
}
@inproceedings{Garcia2018,
abstract = {We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on 'relational' tasks.},
archivePrefix = {arXiv},
arxivId = {1711.04043},
author = {Garcia, Victor and Bruna, Joan},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1711.04043},
file = {:home/andrew/Documents/grad/Papers/1711.04043.pdf:pdf},
title = {{Few-Shot Learning with Graph Neural Networks}},
url = {http://arxiv.org/abs/1711.04043},
year = {2018}
}
@article{Lampinen2019a,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.09950v3},
author = {Lampinen, Andrew K and McClelland, James L},
eprint = {arXiv:1905.09950v3},
file = {:home/andrew/Documents/grad/Papers/1905.09950.pdf:pdf},
journal = {NeurIPS Learning Transferable Skills Workshop},
pages = {1--27},
title = {{Zero-shot task adaptation by homoiconic meta-mapping}},
year = {2019}
}
@article{Hill2019a,
abstract = {The question of whether deep neural networks are good at generalising beyond their immediate training experience is of critical importance for learning-based approaches to AI. Here, we demonstrate strong emergent systematic generalisation in a neural network agent and isolate the factors that support this ability. In environments ranging from a grid-world to a rich interactive 3D Unity room, we show that an agent can correctly exploit the compositional nature of a symbolic language to interpret never-seen-before instructions. We observe this capacity not only when instructions refer to object properties (colors and shapes) but also verb-like motor skills (lifting and putting) and abstract modifying operations (negation). We identify three factors that can contribute to this facility for systematic generalisation: (a) the number of object/word experiences in the training set; (b) the invariances afforded by a first-person, egocentric perspective; and (c) the variety of visual input experienced by an agent that perceives the world actively over time. Thus, while neural nets trained in idealised or reduced situations may fail to exhibit a compositional or systematic understanding of their experience, this competence can readily emerge when, like human learners, they have access to many examples of richly varying, multi-modal observations as they learn.},
archivePrefix = {arXiv},
arxivId = {1910.00571},
author = {Hill, Felix and Lampinen, Andrew and Schneider, Rosalia and Clark, Stephen and Botvinick, Matthew and McClelland, James L. and Santoro, Adam},
eprint = {1910.00571},
file = {:home/andrew/Documents/grad/Papers/1910.00571.pdf:pdf},
journal = {International Conference on Learning Representations},
title = {{Environmental drivers of generalization in a situated agent}},
url = {http://arxiv.org/abs/1910.00571},
year = {2020}
}
@article{Racaniere2019,
abstract = {Reinforcement learning algorithms use correlations between policies and rewards to improve agent performance. But in dynamic or sparsely rewarding environments these correlations are often too small, or rewarding events are too infrequent to make learning feasible. Human education instead relies on curricula--the breakdown of tasks into simpler, static challenges with dense rewards--to build up to complex behaviors. While curricula are also useful for artificial agents, hand-crafting them is time consuming. This has lead researchers to explore automatic curriculum generation. Here we explore automatic curriculum generation in rich, dynamic environments. Using a setter-solver paradigm we show the importance of considering goal validity, goal feasibility, and goal coverage to construct useful curricula. We demonstrate the success of our approach in rich but sparsely rewarding 2D and 3D environments, where an agent is tasked to achieve a single goal selected from a set of possible goals that varies between episodes, and identify challenges for future work. Finally, we demonstrate the value of a novel technique that guides agents towards a desired goal distribution. Altogether, these results represent a substantial step towards applying automatic task curricula to learn complex, otherwise unlearnable goals, and to our knowledge are the first to demonstrate automated curriculum generation for goal-conditioned agents in environments where the possible goals vary between episodes.},
archivePrefix = {arXiv},
arxivId = {1909.12892},
author = {Racaniere, Sebastien and Lampinen, Andrew K. and Santoro, Adam and Reichert, David P. and Firoiu, Vlad and Lillicrap, Timothy P.},
eprint = {1909.12892},
file = {:home/andrew/Documents/grad/Papers/1909.12892.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--17},
title = {{Automated curricula through setter-solver interactions}},
url = {http://arxiv.org/abs/1909.12892},
year = {2020}
}
@article{Zintgraf2018,
abstract = {We propose CAVIA for meta-learning, a simple extension to MAML that is less prone to meta-overfitting, easier to parallelise, and more interpretable. CAVIA partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, only the context parameters are updated, leading to a low-dimensional task representation. We show empirically that CAVIA outperforms MAML for regression, classification, and reinforcement learning. Our experiments also highlight weaknesses in current benchmarks, in that the amount of adaptation needed in some cases is small.},
archivePrefix = {arXiv},
arxivId = {1810.03642},
author = {Zintgraf, Luisa M and Shiarlis, Kyriacos and Kurin, Vitaly and Hofmann, Katja and Whiteson, Shimon},
eprint = {1810.03642},
file = {:home/andrew/Documents/grad/Papers/1810.03642.pdf:pdf},
journal = {Proceedings of the 36th International Conference on Machine Learning},
title = {{Fast Context Adaptation via Meta-Learning}},
url = {http://arxiv.org/abs/1810.03642},
year = {2018}
}
@misc{yu2019,
author = {Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Levine, Sergey and Finn, Chelsea},
title = {{Meta-World: A Benchmark and Evaluation for Multi-Task and Meta-Reinforcement Learning}},
url = {https://github.com/rlworkgroup/metaworld},
year = {2019}
}
@article{Drachman2005,
author = {Drachman, David A},
file = {:home/andrew/Documents/grad/Papers/2004.full.pdf:pdf},
journal = {Neurology},
title = {{Do we have brain to spare ?}},
volume = {64},
year = {2005}
}
@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Hinton, Geoffrey E},
file = {:home/andrew/Documents/grad/Papers/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Florensa2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.06366v5},
author = {Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
eprint = {arXiv:1705.06366v5},
file = {:home/andrew/Documents/grad/Papers/1705.06366.pdf:pdf},
journal = {arXiv preprint},
title = {{Automatic goal generation for reinforcement learning agents}},
year = {2018}
}
@incollection{Chalmers2006,
author = {Chalmers, David J},
booktitle = {The re-emergence of emergence},
editor = {Clayton, P and Davies, P},
file = {:home/andrew/Documents/grad/Papers/emergence.pdf:pdf},
title = {{Strong and Weak Emergence}},
year = {2006}
}
@article{Saxe2019,
author = {Saxe, Andrew M and Mcclelland, James L and Ganguli, Surya},
doi = {10.1073/pnas.1820226116},
file = {:home/andrew/Documents/grad/Papers/19.SMGDev.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
title = {{A mathematical theory of semantic development in deep neural networks}},
year = {2019}
}
@inproceedings{Liu2019,
author = {Liu, Yanbin},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf:pdf},
pages = {1--14},
title = {{Learning to propagate labels: transductive propagation network for few-shot learning}},
year = {2019}
}
@inproceedings{Borsa2019,
author = {Borsa, Diana and Quan, John and Mankowitz, Daniel and Hasselt, Hado Van and Silver, David and Schaul, Tom},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/70175fb6b53961f2a1af03e5550716938c3a5158.pdf:pdf},
number = {2017},
pages = {1--24},
title = {{Universal Successor Features Approximators}},
year = {2019}
}
@inproceedings{Fu2019,
author = {Fu, Justin and Korattikara, Anoop and Levine, Sergey and Guadarrama, Sergio},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/f7eeeb6a5b27a7b9c4bcf6481c0810e7b7ac70a7.pdf:pdf},
pages = {1--14},
title = {{From language to goals: inverse reinforcement learning for vision-based instruction following}},
year = {2019}
}
@article{Li2019,
author = {Li, Xiang and Vilnis, Luke and Zhang, Dongxu and Boratko, Michael and McCallum, Andrew},
file = {:home/andrew/Documents/grad/Papers/30f1421b94869cfbadcc02bf5c259dcd2d51501e.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--14},
title = {{Smoothing the geometry of probabilistic box embeddings}},
year = {2019}
}
@inproceedings{Tenney2019,
author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and Mccoy, R Thomas and Kim, Najoung and Durme, Benjamin Van and Bowman, Samuel R and Das, Dipanjan and Pavlick, Ellie},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/212aeed26e7ac38c925bad565473529502ee88e2.pdf:pdf},
pages = {1--17},
title = {{What do you learn from context? Probing for sentence structure in contextualized word representations}},
year = {2019}
}
@inproceedings{Mao2019,
author = {Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum, Joshua B. and Wu, Jiajun},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf:pdf},
pages = {1--28},
title = {{The neuro-symbolic concept learner: interpreting scenes, words, and sentences from natural supervision}},
year = {2019}
}
@article{Czechowski2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1903.00374v2},
author = {Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
eprint = {arXiv:1903.00374v2},
file = {:home/andrew/Documents/grad/Papers/1903.00374.pdf:pdf},
journal = {arXiv preprint},
title = {{Model Based Reinforcement Learning for Atari}},
year = {2019}
}
@inproceedings{Co-Reyes2019,
author = {Co-Reyes, John D and Gupta, Abhishek and Suvansh, Sanjeev and Altieri, Nick and Andreas, Jacob and DeNero, John and Abbeel, Pieter and Levine, Sergey},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/0d2fa487022c6bef09fe6c4993c83b261997cbe4.pdf:pdf},
pages = {1--17},
title = {{Guiding policies with language via meta-learning}},
year = {2019}
}
@inproceedings{Laroche2017,
author = {Laroche, Romain and Barlier, Merwan},
booktitle = {Proceedings of the Thirty First AAAI Conference on Artificial Intelligence},
file = {:home/andrew/Documents/grad/Papers/14315-66874-1-PB.pdf:pdf},
keywords = {Machine Learning Methods},
pages = {2147--2153},
title = {{Transfer Reinforcement Learning with Shared Dynamics}},
year = {2017}
}
@article{Botvinick2019,
author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X and Kurth-nelson, Zeb and Blundell, Charles and Hassabis, Demis},
doi = {10.1016/j.tics.2019.02.006},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S1364661319300610-main (1).pdf:pdf},
issn = {1364-6613},
journal = {Trends in Cognitive Sciences},
pages = {1--15},
publisher = {Elsevier Ltd},
title = {{Reinforcement Learning , Fast and Slow}},
url = {https://doi.org/10.1016/j.tics.2019.02.006},
year = {2019}
}
@inproceedings{Jin2019,
author = {Jin, Wengong and Yang, Kevin and Barzilay, Regina and Jaakkola, Tommi},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/dee24460691863813212530280fc3d4335b0caac.pdf:pdf},
pages = {1--13},
title = {{Learning multimodal graph-to-graph translation for molecular optimization}},
year = {2019}
}
@article{Rabinowitz2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.01320v1},
author = {Rabinowitz, Neil C},
eprint = {arXiv:1905.01320v1},
file = {:home/andrew/Documents/grad/Papers/1905.01320.pdf:pdf},
journal = {arXiv preprint},
title = {{Meta-learners' learning dynamics are unlike learners'}},
year = {2019}
}
@misc{Hlavac2018,
author = {Hlavac, Marek},
title = {{stargazer: Well-Formatted Regression and Summary Statistics Tables}},
year = {2018}
}
@inproceedings{Chang2019a,
abstract = {How can we build a learner that can capture the essence of what makes a hard problem more complex than a simple one, break the hard problem along characteristic lines into smaller problems it knows how to solve, and sequentially solve the smaller problems until the larger one is solved? To work towards this goal, we focus on learning to generalize in a particular family of problems that exhibit compositional and recursive structure: their solutions can be found by composing in sequence a set of reusable partial solutions. Our key idea is to recast the problem of generalization as a problem of learning algorithmic procedures: we can formulate a solution to this family as a sequential decision-making process over transformations between representations. Our formulation enables the learner to learn the structure and parameters of its own computation graph with sparse supervision, make analogies between problems by transforming one problem representation to another, and exploit modularity and reuse to scale to problems of varying complexity. Experiments on solving a variety of multilingual arithmetic problems demonstrate that our method discovers the hierarchical decomposition of a problem into its subproblems, generalizes out of distribution to unseen problem classes, and extrapolates to harder versions of the same problem, yielding a 10-fold reduction in sample complexity compared to a monolithic recurrent neural network.},
archivePrefix = {arXiv},
arxivId = {1807.04640},
author = {Chang, Michael B. and Gupta, Abhishek and Levine, Sergey and Griffiths, Thomas L.},
booktitle = {International Conference on Learning Representations},
eprint = {1807.04640},
file = {:home/andrew/Documents/grad/Papers/1807.04640.pdf:pdf},
pages = {1--23},
title = {{Automatically Composing Representation Transformations as a Means for Generalization}},
url = {http://arxiv.org/abs/1807.04640},
year = {2019}
}
@inproceedings{Ba2016,
abstract = {Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.},
archivePrefix = {arXiv},
arxivId = {1610.06258},
author = {Ba, Jimmy and Hinton, Geoffrey and Mnih, Volodymyr and Leibo, Joel Z. and Ionescu, Catalin},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1610.06258},
file = {:home/andrew/Documents/grad/Papers/1610.06258.pdf:pdf},
pages = {1--10},
title = {{Using Fast Weights to Attend to the Recent Past}},
url = {http://arxiv.org/abs/1610.06258},
year = {2016}
}
@article{Dasgupta2019,
abstract = {Discovering and exploiting the causal structure in the environment is a crucial challenge for intelligent agents. Here we explore whether causal reasoning can emerge via meta-reinforcement learning. We train a recurrent network with model-free reinforcement learning to solve a range of problems that each contain causal structure. We find that the trained agent can perform causal reasoning in novel situations in order to obtain rewards. The agent can select informative interventions, draw causal inferences from observational data, and make counterfactual predictions. Although established formal causal reasoning algorithms also exist, in this paper we show that such reasoning can arise from model-free reinforcement learning, and suggest that causal reasoning in complex settings may benefit from the more end-to-end learning-based approaches presented here. This work also offers new strategies for structured exploration in reinforcement learning, by providing agents with the ability to perform -- and interpret -- experiments.},
archivePrefix = {arXiv},
arxivId = {1901.08162},
author = {Dasgupta, Ishita and Wang, Jane and Chiappa, Silvia and Mitrovic, Jovana and Ortega, Pedro and Raposo, David and Hughes, Edward and Battaglia, Peter and Botvinick, Matthew and Kurth-Nelson, Zeb},
eprint = {1901.08162},
file = {:home/andrew/Documents/grad/Papers/1901.08162.pdf:pdf},
title = {{Causal Reasoning from Meta-reinforcement Learning}},
url = {http://arxiv.org/abs/1901.08162},
year = {2019}
}
@article{Finn2019,
abstract = {A central capability of intelligent systems is the ability to continuously build upon previous experiences to speed up and enhance learning of new tasks. Two distinct research paradigms have studied this question. Meta-learning views this problem as learning a prior over model parameters that is amenable for fast adaptation on a new task, but typically assumes the set of tasks are available together as a batch. In contrast, online (regret based) learning considers a sequential setting in which problems are revealed one after the other, but conventionally train only a single model without any task-specific adaptation. This work introduces an online meta-learning setting, which merges ideas from both the aforementioned paradigms to better capture the spirit and practice of continual lifelong learning. We propose the follow the meta leader algorithm which extends the MAML algorithm to this setting. Theoretically, this work provides an {\$}\backslashmathcal{\{}O{\}}(\backslashlog T){\$} regret guarantee with only one additional higher order smoothness assumption in comparison to the standard online setting. Our experimental evaluation on three different large-scale tasks suggest that the proposed algorithm significantly outperforms alternatives based on traditional online learning approaches.},
archivePrefix = {arXiv},
arxivId = {1902.08438},
author = {Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey},
eprint = {1902.08438},
file = {:home/andrew/Documents/grad/Papers/1902.08438.pdf:pdf},
journal = {arXiv preprint},
title = {{Online Meta-Learning}},
url = {http://arxiv.org/abs/1902.08438},
year = {2019}
}
@article{Achille2019,
abstract = {We introduce a method to provide vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function defined over those labels, we process images through a "probe network" and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and does not require any understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks (e.g., tasks based on classifying different types of plants are similar) We also demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a new task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well. Selecting a feature extractor with task embedding obtains a performance close to the best available feature extractor, while costing substantially less than exhaustively training and evaluating on all available feature extractors.},
archivePrefix = {arXiv},
arxivId = {1902.03545},
author = {Achille, Alessandro and Lam, Michael and Tewari, Rahul and Ravichandran, Avinash and Maji, Subhransu and Fowlkes, Charless and Soatto, Stefano and Perona, Pietro},
eprint = {1902.03545},
file = {:home/andrew/Documents/grad/Papers/1902.03545.pdf:pdf},
journal = {arXiv preprint},
title = {{Task2Vec: Task Embedding for Meta-Learning}},
url = {http://arxiv.org/abs/1902.03545},
year = {2019}
}
@inproceedings{Riemer2019,
author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/93ad46fe7cff088bd67ef50a6ebc39b64b15344b.pdf:pdf},
pages = {1--31},
title = {{Learning to learn without forgetting by maximizing transfer and minimizing interference}},
year = {2019}
}
@inproceedings{Antol2015,
author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
booktitle = {ICCV},
file = {:home/andrew/Documents/grad/Papers/Antol{\_}VQA{\_}Visual{\_}Question{\_}ICCV{\_}2015{\_}paper.pdf:pdf},
title = {{VQA: Visual Question Answering}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=eric{\&}AN=EJ1118875{\&}site=ehost-live},
year = {2015}
}
@article{Hinton1982,
author = {Hinton, Geoffrey E and Plaut, David C},
file = {:home/andrew/Documents/grad/Papers/eacd80458e70c74494eb1b6759b52ff21399.pdf:pdf},
journal = {Proceedings of the 9th Annual Conference of the Cognitive Science Society},
number = {1987},
title = {{Using Fast Weights to Deblur Old Memories}},
year = {1982}
}
@article{Rusu2016,
abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
archivePrefix = {arXiv},
arxivId = {1606.04671},
author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
eprint = {1606.04671},
file = {:home/andrew/Documents/grad/Papers/1606.04671.pdf:pdf},
journal = {arXiv preprint},
title = {{Progressive neural networks}},
url = {http://arxiv.org/abs/1606.04671},
year = {2016}
}
@inproceedings{Jacobsen2019,
author = {Jacobsen, Jorn-Henrik and Behrmann, Jens and Zemel, Richard and Bethge, Matthias},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/bcabe02aeedd350f06c997ab0013889b5e624155.pdf:pdf},
pages = {1--17},
title = {{Excessive invariance causes adversarial vulnerability}},
year = {2019}
}
@inproceedings{Aljundi2019,
author = {Aljundi, Rahaf and Rohrbach, Marcus and Tuytelaars, Tinne},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/64e66d31f9f3a99f2109c369d875ca110f290d6b.pdf:pdf},
pages = {1--17},
title = {{Selfless sequential learning}},
year = {2019}
}
@inproceedings{Hu2019,
author = {Hu, Wenpeng and Lin, Zhou and Liu, Bing and Tao, Chongyang and Tao, Zhengwei and Zhao, Dongyan and Yan, Rui},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/b9aa0b3c43546e6d382decf7fd1b76acb3584250.pdf:pdf},
pages = {1--13},
title = {{Overcoming catastrophic forgetting for continual learning via model adaptation}},
year = {2019}
}
@inproceedings{Chang2019,
author = {Chang, Michael B and Gupta, Abhishek and Levine, Sergey and Griffiths, Thomas L},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/086663869ea0366468eec4c42a0c6eec539e154c.pdf:pdf},
pages = {1--23},
title = {{Automatically composing representation transformations as a means for generalization}},
year = {2019}
}
@inproceedings{Hsu2019,
author = {Hsu, Kyle and Levine, Sergey and Finn, Chelsea},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/20759676d41afefdd112eaf906cfeb80ab088506.pdf:pdf},
title = {{Unsupervised Learning Via Meta-Learning}},
year = {2019}
}
@inproceedings{Zhang2019,
author = {Zhang, Chris and Ren, Mengye and Urtasun, Raquel and Advanced, Uber and Group, Technologies},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/b9c01adccfbaae7fad1c8e5c2db9ea6c19313a0c.pdf:pdf},
number = {2018},
pages = {1--17},
title = {{Graph HyperNetworks for neural architecture search}},
year = {2019}
}
@inproceedings{Nachum2019,
author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/1bb1b57d88a73d614370d0d3fb6cd75e6c5cab0e.pdf:pdf},
pages = {1--18},
title = {{Near-optimal representation learning for hierarchical reinforcement learning}},
year = {2019}
}
@inproceedings{Cao2019,
author = {Cao, Shengcao and Wang, Xiaofang and Kitani, Kris M},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/b96481e350451fe4d2d428a29a6ea6272af23ab2.pdf:pdf},
pages = {1--17},
title = {{Learnable Embedding Space for Efficient Neural Architecture Compression}},
year = {2019}
}
@inproceedings{Chevalier-Boisvert2019,
author = {Chevalier-Boisvert, Maxime and Bahdanau, Dzmitry and Lahlou, Salem and Willems, Lucas and Chitwan, Saharia and Nguyen, Thien Huu and Bengio, Yoshua},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/b2de8f043a7e7ae5e5a81c626e7b40ca447fc2a7.pdf:pdf},
title = {{BabyAI: A platform to study the sample efficiency of grounded language learning}},
year = {2019}
}
@article{Parisotto2019,
abstract = {State-of-the-art meta reinforcement learning algorithms typically assume the setting of a single agent interacting with its environment in a sequential manner. A negative side-effect of this sequential execution paradigm is that, as the environment becomes more and more challenging, and thus requiring more interaction episodes for the meta-learner, it needs the agent to reason over longer and longer time-scales. To combat the difficulty of long time-scale credit assignment, we propose an alternative parallel framework, which we name "Concurrent Meta-Reinforcement Learning" (CMRL), that transforms the temporal credit assignment problem into a multi-agent reinforcement learning one. In this multi-agent setting, a set of parallel agents are executed in the same environment and each of these "rollout" agents are given the means to communicate with each other. The goal of the communication is to coordinate, in a collaborative manner, the most efficient exploration of the shared task the agents are currently assigned. This coordination therefore represents the meta-learning aspect of the framework, as each agent can be assigned or assign itself a particular section of the current task's state space. This framework is in contrast to standard RL methods that assume that each parallel rollout occurs independently, which can potentially waste computation if many of the rollouts end up sampling the same part of the state space. Furthermore, the parallel setting enables us to define several reward sharing functions and auxiliary losses that are non-trivial to apply in the sequential setting. We demonstrate the effectiveness of our proposed CMRL at improving over sequential methods in a variety of challenging tasks.},
archivePrefix = {arXiv},
arxivId = {1903.02710},
author = {Parisotto, Emilio and Ghosh, Soham and Yalamanchi, Sai Bhargav and Chinnaobireddy, Varsha and Wu, Yuhuai and Salakhutdinov, Ruslan},
eprint = {1903.02710},
file = {:home/andrew/Documents/grad/Papers/1903.02710.pdf:pdf},
title = {{Concurrent Meta Reinforcement Learning}},
url = {http://arxiv.org/abs/1903.02710},
year = {2019}
}
@inproceedings{Wang2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.00103v7},
author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
booktitle = {International Conference on Learning Representations},
doi = {10.1007/s11695-007-9059-5},
eprint = {arXiv:1709.00103v7},
file = {:home/andrew/Documents/grad/Papers/e661931af788bb41220e35ab989a6e051b9e602b.pdf:pdf},
isbn = {0960-8923 (Print)},
issn = {0960-8923},
pmid = {17546839},
title = {{GLUE: A multi-task benchmark and analysis platform for natural language understanding}},
year = {2019}
}
@article{Hafner2018,
abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this problem using a latent dynamics model with both deterministic and stochastic transition components and a multi-step variational inference objective that we call latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
archivePrefix = {arXiv},
arxivId = {1811.04551},
author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
eprint = {1811.04551},
file = {:home/andrew/Documents/grad/Papers/1811.04551.pdf:pdf},
title = {{Learning Latent Dynamics for Planning from Pixels}},
url = {http://arxiv.org/abs/1811.04551},
year = {2018}
}
@article{Agrawal2018,
abstract = {Advances in Deep Reinforcement Learning have led to agents that perform well across a variety of sensory-motor domains. In this work, we study the setting in which an agent must learn to generate programs for diverse scenes conditioned on a given symbolic instruction. Final goals are specified to our agent via images of the scenes. A symbolic instruction consistent with the goal images is used as the conditioning input for our policies. Since a single instruction corresponds to a diverse set of different but still consistent end-goal images, the agent needs to learn to generate a distribution over programs given an instruction. We demonstrate that with simple changes to the reinforced adversarial learning objective, we can learn instruction conditioned policies to achieve the corresponding diverse set of goals. Most importantly, our agent's stochastic policy is shown to more accurately capture the diversity in the goal distribution than a fixed pixel-based reward function baseline. We demonstrate the efficacy of our approach on two domains: (1) drawing MNIST digits with a paint software conditioned on instructions and (2) constructing scenes in a 3D editor that satisfies a certain instruction.},
archivePrefix = {arXiv},
arxivId = {1812.00898},
author = {Agrawal, Aishwarya and Malinowski, Mateusz and Hill, Felix and Eslami, Ali and Vinyals, Oriol and Kulkarni, Tejas},
eprint = {1812.00898},
file = {:home/andrew/Documents/grad/Papers/1812.00898.pdf:pdf},
title = {{Generating Diverse Programs with Instruction Conditioned Reinforced Adversarial Learning}},
url = {http://arxiv.org/abs/1812.00898},
year = {2018}
}
@article{Atkinson2018,
abstract = {In general, neural networks are not currently capable of learning tasks in a sequential fashion. When a novel, unrelated task is learnt by a neural network, it substantially forgets how to solve previously learnt tasks. One of the original solutions to this problem is pseudo-rehearsal, which involves learning the new task while rehearsing generated items representative of the previous task/s. This is very effective for simple tasks. However, pseudo-rehearsal has not yet been successfully applied to very complex tasks because in these tasks it is difficult to generate representative items. We accomplish pseudo-rehearsal by using a Generative Adversarial Network to generate items so that our deep network can learn to sequentially classify the CIFAR-10, SVHN and MNIST datasets. After training on all tasks, our network loses only 1.67{\%} absolute accuracy on CIFAR-10 and gains 0.24{\%} absolute accuracy on SVHN. Our model's performance is a substantial improvement compared to the current state of the art solution.},
archivePrefix = {arXiv},
arxivId = {1802.03875},
author = {Atkinson, Craig and McCane, Brendan and Szymanski, Lech and Robins, Anthony},
eprint = {1802.03875},
file = {:home/andrew/Documents/grad/Papers/1802.03875.pdf:pdf},
journal = {arXiv preprint},
title = {{Pseudo-Recursal: Solving the Catastrophic Forgetting Problem in Deep Neural Networks}},
url = {http://arxiv.org/abs/1802.03875},
year = {2018}
}
@inproceedings{Zenke2017,
abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.},
archivePrefix = {arXiv},
arxivId = {1703.04200},
author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
eprint = {1703.04200},
file = {:home/andrew/Documents/grad/Papers/17.intelligentsynapses.pdf:pdf},
title = {{Continual Learning Through Synaptic Intelligence}},
url = {http://arxiv.org/abs/1703.04200},
year = {2017}
}
@article{Cobbe2018,
abstract = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
archivePrefix = {arXiv},
arxivId = {1812.02341},
author = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
eprint = {1812.02341},
file = {:home/andrew/Documents/grad/Papers/1812.02341.pdf:pdf},
title = {{Quantifying Generalization in Reinforcement Learning}},
url = {http://arxiv.org/abs/1812.02341},
year = {2018}
}
@article{Franco2011,
abstract = {Statistical learning is assumed to occur automatically and implicitly, but little is known about the extent to which the representations acquired over training are available to conscious awareness. In this study, we focus on whether the knowledge acquired in a statistical learning situation is available to conscious control. Participants were first exposed to an artificial language presented auditorily. Immediately thereafter, they were exposed to a second artificial language. Both languages were composed of the same corpus of syllables and differed only in the transitional probabilities. We first determined that both languages were equally learnable (Experiment 1) and that participants could learn the two languages and differentiate between them (Experiment 2). Then, in Experiment 3, we used an adaptation of the Process-Dissociation Procedure (Jacoby, 1991) to explore whether participants could consciously manipulate the acquired knowledge. Results suggest that statistical information can be used to parse and differentiate between two different artificial languages, and that the resulting representations are available to conscious control.},
author = {Franco, Ana and Cleeremans, Axel and Destrebecqz, Arnaud},
doi = {10.3389/fpsyg.2011.00229},
file = {:home/andrew/Documents/grad/Papers/fpsyg-02-00229.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Consciousness,Implicit learning,Process-dissociation procedure,Statistical learning},
number = {SEP},
pages = {1--12},
title = {{Statistical learning of two artificial languages presented successively: How conscious?}},
volume = {2},
year = {2011}
}
@article{Barrett2018,
abstract = {Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation `regimes' in which the training and test data differ in clearly-defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.},
archivePrefix = {arXiv},
arxivId = {1807.04225},
author = {Barrett, David G. T. and Hill, Felix and Santoro, Adam and Morcos, Ari S. and Lillicrap, Timothy},
eprint = {1807.04225},
file = {:home/andrew/Documents/grad/Papers/1807.04225.pdf:pdf},
title = {{Measuring abstract reasoning in neural networks}},
url = {http://arxiv.org/abs/1807.04225},
year = {2018}
}
@article{Saxton2019,
abstract = {Mathematical reasoning-a core ability within human intelligence-presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test splits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.},
archivePrefix = {arXiv},
arxivId = {arXiv:1904.01557v1},
author = {Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
eprint = {arXiv:1904.01557v1},
file = {:home/andrew/Documents/grad/Papers/1904.01557.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--17},
title = {{Analysing Mathematical Reasoning Abilities of Neural Models}},
year = {2019}
}
@article{Bahdanau2018,
abstract = {Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurationsâ€”and for instructionsâ€”not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and under- specified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.00781v2},
author = {Bahdanau, Dzmitry and Hill, Felix and Grefenstette, Edward},
eprint = {arXiv:1803.00781v2},
file = {:home/andrew/Documents/grad/Papers/02befcff74b4bc496cdf3fb1d52cb7a29597fade.pdf:pdf},
keywords = {autonomous goal setting,deep neural network,diversity,exploration,learning,unsupervised},
pages = {1--26},
title = {{Learning To Understand Goal Specifications By Modelling Reward}},
url = {https://arxiv.org/pdf/1803.00781.pdf},
year = {2018}
}
@article{Lake2019,
abstract = {Three years ago, we released the Omniglot dataset for developing more human-like learning algorithms. Omniglot is a one-shot learning challenge, inspired by how people can learn a new concept from just one or a few examples. Along with the dataset, we proposed a suite of five challenge tasks and a computational model based on probabilistic program induction that addresses them. The computational model, although powerful, was not meant to be the final word on Omniglot; we hoped that the machine learning community would both build on our work and develop novel approaches to tackling the challenge. In the time since, we have been pleased to see the wide adoption of Omniglot and notable technical progress. There has been genuine progress on one-shot classification, but it has been difficult to measure since researchers have adopted different splits and training procedures that make the task easier. The other four tasks, while essential components of human conceptual understanding, have received considerably less attention. We review the progress so far and conclude that neural networks are still far from human-like concept learning on Omniglot, a challenge that requires performing all of the tasks with a single model. We also discuss new tasks to stimulate further progress.},
archivePrefix = {arXiv},
arxivId = {1902.03477},
author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
eprint = {1902.03477},
file = {:home/andrew/Documents/grad/Papers/1902.03477.pdf:pdf},
title = {{The Omniglot Challenge: A 3-Year Progress Report}},
url = {http://arxiv.org/abs/1902.03477},
year = {2019}
}
@article{Zoph2016,
abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
archivePrefix = {arXiv},
arxivId = {1611.01578},
author = {Zoph, Barret and Le, Quoc V.},
eprint = {1611.01578},
file = {:home/andrew/Documents/grad/Papers/1611.01578.pdf:pdf},
journal = {arXiv preprint},
pages = {1--16},
title = {{Neural Architecture Search with Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.01578},
year = {2016}
}
@article{Rusu2019,
abstract = {Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space.},
archivePrefix = {arXiv},
arxivId = {1807.05960},
author = {Rusu, Andrei A. and Rao, Dushyant and Sygnowski, Jakub and Vinyals, Oriol and Pascanu, Razvan and Osindero, Simon and Hadsell, Raia},
eprint = {1807.05960},
file = {:home/andrew/Documents/grad/Papers/1807.05960.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--17},
title = {{Meta-Learning with Latent Embedding Optimization}},
url = {http://arxiv.org/abs/1807.05960},
year = {2019}
}
@article{Racaniere2017,
abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
archivePrefix = {arXiv},
arxivId = {arXiv:1707.06203v2},
author = {Racani{\`{e}}re, S{\'{e}}bastien and Weber, Th{\'{e}}ophane and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo and Badia, Adria Puigdom{\`{e}}nech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
eprint = {arXiv:1707.06203v2},
file = {:home/andrew/Documents/grad/Papers/1707.06203.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {5691--5702},
title = {{Imagination-augmented agents for deep reinforcement learning}},
volume = {2017-Decem},
year = {2017}
}
@article{LaurensvanderMaaten2008,
abstract = {We present a new technique called â€œt-SNEâ€ that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence theway in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
archivePrefix = {arXiv},
arxivId = {1307.1662},
author = {{Laurens van der Maaten} and Hinton, Geoffrey},
doi = {10.1007/s10479-011-0841-3},
eprint = {1307.1662},
file = {:home/andrew/Documents/grad/Papers/vandermaaten08a.pdf:pdf},
isbn = {1532-4435},
issn = {02624079},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
pmid = {20652508},
title = {{Visualizing Data using t-SNE}},
volume = {9},
year = {2008}
}
@article{Guez2019,
abstract = {The field of reinforcement learning (RL) is facing increasingly challenging domains with combinatorial complexity. For an RL agent to address these challenges, it is essential that it can plan effectively. Prior work has typically utilized an explicit model of the environment, combined with a specific planning algorithm (such as tree search). More recently, a new family of methods have been proposed that learn how to plan, by providing the structure for planning via an inductive bias in the function approximator (such as a tree structured neural network), trained end-to-end by a model-free RL algorithm. In this paper, we go even further, and demonstrate empirically that an entirely model-free approach, without special structure beyond standard neural network components such as convolutional networks and LSTMs, can learn to exhibit many of the characteristics typically associated with a model-based planner. We measure our agent's effectiveness at planning in terms of its ability to generalize across a combinatorial and irreversible state space, its data efficiency, and its ability to utilize additional thinking time. We find that our agent has many of the characteristics that one might expect to find in a planning algorithm. Furthermore, it exceeds the state-of-the-art in challenging combinatorial domains such as Sokoban and outperforms other model-free approaches that utilize strong inductive biases toward planning.},
archivePrefix = {arXiv},
arxivId = {1901.03559},
author = {Guez, Arthur and Mirza, Mehdi and Gregor, Karol and Kabra, Rishabh and Racani{\`{e}}re, S{\'{e}}bastien and Weber, Th{\'{e}}ophane and Raposo, David and Santoro, Adam and Orseau, Laurent and Eccles, Tom and Wayne, Greg and Silver, David and Lillicrap, Timothy},
eprint = {1901.03559},
file = {:home/andrew/Documents/grad/Papers/1901.03559.pdf:pdf},
journal = {arXiv preprint},
title = {{An investigation of model-free planning}},
url = {http://arxiv.org/abs/1901.03559},
year = {2019}
}
@article{Wolpert1996,
abstract = {This is the first of two papers that use off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are â€œas manyâ€ targets (or priors over targets) for which A has lower expected OTS error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is â€œanti-cross-validationâ€ (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your OTS error is small. Other implications for â€œmembership queriesâ€ algorithms and â€œpuntingâ€ algorithms are also discussed.},
author = {Wolpert, David H.},
doi = {10.1162/neco.1996.8.7.1391},
file = {:home/andrew/Documents/grad/Papers/10.1.1.390.9412.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
number = {7},
pages = {1391--1420},
title = {{The Lack of a Priori Distinctions between Learning Algorithms}},
volume = {8},
year = {1996}
}
@article{Gregor2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1806.03107v3},
author = {Gregor, Karol and Papamakarios, George and Besse, Frederic and Buesing, Lars and Weber, Th{\'{e}}ophane},
eprint = {arXiv:1806.03107v3},
file = {:home/andrew/Documents/grad/Papers/1806.03107.pdf:pdf},
journal = {arXiv preprint},
pages = {1--17},
title = {{Temporal difference variational auto-encoder}},
year = {2018}
}
@article{Rosch1976,
author = {Rosch, Eleanor and Mervis, Carolyn B. and Gray, Wayne D. and Johnson, David M. and Boyes-Braem, Penny},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-001002857690013X-main.pdf:pdf},
journal = {Cognitive Psychology},
pages = {382--439},
title = {{Basic Objects in Natural categories}},
volume = {8},
year = {1976}
}
@article{Talvitie2014,
abstract = {When an imperfect model is used to generate sample rollouts, its errors tend to compound - a flawed sample is given as input to the model, which causes more errors, and so on. This presents a barrier to applying rollout-based planning algorithms to learned models. To address this issue, a training methodology called "hallucinated replay" is introduced, which adds samples from the model into the training data, thereby training the model to produce sensible predictions when its own samples are given as input. Capabilities and limitations of this approach are studied empirically. In several examples hallucinated replay allows effective planning with imperfect models while models trained using only real experience fail dramatically.},
author = {Talvitie, Erik},
file = {:home/andrew/Documents/grad/Papers/talvitiemodelregularization.pdf:pdf},
isbn = {9780974903910},
journal = {Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014},
pages = {780--789},
title = {{Model regularization for stable sample rollouts}},
year = {2014}
}
@article{Sutton2011,
abstract = {Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other arti cial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a sin- gle predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the sys- tem's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, re- ward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever ac- tions are taken by the system as a whole. Gradient-based temporal-di erence learning methods are used to learn ef- ciently and reliably with function approximation in this o -policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real- time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from o - policy experience. Horde is a signi cant incremental step towards a real-time architecture for ecient learning of gen-},
author = {Sutton, Richard S. and Modayil, Joseph and Degris, Michael Delp Thomas and Pilarski, Patrick M. and White, Adam and Precup, Doina},
file = {:home/andrew/Documents/grad/Papers/horde1.pdf:pdf},
journal = {10th International Conference on Autonomous Agents and Multiagent Systems 2011, AAMAS 2011},
keywords = {Artificial intelligence,Knowledge representation,Off-policy learning,Real-time,Reinforcement learning,Robotics,Temporal-difference learning,Value function approximation},
number = {1972},
pages = {713--720},
title = {{Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction}},
volume = {2},
year = {2011}
}
@article{Tamar2017,
abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation.We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.This paper is a significantly abridged and IJCAI audience targeted version of the original NIPS 2016 paper with the same title, available here: https://arxiv.org/abs/1602.02867},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.02867v4},
author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
eprint = {arXiv:1602.02867v4},
file = {:home/andrew/Documents/grad/Papers/1602.02867.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {Neural Information Processing Systems},
number = {Nips},
pages = {4949--4953},
title = {{Value iteration networks}},
year = {2017}
}
@article{Gregor2019,
abstract = {When agents interact with a complex environment, they must form and maintain beliefs about the relevant aspects of that environment. We propose a way to efficiently train expressive generative models in complex environments. We show that a predictive algorithm with an expressive generative model can form stable belief-states in visually rich and dynamic 3D environments. More precisely, we show that the learned representation captures the layout of the environment as well as the position and orientation of the agent. Our experiments show that the model substantially improves data-efficiency on a number of reinforcement learning (RL) tasks compared with strong model-free baseline agents. We find that predicting multiple steps into the future (overshooting), in combination with an expressive generative model, is critical for stable representations to emerge. In practice, using expressive generative models in RL is computationally expensive and we propose a scheme to reduce this computational burden, allowing us to build agents that are competitive with model-free baselines.},
archivePrefix = {arXiv},
arxivId = {1906.09237},
author = {Gregor, Karol and Rezende, Danilo Jimenez and Besse, Frederic and Wu, Yan and Merzic, Hamza and van den Oord, Aaron},
eprint = {1906.09237},
file = {:home/andrew/Documents/grad/Papers/1906.09237.pdf:pdf},
journal = {arXiv preprint},
title = {{Shaping Belief States with Generative Environment Models for RL}},
url = {http://arxiv.org/abs/1906.09237},
year = {2019}
}
@book{Lakoff1980,
author = {Lakoff, George and Johnson, Mark},
file = {:home/andrew/Documents/grad/Papers/Lakoff{\_}Johnson.pdf:pdf},
publisher = {University of Chicago press},
title = {{Metaphors we live by}},
year = {1980}
}
@article{Kim2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1711.11279v5},
author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
eprint = {arXiv:1711.11279v5},
file = {:home/andrew/Documents/grad/Papers/1711.11279.pdf:pdf},
journal = {arXiv preprint},
title = {{Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors ( TCAV )}},
year = {2018}
}
@article{Higgins2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1812.02230v1},
author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
eprint = {arXiv:1812.02230v1},
file = {:home/andrew/Documents/grad/Papers/1812.02230.pdf:pdf},
journal = {arXiv preprint},
pages = {1--29},
title = {{Towards a Definition of Disentangled Representations}},
year = {2018}
}
@article{Li2019a,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.06331v1},
author = {Li, Huaiyu and Dong, Weiming and Mei, Xing and Ma, Chongyang and Huang, Feiyue and Hu, Bao-Gang},
eprint = {arXiv:1905.06331v1},
file = {:home/andrew/Documents/grad/Papers/1905.06331.pdf:pdf},
journal = {Proceedings of the 36th International Conference on Machine Learning},
title = {{LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning}},
year = {2019}
}
@article{Mitchell2018,
author = {Mitchell, T and Cohen, W and Hruschka, E and Talukdar, P and Betteridge, J and Carlson, A and Dalvi, B and Gardner, M and Kisiel, B and Krishnamurthy, J and Lao, N and Mazaitis, K and Al., Et},
file = {:home/andrew/Documents/grad/Papers/p103-mitchell.pdf:pdf},
journal = {Communications of the ACM},
pages = {2302--2310},
title = {{Never-Ending Learning}},
year = {2018}
}
@article{Dove2016,
author = {Dove, Guy},
doi = {10.3758/s13423-015-0825-4},
file = {:home/andrew/Documents/grad/Papers/Dove2016{\_}Article{\_}ThreeSymbolUngroundingProblems.pdf:pdf},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
keywords = {Concepts,Embodied cognition,Grounded cognition,Semantic memory,Word meaning,concepts,embodied cognition,for a long time,grounded cognition,science held,semantic memory,system was functionally distinct,that the human conceptual,the orthodoxy within cognitive,word meaning},
number = {4},
pages = {1109--1121},
publisher = {Psychonomic Bulletin {\&} Review},
title = {{Three symbol ungrounding problems : Abstract concepts and the future of embodied cognition}},
url = {http://dx.doi.org/10.3758/s13423-015-0825-4},
volume = {23},
year = {2016}
}
@article{Cisek1999,
author = {Cisek, Paul},
file = {:home/andrew/Documents/grad/Papers/10.1.1.183.1585.pdf:pdf},
journal = {Journal of Consciousness Studies},
number = {11-12},
title = {{Beyond the computer metaphor: Behavior as interaction}},
volume = {6},
year = {1999}
}
@article{Cisek2019,
author = {Cisek, Paul},
file = {:home/andrew/Documents/grad/Papers/Cisek2019{\_}Article{\_}ResynthesizingBehaviorThroughP.pdf:pdf},
isbn = {1341401901},
journal = {Attention, Perception, {\&} Psychophysics},
keywords = {Animal cognition,Cognitive neuroscience,Evolution,Neural mechanisms,a major challenge of,about our,animal cognition,any scientific endeavor is,but to find good,cognitive neuroscience,evolution,neural mechanisms,not only to,provide good answers to,questions to ask in,the first place,the questions we ask,world},
publisher = {Attention, Perception, {\&} Psychophysics},
title = {{Resynthesizing behavior through phylogenetic refinement}},
year = {2019}
}
@article{Garnelo2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1807.01613v1},
author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo J. and Eslami, S. M. Ali},
eprint = {arXiv:1807.01613v1},
file = {:home/andrew/Documents/grad/Papers/1807.01613.pdf:pdf},
journal = {arXiv preprint},
title = {{Conditional Neural Processes}},
year = {2018}
}
@article{Veness2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1712.01897v1},
author = {Veness, Joel and Lattimore, Tor and Bhoopchand, Avishkar and Grabska-Barwinska, Agneiszka and Mattern, Christopher and Toth, Peter},
eprint = {arXiv:1712.01897v1},
file = {:home/andrew/Documents/grad/Papers/1712.01897.pdf:pdf},
journal = {arXiv preprint},
keywords = {compression,geometric mixing,logarithmic loss,online learning},
pages = {1--40},
title = {{Online Learning with Gated Linear Networks}},
year = {2017}
}
@article{Rosenthal1990,
author = {Rosenthal, David M.},
file = {:home/andrew/Documents/grad/Papers/6d19b47ad177c1450ff0a62d5f0abe423663.pdf:pdf},
journal = {Zentrum f{\"{u}}r interdisziplin{\"{a}}re Forschung},
title = {{A Theory of Consciousness}},
year = {1990}
}
@book{Lakoff1999,
author = {Lakoff, George and Johnson, Mark},
title = {{Philosophy in the flesh: the embodied mind and its challenge to western thought}},
year = {1999}
}
@article{Anderson2003,
author = {Anderson, Michael L},
doi = {10.1016/S0004-3702(03)00054-7},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S0004370203000547-main.pdf:pdf},
journal = {Artificial Intelligence},
pages = {91--130},
title = {{Embodied Cognition : A field guide}},
volume = {149},
year = {2003}
}
@article{Lu2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1908.02265v1},
author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
eprint = {arXiv:1908.02265v1},
file = {:home/andrew/Documents/grad/Papers/1908.02265.pdf:pdf},
journal = {arXiv preprint},
pages = {1--11},
title = {{ViLBERT : Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks}},
year = {2019}
}
@article{Tomasello1993,
author = {Tomasello, Michael and Kruger, Ann Cale and Ratner, Hilary Horn},
doi = {10.1007/978-1-4419-1428-6_778},
file = {:home/andrew/Documents/grad/Papers/85220180.pdf:pdf},
journal = {Behavioral and Brain Sciences},
keywords = {animal cognition,attention,cognitive development,collaboration,cultural learning,culture,cultures are most clearly,distin-,humans live in cultures,imitation,in complex social groups,instruction,intersubjectivity,intontionality,many animal species live,only,social learning,theory of mind},
number = {16},
title = {{Cultural Learning}},
year = {1993}
}
@inproceedings{Abbeel2004,
abstract = {MANUAL FOR SORVALL LEGEND MICRO 21R},
archivePrefix = {arXiv},
arxivId = {1206.5264},
author = {Abbeel, Pieter and Ng, Andrew Y.},
booktitle = {International Conference on Machine Learning},
doi = {10.1145/1015330.1015430},
eprint = {1206.5264},
file = {:home/andrew/Documents/grad/Papers/icml04-apprentice.pdf:pdf},
isbn = {1581138285},
issn = {0028-0836},
pmid = {25719670},
title = {{Apprenticeship learning via inverse reinforcement learning}},
year = {2004}
}
@inproceedings{Ng2000,
abstract = {This paper addresses the problem of inverse reinforcement learning (IRL) in Markov decision processes, that is, the problem of extracting a reward function given observed, optimal behaviour. IRL may be useful for apprenticeship learning to acquire skilled behaviour, and for ascertaining the reward function being optimized by a natural system. We rst characterize the set of all reward functions for which a given policy is optimal. We then derive three algorithms for IRL. The rst two deal with the case where the entire policy is known; we handle tabulated reward functions on a nite state space and linear functional approximation of the reward function over a potentially in- nite state space. The third algorithm deals with the more realistic case in which the policy is known only through a nite set of observed trajectories. In all cases, a key issue is degeneracythe existence of a large set of reward functions for which the observed policy is optimal. To remove...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ng, Andrew Y and Russell, Stuart},
booktitle = {International Conference on Machine Learning},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/andrew/Documents/grad/Papers/icml00-irl.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
title = {{Algorithms for Inverse Reinforcement Learning}},
year = {2000}
}
@article{Carpenter2005,
author = {Carpenter, Malinda and Moll, Henrike and Tomasello, Michael and Call, Josep and Behne, Tanya},
doi = {10.1017/s0140525x05000129},
file = {:home/andrew/Documents/grad/Papers/understanding{\_}and{\_}sharing{\_}intentions{\_}the{\_}origins{\_}of{\_}cultural{\_}cognition.pdf:pdf},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
keywords = {as,collaboration,collaboration, cooperation, cultural learning, cul,compared with other species,cooperation,cultural learning,culture,evolutionary psychology,human beings are the,humans are much more,intentions,joint attention,nition,reading,s experts at mind,shared intentionality,skill-,social cog-,social learning,theory of mind,world},
number = {05},
pages = {675--735},
publisher = {Stanford Libraries},
title = {{Understanding and sharing intentions: The origins of cultural cognition}},
volume = {28},
year = {2005}
}
@inproceedings{Tishby2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.02406v1},
author = {Tishby, Naftali and Zaslavsky, Noga},
booktitle = {IEEE Information Theory Workshop (ITW)},
eprint = {arXiv:1503.02406v1},
file = {:home/andrew/Documents/grad/Papers/1503.02406.pdf:pdf},
title = {{Deep Learning and the Information Bottleneck Principle}},
year = {2015}
}
@article{McClelland2002,
author = {McClelland, James and Patterson, Karalyn},
file = {:home/andrew/Documents/grad/Papers/RulesOrConnections.pdf:pdf},
issn = {1364-6613},
journal = {Trends in cognitive sciences},
keywords = {47566,connectionist models,inflectional morphology,langauge processing,mh,parallel-distributed processing,past,preparation of this article,rules,tense,was supported by mh},
number = {11},
pages = {465--472},
title = {{Rules or connections in past-tense inflections: what does the evidence rule out?}},
volume = {6},
year = {2002}
}
@book{MacLane1986,
author = {{Mac Lane}, Saunders},
file = {:home/andrew/Documents/grad/Papers/1986{\_}Book{\_}MathematicsFormAndFunction.pdf:pdf},
isbn = {9781461293408},
publisher = {Springer-Verlag},
title = {{Mathematics: Form and Function}},
year = {1986}
}
@book{Hersh1997,
author = {Hersh, Ruben},
doi = {10.5840/intstudphil2003354114},
publisher = {Oxford University Press},
title = {{What is Mathematics, Really?}},
year = {1997}
}
@article{Chi1994,
abstract = {Learning involves the integration of new information into existing knowledge. Generating explanations to oneself (self-explaining) facilitates that integration process. Previously, self-explanation has been shown to improve the acquisition of problem-solving skills when studying worked-out examples. This study extends that finding, showing that self-explanation can also be facilitative when it is explicitly promoted, in the context of learning declarative knowledge from an expository text. Without any extensive training, 14 eighth-grade students were merely asked to self-explain after reading each line of a passage on the human circulatory system. Ten students in the control group read the same text twice, but were not prompted to self-explain. All of the students were tested for their circulatory system knowledge before and after reading the text. The prompted group had a greater gain from the pretest to the posttest. Moreover, prompted students who generated a large number of self-explanations (the high explainers) learned with greater understanding than low explainers. Understanding was assessed by answering very complex questions and inducing the function of a component when it was only implicitly stated. Understanding was further captured by a mental model analysis of the self-explanation protocols. High explainers all achieved the correct mental model of the circulatory system, whereas many of the unprompted students as well as the low explainers did not. Three processing characteristics of self-explaining are considered as reasons for the gains in deeper understanding. {\textcopyright} 1994.},
author = {Chi, Michelene T.H. and {De Leeuw}, Nicholas and Chiu, Mei Hung and Lavancher, Christian},
doi = {10.1016/0364-0213(94)90016-7},
file = {:home/andrew/Documents/grad/Papers/1d9f2930b32cae64da012943cc52128a9e57.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
number = {3},
pages = {439--477},
title = {{Eliciting self-explanations improves understanding}},
volume = {18},
year = {1994}
}
@article{Chi1989,
abstract = {The present paper analyzes the self-generated explanations (from talk-aloud protocols) that Good and Poor students produce while studying worked-out examples of mechanics problems, and their subsequent reliance on examples during problem solving. We find that Good students learn with understanding: They generate many explanations which refine and expand the conditions for the action parts of the example solutions, and relate these actions to principles in the text. These self-explanations are guided by accurate monitoring of their own understanding and misunderstanding. Such learning results in example-independent knowledge and in a better understanding of the principles presented in the text. Poor students do not generate sufficient self-explanations, monitor their learning inaccurately, and subsequently rely heavily on examples. We then discuss the role of self-explanations in facilitating problem solving, as well as the adequacy of current AI models of explanation-based learning to account for these psychological findings.},
author = {Chi, M T H and Lewis, Matthew W and Reimann, Peter and Glaser, Robert},
doi = {10.1016/0364-0213(89)90002-5},
file = {:home/andrew/Documents/grad/Papers/eefc9b2a0eb8b92b4b79a878d39163c0d428.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
number = {2},
pages = {145--182},
title = {{How Students Study and Use Examples in Learning to Solve Problems}},
url = {http://dx.doi.org/10.1016/0364-0213(89)90002-5},
volume = {13},
year = {1989}
}
@article{Silver2017,
abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, su-perhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated posi-tions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here, we introduce an algorithm based solely on reinforcement learning, without hu-man data, guidance, or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of tree search, re-sulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. Much progress towards artificial intelligence has been made using supervised learning sys-tems that are trained to replicate the decisions of human experts 1â€“4 . However, expert data is often expensive, unreliable, or simply unavailable. Even when reliable data is available it may impose a ceiling on the performance of systems trained in this manner 5 . In contrast, reinforcement learn-ing systems are trained from their own experience, in principle allowing them to exceed human capabilities, and to operate in domains where human expertise is lacking. Recently, there has been rapid progress towards this goal, using deep neural networks trained by reinforcement learning. These systems have outperformed humans in computer games such as Atari 6, 7 and 3D virtual en-vironments 8â€“10 . However, the most challenging domains in terms of human intellect â€“ such as the 1},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {Van Den Driessche}, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
file = {:home/andrew/Downloads/agz{\_}unformatted{\_}nature.pdf:pdf},
isbn = {3013372370},
issn = {0028-0836},
journal = {Nature},
pmid = {29052630},
title = {{Mastering the game of go without human knowledge}},
volume = {550},
year = {2017}
}
@article{Schapiro2009,
author = {Schapiro, Anna C and Mcclelland, James L},
doi = {10.1016/j.cognition.2008.11.017},
file = {:home/andrew/Documents/grad/Papers/SchapiroMcC09BalScale.pdf:pdf},
journal = {Cognition},
pages = {395--411},
title = {{A connectionist model of a continuous developmental transition in the balance scale task}},
volume = {110},
year = {2009}
}
@article{Kennedy1996,
abstract = {ISSN: 0361-0918 (Print) 1532-4141 (Online) Journal homepage: http://www.tandfonline.com/loi/lssp20 ABSTRACT Four generic means of conducting randomization tcsts in the context of multiple regression are analysed. Based on their performance in traditional repeated samples, three of these are shown to be inappropriate or applicable only in special circumstances; their shortcomings are illustrated via Monte Carlo studies.},
author = {Kennedy, Peler E. and Cade, Brian S.},
doi = {10.1080/03610919608813350},
file = {:home/andrew/Documents/grad/Papers/Randomization tests for multiple regression.pdf:pdf},
issn = {03610918},
journal = {Communications in Statistics Part B: Simulation and Computation},
keywords = {Distribution-free tests,Permutation tests},
number = {4},
pages = {923--936},
title = {{Randomization tests for multiple regression}},
volume = {25},
year = {1996}
}
@inproceedings{Poon2011,
author = {Poon, Hoifung and Domingos, Pedro},
booktitle = {IEEE International Conference on Computer Vision Workshops (ICCV Workshops)},
file = {:home/andrew/Documents/grad/Papers/1202.3732.pdf:pdf},
title = {{Sum-Product Networks: A New Deep Architecture}},
year = {2011}
}
@article{Lynn2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1805.12491v2},
author = {Lynn, Christopher W and Kahn, Ari E and Bassett, Danielle S},
eprint = {arXiv:1805.12491v2},
file = {:home/andrew/Documents/grad/Papers/1805.12491.pdf:pdf},
journal = {arXiv preprint},
pages = {1--62},
title = {{Structure from noise : Mental errors yield abstract representations of events}},
year = {2018}
}
@article{Karuza2017,
abstract = {Network science has emerged as a powerful tool through which we can study the higher-order architectural properties of the world around us. How human learners exploit this information remains an essential question. Here, we focus on the temporal constraints that govern such a process. Participants viewed a continuous sequence of images generated by three distinct walks on a modular network. Walks varied along two critical dimensions: their predictability and the density with which they sampled from communities of images. Learners exposed to walks that richly sampled from each community exhibited a sharp increase in processing time upon entry into a new community. This effect was eliminated in a highly regular walk that sampled exhaustively from images in short, successive cycles (i.e., that increasingly minimized uncertainty about the nature of upcoming stimuli). These results demonstrate that temporal organization plays an essential role in learners' sensitivity to the network architecture underlying sensory input.},
author = {Karuza, Elisabeth A. and Kahn, Ari E. and Thompson-Schill, Sharon L. and Bassett, Danielle S.},
doi = {10.1038/s41598-017-12876-5},
file = {:home/andrew/Documents/grad/Papers/s41598-017-12876-5.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--9},
publisher = {Springer US},
title = {{Process reveals structure: How a network is traversed mediates expectations about its architecture}},
url = {http://dx.doi.org/10.1038/s41598-017-12876-5},
volume = {7},
year = {2017}
}
@inproceedings{Brock2018a,
archivePrefix = {arXiv},
arxivId = {arXiv:1708.05344v1},
author = {Brock, Andrew and Lim, Theodore and Ritchie, J M and Weston, Nick},
booktitle = {International Conference on Learning Representations},
eprint = {arXiv:1708.05344v1},
file = {:home/andrew/Documents/grad/Papers/1708.05344.pdf:pdf},
title = {{SMASH: One-Shot Model Architecture Search through HyperNetworks}},
year = {2018}
}
@article{Achille2017,
abstract = {Critical periods are phases in the early development of humans and animals during which experience can irreversibly affect the architecture of neuronal networks. In this work, we study the effects of visual stimulus deficits on the training of artificial neural networks (ANNs). Introducing well-characterized visual deficits, such as cataract-like blurring, in the early training phase of a standard deep neural network causes a permanent performance loss that closely mimics critical period behavior in humans and animal models. Deficits that do not affect low-level image statistics, such as vertical flipping of the images, have no lasting effect on the ANNs' performance and can be rapidly overcome with further training. In addition, the deeper the ANN is, the more pronounced the critical period. To better understand this phenomenon, we use Fisher Information as a measure of the strength of the network's connections during the training. Our information-theoretic analysis suggests that the first few epochs are critical for the creation of strong connections across different layers, optimal for processing the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial rapid learning phase of ANN training, under-scrutinized compared to its asymptotic behavior, plays a key role in defining the final performance of networks. Our results also show how critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.},
archivePrefix = {arXiv},
arxivId = {1711.08856},
author = {Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
eprint = {1711.08856},
file = {:home/andrew/Documents/grad/Papers/1711.08856.pdf:pdf},
journal = {arXiv preprint},
title = {{Critical Learning Periods in Deep Neural Networks}},
url = {http://arxiv.org/abs/1711.08856},
year = {2017}
}
@book{Singley1989,
author = {Singley, Mark K. and Anderson, John R.},
title = {{The transfer of cognitive skill}},
year = {1989}
}
@inproceedings{Sprechmann2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1802.10542v1},
author = {Sprechmann, Pablo and Jayakumar, Siddhant M and Rae, Jack W and Pritzel, Alexander and Uria, Benigno and Vinyals, Oriol and Hassabis, Demis and Pascanu, Razvan and Blundell, Charles},
booktitle = {International Conference on Learning Representations},
eprint = {arXiv:1802.10542v1},
file = {:home/andrew/Documents/grad/Papers/1802.10542.pdf:pdf},
title = {{Memory-based parameter Adaptation}},
year = {2018}
}
@article{Nunez2011,
abstract = {Many authors in the field of numerical cognition have adopted a rather nativist view that all humans share the intuition that numbers map onto space and, more specifically, that an oriented left-to-right mental number line (MNL) is localized bilaterally in the intraparietal sulcus of the human brain. We review results from archaeological and historical (diachronic) studies as well as cross-cultural (synchronic) ones and contends that these claims are not well founded. The data actually suggest that the MNL is not innate. We argue that the MNL-and number-to-space mappings in general-emerges outside of natural selection proper requiring top-down dynamics that are culturally and historically mediated through high-order cognitive mechanisms such as fictive motion, conceptual mappings, and external representational media. These mechanisms, which are not intrinsically numerical and usually are acquired through education, are not genetically determined and are biologically realized through the systematic consolidation of specific brain phenotypes that support number-to-space mappings.},
author = {N{\'{u}}{\~{n}}ez, Rafael E.},
doi = {10.1177/0022022111406097},
file = {:home/andrew/Documents/grad/Papers/Nunez{\_}11{\_}JCCP.pdf:pdf},
issn = {00220221},
journal = {Journal of Cross-Cultural Psychology},
keywords = {cognitive,innate,number line,numerical cognition,universal},
number = {4},
pages = {651--668},
title = {{No innate number line in the human brain}},
volume = {42},
year = {2011}
}
@article{Rittle-Johnson2001,
author = {Rittle-Johnson, Bethany and Siegler, Robert S. and Alibali, Martha W},
file = {:home/andrew/Documents/grad/Papers/r-jhnsn-etal-01.pdf:pdf},
journal = {Journal of Educational Psychology},
number = {2},
title = {{Developing conceptual understanding and procedural skill in mathematics: an iterative process}},
volume = {93},
year = {2001}
}
@article{Rittle-Johnson1999,
author = {Rittle-Johnson, Bethany and Alibali, Martha W},
file = {:home/andrew/Documents/grad/Papers/ATME{\_}Rittle-JohnsonandAlibali{\_}1999.pdf:pdf},
journal = {Journal of Educational Psychology},
number = {1},
pages = {175--189},
title = {{Conceptual and procedural knowledge of mathematics: does one lead to the other?}},
volume = {91},
year = {1999}
}
@article{Gordon2004,
author = {Gordon, Peter},
file = {:home/andrew/Documents/grad/Papers/Gordon{\_}Piraha.pdf:pdf},
journal = {Science},
number = {October},
pages = {496--499},
title = {{Numerical Cognition Without Words: Evidence from Amazonia}},
volume = {306},
year = {2004}
}
@incollection{Feltovich2012,
abstract = {Learning as expertise development},
author = {Feltovich, Paul J. and Prietula, Michael J. and Ericsson, K. Anders},
booktitle = {The Cambridge Handbook of Expertise and Expert Performance},
doi = {10.1017/cbo9780511816796.004},
file = {:home/andrew/Documents/grad/Papers/4297c335d804a55f65ffca140c1f97af8a28.pdf:pdf},
pages = {41--68},
title = {{Studies of Expertise from Psychological Perspectives}},
year = {2012}
}
@article{Larochelle2008,
abstract = {We introduce the problem of zero-data learning, where a model must generalize to classes or tasks for which no training data are available and only a description of the classes or tasks are provided. Zero-data learning is useful for problems where the set of classes to distinguish or tasks to solve is very large and is not entirely covered by the training data. The main contributions of this work lie in the presentation of a general formalization of zero-data learning, in an experimental analysis of its properties and in empirical evidence showing that generalization is possible and significant in this context. The experimental work of this paper addresses two classification problems of character recognition and a multitask ranking problem in the context of drug discovery. Finally, we conclude by discussing how this new framework could lead to a novel perspective on how to extend machine learning towards {\{}AI{\}}, where an agent can be given a specification for a learning problem before attempting to solve it (with very few or even zero examples).},
author = {Larochelle, Hugo and Erhan, Dumitru and Bengio, Yoshua},
file = {:home/andrew/Documents/grad/Papers/AAAI08-103.pdf:pdf},
isbn = {978-1-57735-368-3},
issn = {1064-3745},
journal = {Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence},
keywords = {multitask,zero shot},
number = {Miller 2002},
pages = {646--651},
pmid = {10906994},
title = {{Zero-data learning of new tasks}},
year = {2008}
}
@article{McNeil2005,
abstract = {This study examined whether knowledge of arithmetic contributes to difficulties with equations. In Experiment 1, children (ages 7â€“11) completed tasks to assess their adherence to 3 operational patterns prevalent in arith- metic: (a) the strategy of performing all given},
author = {McNeil, Nicole M. and Alibali, Martha W.},
file = {:home/andrew/Documents/grad/Papers/d0aaa973a7f444ecbc17ddd51f4b83084ade.pdf:pdf},
journal = {Child development},
number = {4},
pages = {883--899},
title = {{Why won't you change your mind? Knowledge of operational patterns hinders learning and performance on equations}},
url = {http://www.homepage.psy.utexas.edu/HomePage/Class/Psy394N/Woolley/11{\%}2520Apr{\%}25204{\%}2520Academic{\%}2520Skills/McNeil{\%}2520Alibali{\%}25202005.pdf{\%}5Cnpapers://5587b4e2-af07-460c-b041-e9e68a9c0f25/Paper/p3483},
volume = {76},
year = {2005}
}
@incollection{Grice1975,
author = {Grice, H},
booktitle = {Syntax And Semantics},
file = {:home/andrew/Documents/grad/Papers/grice1975logic-and-conversation.pdf:pdf},
isbn = {0127854231},
number = {January},
pages = {41--58},
title = {{Logic and Conversation}},
year = {1975}
}
@article{Nunez2006,
author = {N{\'{u}}nez, R and Sweetser, E},
file = {:home/andrew/Documents/grad/Papers/FINALpblshd.pdf:pdf},
journal = {Cognitive Science},
keywords = {aymara,conceptual metaphor,conceptual systems,embodied cognition,gestures,inferential organization,spatial construals of time},
pages = {1--49},
title = {{Aymara, where the future is behind you: Convergent evidence from language and gesture in the cross-linguistic comparison of spatial construals of time}},
url = {papers3://publication/uuid/F75326F6-08A7-47FC-84A6-524334E5893C},
volume = {30},
year = {2006}
}
@article{VanDamme2002,
abstract = {Here we show that if an adult demonstrates a new way to execute a task to a group of infants aged 14 months, the children will use this action to achieve the same goal only if they consider it to be the most rational alternative. Our results indicate that imitation of goal-directed action by preverbal infants is a selective, interpretative process, rather than a simple re-enactment of the means used by a demonstrator, as was previously thought.},
author = {{Van Damme}, Raoul and Wilson, Robbie S. and Vanhooyconck, Bieke and Aerts, Peter},
doi = {10.1038/415755a},
file = {:home/andrew/Documents/grad/Papers/415755a.pdf:pdf},
isbn = {1476-4687 (Electronic); 0028-0836 (Print)},
issn = {0028-0836},
journal = {Nature},
number = {6873},
pages = {755},
pmid = {11845198},
title = {{Rational imitation in preverbal infants}},
volume = {415},
year = {2002}
}
@article{Lombrozo2006,
abstract = {Generating and evaluating explanations is spontaneous, ubiquitous and fundamental to our sense of understanding. Recent evidence suggests that in the course of an individual's reasoning, engaging in explanation can have profound effects on the probability assigned to causal claims, on how properties are generalized and on learning. These effects follow from two properties of the structure of explanations: explanations accommodate novel information in the context of prior beliefs, and do so in a way that fosters generalization. The study of explanation thus promises to shed light on core cognitive issues, such as learning, induction and conceptual representation. Moreover, the influence of explanation on learning and inference presents a challenge to theories that neglect the roles of prior knowledge and explanation-based reasoning. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Lombrozo, Tania},
doi = {10.1016/j.tics.2006.08.004},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S1364661306002117-main.pdf:pdf},
isbn = {1364-6613},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {10},
pages = {464--470},
pmid = {16942895},
title = {{The structure and function of explanations}},
volume = {10},
year = {2006}
}
@article{Keil2006,
abstract = {The study of explanation, while related to intuitive theories, concepts, and mental models, offers important new perspectives on high-level thought. Explanations sort themselves into several distinct types corresponding to patterns of causation, content domains, and explanatory stances, all of which have cognitive consequences. Although explanations are necessarily incompleteâ€”often dramatically so in laypeopleâ€”those gaps are difficult to discern. Despite such gaps and the failure to recognize them fully, people do have skeletal explanatory senses, often implicit, of the causal structure of the world. They further leverage those skeletal understandings by knowing how to access additional explanatory knowledge in other minds and by being particularly adept at using situational support to build explanations on the fly in real time. Across development and cultures, there are differences in preferred explanatory schemes, but rarely are any kinds of schemes completely unavailable to a group},
author = {Keil, Frank C.},
doi = {10.4324/9781315823171},
file = {:home/andrew/Documents/grad/Papers/nihms268236.pdf:pdf},
isbn = {9781315823171},
journal = {Annual Review of Psychology},
keywords = {causality,cognition,cognitive development,concepts,domain specificity,illusions of knowing},
pages = {227--254},
title = {{Explanation and understanding}},
volume = {57},
year = {2006}
}
@inproceedings{Xu2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1805.09801v1},
author = {Xu, Zhongwen and van Hassellt, Hado and Silver, David},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {arXiv:1805.09801v1},
file = {:home/andrew/Documents/grad/Papers/1805.09801.pdf:pdf},
title = {{Meta-Gradient Reinforcement Learning}},
year = {2018}
}
@article{He2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1811.08883v1},
author = {He, Kaiming and Girshick, Ross and Dollar, Piotr},
eprint = {arXiv:1811.08883v1},
file = {:home/andrew/Documents/grad/Papers/1811.08883.pdf:pdf},
journal = {arXiv preprint},
title = {{Rethinking ImageNet Pre-training}},
year = {2018}
}
@inproceedings{Lampinen2016,
author = {Lampinen, Andrew and McClelland, James L.},
booktitle = {15th Neural Computation and Psychology workshop},
file = {:home/andrew/Documents/grad/res/networks/compositional-learning/writing/contemp{\_}NN{\_}models{\_}abstract.pdf:pdf},
title = {{Fast {\&} Sparse Learning with Compositional Concept Training}},
year = {2016}
}
@inproceedings{LeCun2016,
author = {{Le Cun}, Yann},
booktitle = {NIPS},
title = {{Predictive Learning (NIPS Keynote)}},
year = {2016}
}
@inproceedings{Achille2018a,
abstract = {Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.},
archivePrefix = {arXiv},
arxivId = {1808.06508},
author = {Achille, Alessandro and Eccles, Tom and Matthey, Loic and Burgess, Christopher P. and Watters, Nick and Lerchner, Alexander and Higgins, Irina},
booktitle = {Advances in Neural Information Processing Systems},
doi = {10.1002/hyp.6850},
eprint = {1808.06508},
file = {:home/andrew/Documents/grad/Papers/8193-life-long-disentangled-representation-learning-with-cross-domain-latent-homologies.pdf:pdf},
isbn = {0250-8095 (Print)},
issn = {08856087},
pmid = {3239590},
title = {{Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies}},
url = {http://arxiv.org/abs/1808.06508},
year = {2018}
}
@article{Radford2019,
author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
file = {:home/andrew/Documents/grad/Papers/language{\_}models{\_}are{\_}unsupervised{\_}multitask{\_}learners.pdf:pdf},
journal = {arXiv preprint},
title = {{Language models are unsupervised multitask learners}},
year = {2019}
}
@inproceedings{Santoro2016,
author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
booktitle = {Proceedings of the 33rd International Conference on Machine Learning},
file = {:home/andrew/Documents/grad/Papers/santoro16.pdf:pdf},
title = {{Meta-Learning with Memory-Augmented Neural Networks}},
volume = {48},
year = {2016}
}
@article{Luchins1942,
author = {Luchins, Abraham S.},
file = {:home/andrew/Documents/grad/Papers/2011-21639-001.pdf:pdf},
journal = {Psychological Monographs},
title = {{Mechanization in problem solving}},
year = {1942}
}
@inproceedings{Finn2017a,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.03400v1},
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
booktitle = {Proceedings of the 34th Annual Conference on Machine Learning},
eprint = {arXiv:1703.03400v1},
file = {:home/andrew/Documents/grad/Papers/3186c368544ded00a444be33153420baa254.pdf:pdf},
title = {{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}},
year = {2017}
}
@book{Karmiloff-Smith1992,
author = {Karmiloff-Smith, Annette},
publisher = {The MIT Press},
title = {{Beyond modularity: A developmental perspective}},
year = {1992}
}
@article{Landrum2005,
author = {Landrum, R. Eric.},
file = {:home/andrew/Documents/grad/Papers/pr0.97.3.861-866.pdf:pdf},
journal = {Psychological Reports},
pages = {861--866},
title = {{Production of negative transfer in a problem-solving task}},
volume = {97},
year = {2005}
}
@article{Doebel2015,
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Doebel, Sabine and Zelazo, Philip David},
doi = {10.1177/0333102415576222.Is},
eprint = {15334406},
file = {:home/andrew/Documents/grad/Papers/nihms732247.pdf:pdf},
isbn = {3037242094},
issn = {0959-437X},
journal = {Developmental Review},
pages = {241--268},
pmid = {26928661},
title = {{A meta-analysis of the Dimensional Change Card Sort: Implications for developmental theories and the measurement of executive function in children}},
volume = {38},
year = {2015}
}
@article{Lampinen2018a,
archivePrefix = {arXiv},
arxivId = {arXiv:1710.10280v2},
author = {Lampinen, Andrew K and McClelland, James L},
eprint = {arXiv:1710.10280v2},
file = {:home/andrew/Documents/grad/Papers/1710.10280.pdf:pdf},
journal = {arXiv preprint},
title = {{One-shot and few-shot learning of word embeddings}},
year = {2017}
}
@article{Sutton1999,
author = {Sutton, Richard S and Precup, Doina and Singh, Satinder},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S0004370299000521-main.pdf:pdf},
pages = {181--211},
title = {{Between MDPs and semi-MDPs : A framework for temporal abstraction in reinforcement learning}},
volume = {112},
year = {1999}
}
@incollection{McClelland1998,
author = {McClelland, James L.},
booktitle = {Rational models of cognition},
file = {:home/andrew/Documents/grad/Papers/warwick.pdf:pdf},
pages = {21--53},
title = {{Connectionist models and Bayesian inference}},
year = {1998}
}
@article{Marcus2018,
author = {Marcus, Gary},
file = {:home/andrew/Documents/grad/Papers/1801.00631.pdf:pdf},
journal = {arXiv preprint},
pages = {1--27},
title = {{Deep Learning: A Critical Appraisal}},
year = {2018}
}
@article{Blodgett1929,
abstract = {"The purpose of this investigation was to study the efficiency of units of practice when unaccompanied by reward. The method devised was that of running two groups of rats through the maze: an experimental group which received no reward during the first part of learning, but which suddenly had reward introduced in the latter part of learning, and a control group which received reward throughout the whole of learning. The answer to the question as to the efficiency of non-reward units of practice was sought in a comparison of the learning curve of the experimental group (both before and after the introduction of reward) with that of the control group." Three mazes were used, two with ordinary blinds and one with blinds arranged so that the animal could go two ways as well as having as alternatives a long and a short path. "They prevented retracings from one section of the maze to another, they were noiseless, and they caused no excitement in the animals." There were 36 rats in each group. The results show that: "(1) Rats run under a non-reward condition learned much more slowly than rats run under a reward condition." "(2) Rats previously run under a non-reward condition, when suddenly rewarded made a great improvement." "(3) During the non-reward period, the rats were developing a latent learning of the maze which they were able to utilize as soon as reward was introduced." "(5) It was demonstrated by the use of the two-path maze that the latent learning which was developed under non-reward conditions and was made manifest as soon as reward was introduced was not the result of any very consistently greater frequency of the correct over the incorrect path during the non-reward period." Bibliography and discussions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
author = {Blodgett, H C},
journal = {University of California Publications in Psychology},
pages = {113--134},
title = {{The effect of the introduction of reward upon the maze performance of rats.}},
volume = {4},
year = {1929}
}
@article{Park2014,
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Park, Joonkoo and Brannon, Elizabeth M.},
doi = {10.1016/j.dcn.2011.01.002.The},
eprint = {NIHMS150003},
file = {:home/andrew/Documents/grad/Papers/nihms614955.pdf:pdf},
isbn = {6176321972},
issn = {15378276},
journal = {Cognition},
keywords = {adolescence,affiliative,childhood,emotion},
number = {1},
pages = {188--200},
title = {{Improving Arithmetic Performance with Number Sense Training: An Investigation of Underlying Mechanism}},
volume = {133},
year = {2014}
}
@article{Park2013,
abstract = {Humans and nonhuman animals share an approximate number system (ANS) that permits estimation and rough calculation of quantities without symbols. Recent studies show a correlation between the acuity of the ANS and performance in symbolic math throughout development and into adulthood, which suggests that the ANS may serve as a cognitive foundation for the uniquely human capacity for symbolic math. Such a proposition leads to the untested prediction that training aimed at improving ANS performance will transfer to improvement in symbolic-math ability. In the two experiments reported here, we showed that ANS training on approximate addition and subtraction of arrays of dots selectively improved symbolic addition and subtraction. This finding strongly supports the hypothesis that complex math skills are fundamentally linked to rudimentary preverbal quantitative abilities and provides the first direct evidence that the ANS and symbolic math may be causally related. It also raises the possibility that interventions aimed at the ANS could benefit children and adults who struggle with math.},
archivePrefix = {arXiv},
arxivId = {10.1177/0956797613482944},
author = {Park, Joonkoo and Brannon, Elizabeth M.},
doi = {10.1177/0956797613482944},
eprint = {0956797613482944},
file = {:home/andrew/Documents/grad/Papers/0956797613482944.pdf:pdf},
isbn = {0956-7976},
issn = {14679280},
journal = {Psychological Science},
keywords = {mathematical ability,number comprehension},
number = {10},
pages = {2013--2019},
pmid = {23921769},
primaryClass = {10.1177},
title = {{Training the Approximate Number System Improves Math Proficiency}},
volume = {24},
year = {2013}
}
@article{Gopnik2014,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Three studies explored whether and when children could categorize objects on the basis of a novel underlying causal power. To test this we constructed a "blicket detector," a machine that lit up and played music when cer-tain objects were placed on it. First, 2-, 3-and 4-year-old children saw that an object labeled as a "blicket" would set off the machine. In a categorization task, other objects were demonstrated on the machine. Some set it off and some did not. Children were asked to say which objects were "blickets." In an induction task, other objects were or were not labeled as "blickets." Children had to predict which objects would have the causal power to set off the machine. The causal power could conflict with perceptual properties of the object, such as color and shape. In an association task the object was associated with the machine's lighting up but did not cause it to light up. Even the youngest children sometimes used the causal power to determine the object's name rather than using its perceptual properties and sometimes used the object's name rather than its percep-tual properties to predict the object's causal powers. Children rarely categorized the object on the basis of the associated event. Young children also sometimes made interesting memory errors-they incorrectly reported that objects with the same perceptual features had had the same causal power. These studies demonstrate that even very young children will easily and swiftly learn about a new causal power of an object and spontane-ously use that information in classifying and naming the object.},
author = {Gopnik, Alison and Sobel, David M},
file = {:home/andrew/Documents/grad/Papers/1467-8624.00224.pdf:pdf},
journal = {Source: Child Development Child Development},
number = {5},
pages = {1205--1222},
title = {{Detecting Blickets: How Young Children Use Information about Novel Causal Powers in Categorization and Induction Detecting Blickets: HowYoung Children Use Information about Novel Causal Powers in Categorization and Induction}},
url = {http://www.jstor.org/stable/1131970{\%}0Ahttp://www.jstor.org/stable/1131970?seq=1{\&}cid=pdf-reference{\#}references{\_}tab{\_}contents{\%}0Ahttp://about.jstor.org/terms},
volume = {71},
year = {2014}
}
@article{Sobel2004,
abstract = {Previous research suggests that children can infer causal relations from patterns of events. However, what appear to be cases of causal inference may simply reduce to children recognizing relevant associations among events, and responding based on those associations. To examine this claim, in Experiments 1 and 2, children were introduced to a "blicket detector," a machine that lit up and played music when certain objects were placed upon it. Children observed patterns of contingency between objects and the machine's activation that required them to use indirect evidence to make causal inferences. Critically, associative models either made no predictions, or made incorrect predictions about these inferences. In general, children were able to make these inferences, but some developmental differences between 3- and 4-year-olds were found. We suggest that children's causal inferences are not based on recognizing associations, but rather that children develop a mechanism for Bayesian structure learning. Experiment 3 explicitly tests a prediction of this account. Children were asked to make an inference about ambiguous data based on the base rate of certain events occurring. Four-year-olds, but not 3-year-olds were able to make this inference. {\textcopyright} 2003 Cognitive Science Society, Inc. All rights reserved.},
author = {Sobel, David M. and Tenenbaum, Joshua B. and Gopnik, Alison},
doi = {10.1016/j.cogsci.2003.11.001},
file = {:home/andrew/Documents/grad/Papers/s15516709cog2803{\_}1.pdf:pdf},
isbn = {0364-0213},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Causal reasoning,Cognitive development},
number = {3},
pages = {303--333},
pmid = {16554755},
title = {{Children's causal inferences from indirect evidence: Backwards blocking and Bayesian reasoning in preschoolers}},
volume = {28},
year = {2004}
}
@article{Barsalou2007,
abstract = {Grounded cognition rejects traditional views that cognition is computation on amodal symbols in a modular system, independent of the brain's modal systems for perception, action, and introspection. Instead, grounded cognition proposes that modal simulations, bodily states, and situated action underlie cognition. Accumulating behavioral and neural evidence supporting this view is reviewed from research on perception, memory, knowledge, language, thought, social cognition, and development. Theories of grounded cognition are also reviewed, as are origins of the area and common misperceptions of it. Theoretical, empirical, and methodological issues are raised whose future treatment is likely to affect the growth and impact of grounded cognition.},
archivePrefix = {arXiv},
arxivId = {1407.5757},
author = {Barsalou, Lawrence W.},
doi = {10.1146/annurev.psych.59.103006.093639},
eprint = {1407.5757},
file = {:home/andrew/Documents/grad/Papers/b8.pdf:pdf},
isbn = {0-7695-2786-8},
issn = {0066-4308},
journal = {Annual Review of Psychology},
keywords = {cognitive architecture,imagery,representation,simulation},
number = {1},
pages = {617--645},
pmid = {17705682},
title = {{Grounded Cognition}},
volume = {59},
year = {2007}
}
@unpublished{Mcclelland2016,
author = {McClelland, James L and Mickey, Kevin and Hansen, Steven and Yuan, Arianna and Lu, Qihong},
file = {:home/andrew/Documents/grad/Papers/McCEtAl16MsPDPApproachToMathematicalCognition.pdf:pdf},
title = {{A Parallel-Distributed Processing Approach to Mathematical Cognition}},
year = {2016}
}
@article{Marmanis2016,
author = {Marmanis, Dimitrios and Datcu, Mihai and Esch, Thomas and Stilla, Uwe},
file = {:home/andrew/Documents/grad/Papers/Deep{\_}Learning{\_}Earth{\_}Observation{\_}Classification{\_}Using{\_}ImageNet{\_}Pre-trained{\_}Networks.pdf:pdf},
journal = {IEEE Geoscience and Remote Sensing Letters},
number = {1},
pages = {105--109},
title = {{Deep Learning Earth Observation Classification Using ImageNet Pretrained Networks}},
volume = {13},
year = {2016}
}
@article{Hinton1995,
author = {Hinton, Geoffrey E and Dayan, Peter and Frey, Brendan J and Neal, Radford M},
file = {:home/andrew/Documents/grad/Papers/ws.pdf:pdf},
journal = {Science},
title = {{The "wake-sleep" algorithm for unsupervised neural networks}},
volume = {268},
year = {1995}
}
@article{Ravichandran2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.04398v1},
author = {Ravichandran, Avinash and Bhotika, Rahul and Soatto, Stefano},
eprint = {arXiv:1905.04398v1},
file = {:home/andrew/Documents/grad/Papers/1905.04398.pdf:pdf},
journal = {arXiv preprint},
title = {{Few-Shot Learning with Embedded Class Models and Shot-Free Meta Training}},
year = {2019}
}
@article{Huh2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1608.08614v2},
author = {Huh, Minyoung and Agrawal, Pulkit and Efros, Alexei A.},
eprint = {arXiv:1608.08614v2},
file = {:home/andrew/Documents/grad/Papers/1608.08614.pdf:pdf},
journal = {arXiv preprint},
title = {{What makes ImageNet good for transfer learning?}},
year = {2016}
}
@article{Barlow1975,
author = {Barlow, H. B.},
file = {:home/andrew/Documents/grad/Papers/Barlow{\_}Nat75.pdf:pdf},
journal = {Nature},
title = {{Visual experience and cortical development}},
volume = {258},
year = {1975}
}
@article{Dinh2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.04933v2},
author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
eprint = {arXiv:1703.04933v2},
file = {:home/andrew/Documents/grad/Papers/1703.04933.pdf:pdf},
journal = {International Conference on Machine Learning},
title = {{Sharp Minima Can Generalize For Deep Nets}},
year = {2017}
}
@article{Morcos2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1906.02773v1},
author = {Morcos, Ari S and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
eprint = {arXiv:1906.02773v1},
file = {:home/andrew/Documents/grad/Papers/1906.02773.pdf:pdf},
journal = {arXiv preprint},
pages = {1--13},
title = {{One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers}},
year = {2019}
}
@article{Andrychowicz2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.01495v3},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and Mcgrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
eprint = {arXiv:1707.01495v3},
file = {:home/andrew/Documents/grad/Papers/1707.01495.pdf:pdf},
journal = {Neural Information Processing Systems},
number = {Nips},
title = {{Hindsight Experience Replay}},
year = {2017}
}
@inproceedings{Perez2019,
author = {P{\'{e}}rez, Guillermo Valle and Camargo, Chico Q and Louis, Ard A},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/047045cff29435c46508e8cd66055d206867c3e4.pdf:pdf},
title = {{Deep learning generalizes because the parameter-function map is biased towards simple functions}},
year = {2019}
}
@article{Kornblith2019,
author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V},
file = {:home/andrew/Documents/grad/Papers/Kornblith{\_}Do{\_}Better{\_}ImageNet{\_}Models{\_}Transfer{\_}Better{\_}CVPR{\_}2019{\_}paper.pdf:pdf},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {2661--2671},
title = {{Do Better ImageNet Models Transfer Better?}},
year = {2019}
}
@article{Nichol2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1803.02999v3},
author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
eprint = {arXiv:1803.02999v3},
file = {:home/andrew/Documents/grad/Papers/1803.02999.pdf:pdf},
journal = {arXiv preprint},
pages = {1--15},
title = {{On First-Order Meta-Learning Algorithms}},
year = {2018}
}
@article{Liu2019a,
archivePrefix = {arXiv},
arxivId = {arXiv:1901.08933v2},
author = {Liu, Shikun and Davison, Andrew J and Johns, Edward},
eprint = {arXiv:1901.08933v2},
file = {:home/andrew/Documents/grad/Papers/1901.08933.pdf:pdf},
journal = {arXiv preprint},
title = {{Self-Supervised Generalisation with Meta Auxiliary Learning}},
year = {2019}
}
@article{Locatello2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1811.12359v4},
author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R{\"{a}}tsch, Gunnar and Gelly, Sylvain and Sch{\"{o}}lkopf, Bernhard and Bachem, Olivier},
eprint = {arXiv:1811.12359v4},
file = {:home/andrew/Documents/grad/Papers/1811.12359.pdf:pdf},
journal = {Proceedings of the 36th International Conference on Machine Learning},
title = {{Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations}},
year = {2019}
}
@article{Xu2018a,
author = {Xu, Zhongwen and Silver, David},
file = {:home/andrew/Documents/grad/Papers/7507-meta-gradient-reinforcement-learning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Meta-Gradient Reinforcement Learning}},
year = {2018}
}
@article{Goldt,
archivePrefix = {arXiv},
arxivId = {arXiv:1906.08632v1},
author = {Goldt, Sebastian and Advani, Madhu S and Saxe, Andrew M and Krzakala, Florent and Zdeborov, Lenka},
eprint = {arXiv:1906.08632v1},
file = {:home/andrew/Documents/grad/Papers/1906.08632.pdf:pdf},
journal = {arXiv preprint},
title = {{Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup}}
}
@article{Petroski2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1712.06567v3},
author = {Petroski, Felipe and Vashisht, Such and Edoardo, Madhavan and Joel, Conti and Kenneth, Lehman and Jeff, O Stanley},
eprint = {arXiv:1712.06567v3},
file = {:home/andrew/Documents/grad/Papers/1712.06567.pdf:pdf},
journal = {arXiv preprint},
title = {{Deep Neuroevolution : Genetic Algorithms are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning}},
year = {2018}
}
@article{Dingle2018,
author = {Dingle, Kamaludin and Camargo, Chico Q and Louis, Ard A},
doi = {10.1038/s41467-018-03101-6},
file = {:home/andrew/Documents/grad/Papers/s41467-018-03101-6.pdf:pdf},
issn = {2041-1723},
journal = {Nature Communications},
number = {1},
publisher = {Springer US},
title = {{Inputâ€“output maps are strongly biased towards simple outputs}},
url = {http://dx.doi.org/10.1038/s41467-018-03101-6},
volume = {9},
year = {2018}
}
@article{Jiang2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1906.07343v1},
author = {Jiang, Yiding and Gu, Shixiang and Murphy, Kevin and Finn, Chelsea},
eprint = {arXiv:1906.07343v1},
file = {:home/andrew/Documents/grad/Papers/1906.07343.pdf:pdf},
journal = {arXiv preprint},
title = {{Language as an Abstraction for Hierarchical Deep Reinforcement Learning}},
year = {2019}
}
@inproceedings{Saxe2018a,
author = {Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/766e53de9a687876620f7372c97175003f9b89a3.pdf:pdf},
pages = {1--27},
title = {{On the Information Bottleneck Theory of Deep Learning}},
year = {2018}
}
@inproceedings{Bellemare2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.06887v1},
author = {Bellemare, Marc G and Dabney, Will},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
eprint = {arXiv:1707.06887v1},
file = {:home/andrew/Documents/grad/Papers/1707.06887.pdf:pdf},
title = {{A Distributional Perspective on Reinforcement Learning}},
year = {2017}
}
@inproceedings{Eysenbach2019,
author = {Eysenbach, Benjamin and Gupta, Abishek and Ibarz, Julian and Levine, Sergey},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/2d6a7564b82c143d515169e2a6f787a417379a47.pdf:pdf},
pages = {1--22},
title = {{Diversity is all you need: learning skills without a reward function}},
year = {2019}
}
@inproceedings{Vuong2019,
author = {Vuong, Quan and Ross, Keith and Zhang, Yiming},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/Documents/grad/Papers/39290ac8296a92b670e01d99790fb7d45a3a265e.pdf:pdf},
number = {2},
pages = {1--21},
title = {{Supervised policy update for deep reinforcement learning}},
year = {2019}
}
@article{Dehaene2017,
abstract = {The controversial question of whether machines may ever be conscious must be based on a careful consideration of how consciousness arises in the only physical system that undoubtedly possesses it: the human brain. We suggest that the word â€œconsciousnessâ€ conflates two different types of information-processing computations in the brain: the selection of information for global broadcasting, thus making it flexibly available for computation and report (C1, consciousness in the first sense), and the self-monitoring of those computations, leading to a subjective sense of certainty or error (C2, consciousness in the second sense). We argue that despite their recent successes, current machines are still mostly implementing computations that reflect unconscious processing (C0) in the human brain. We review the psychological and neural science of unconscious (C0) and conscious computations (C1 and C2) and outline how they may inspire novel machine architectures.},
author = {Dehaene, Stanislas and Lau, Hakwan and Kouider, Sid},
doi = {10.1126/science.aan8871},
file = {:home/andrew/Documents/grad/Papers/486.full.pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {6362},
pages = {484--489},
pmid = {29074769},
title = {{What is consciousness, and could machines have it?}},
volume = {358},
year = {2017}
}
@article{Richland2016,
author = {Richland, Lindsey Engle and Begolli, Kreshnik Nasi},
doi = {10.1177/2372732216629795},
file = {:home/andrew/Documents/grad/Papers/RichlandBegoli16AnalogyMath.pdf:pdf},
issn = {23727330},
journal = {Policy Insights from the Behavioral and Brain Sciences},
keywords = {CCSS mathematical practice standards,analogy,drawing connections,higher order thinking,mathematics education},
number = {2},
pages = {160--168},
title = {{Analogy and Higher Order Thinking: Learning Mathematics as an Example}},
volume = {3},
year = {2016}
}
@article{Kahn2018,
abstract = {Human learners are adept at grasping the complex relationships underlying incoming sequential input. In the present work, we formalize complex relationships as graph structures derived from temporal associations in motor sequences. Next, we explore the extent to which learners are sensitive to key variations in the topological properties inherent to those graph structures. Participants performed a probabilistic motor sequence task in which the order of button presses was determined by the traversal of graphs with modular, lattice-like, or random organization. Graph nodes each represented a unique button press and edges represented a transition between button presses. Results indicate that learning, indexed here by participants' response times, was strongly mediated by the graph's meso-scale organization, with modular graphs being associated with shorter response times than random and lattice graphs. Moreover, variations in a node's number of connections (degree) and a node's role in mediating long-distance communication (betweenness centrality) impacted graph learning, even after accounting for level of practice on that node. These results demonstrate that the graph architecture underlying temporal sequences of stimuli fundamentally constrains learning, and moreover that tools from network science provide a valuable framework for assessing how learners encode complex, temporally structured information.},
archivePrefix = {arXiv},
arxivId = {1709.03000},
author = {Kahn, Ari E. and Karuza, Elisabeth A. and Vettel, Jean M. and Bassett, Danielle S.},
doi = {10.1038/s41562-018-0463-8},
eprint = {1709.03000},
file = {:home/andrew/Documents/grad/Papers/1709.03000.pdf:pdf},
issn = {23973374},
journal = {Nature Human Behaviour},
pages = {1--29},
title = {{Network constraints on learnability of probabilistic motor sequences}},
year = {2018}
}
@article{Cleeremans1991,
abstract = {How is complex sequential material acquired, processed, and represented when there is no intention to learn? Two experiments exploring a choice reaction time task are reported. Unknown to Ss, successive stimuli followed a sequence derived from a "noisy" finite-state grammar. After considerable practice (60,000 exposures) with Experiment 1, Ss acquired a complex body of procedural knowledge about the sequential structure of the material. Experiment 2 was an attempt to identify limits on Ss ability to encode the temporal context by using more distant contingencies that spanned irrelevant material. Taken together, the results indicate that Ss become increasingly sensitive to the temporal context set by previous elements of the sequence, up to 3 elements. Responses are also affected by priming effects from recent trials. A connectionist model that incorporates sensitivity to the sequential structure and to priming effects is shown to capture key aspects of both acquisition and processing and to account for the interaction between attention and sequence structure reported by Cohen, Ivry, and Keele (1990).},
author = {Cleeremans, Axel and McClelland, James L.},
doi = {10.1037/0096-3445.120.3.235},
file = {:home/andrew/Documents/grad/Papers/CleeremansMcClelland91.pdf:pdf},
isbn = {0096-3445 (Print)},
issn = {00963445},
journal = {Journal of Experimental Psychology: General},
number = {3},
pages = {235--253},
pmid = {1836490},
title = {{Learning the Structure of Event Sequences}},
volume = {120},
year = {1991}
}
@inproceedings{Hill2019,
author = {Hill, Felix and Santoro, Adam and Barrett, David and Morcos, Ari and Lillicrap, Timothy},
booktitle = {ICLR},
file = {:home/andrew/Documents/grad/Papers/eb0500e9f7dc469ba25858dad0a6fdd4b8cf994b.pdf:pdf},
keywords = {rking t ogether to},
title = {{Learning to make analogies by contrasting abstract relational structure}},
year = {2019}
}
@article{Neyshabur2018,
abstract = {Despite existing work on ensuring generalization of neural networks in terms of scale sensitive complexity measures, such as norms, margin and sharpness, these complexity measures do not offer an explanation of why neural networks generalize better with over-parametrization. In this work we suggest a novel complexity measure based on unit-wise capacities resulting in a tighter generalization bound for two layer ReLU networks. Our capacity bound correlates with the behavior of test error with increasing network sizes, and could potentially explain the improvement in generalization with over-parametrization. We further present a matching lower bound for the Rademacher complexity that improves over previous capacity lower bounds for neural networks.},
archivePrefix = {arXiv},
arxivId = {1805.12076},
author = {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
eprint = {1805.12076},
file = {:home/andrew/Documents/grad/Papers/1805.12076.pdf:pdf},
journal = {arXiv preprint},
title = {{Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks}},
url = {http://arxiv.org/abs/1805.12076},
year = {2018}
}
@article{Saxe2018,
abstract = {An extensive body of empirical research has revealed remarkable regularities in the acquisition, organization, deployment, and neural representation of human semantic knowledge, thereby raising a fundamental conceptual question: what are the theoretical principles governing the ability of neural networks to acquire, organize, and deploy abstract knowledge by integrating across many individual experiences? We address this question by mathematically analyzing the nonlinear dynamics of learning in deep linear networks. We find exact solutions to this learning dynamics that yield a conceptual explanation for the prevalence of many disparate phenomena in semantic cognition, including the hierarchical differentiation of concepts through rapid developmental transitions, the ubiquity of semantic illusions between such transitions, the emergence of item typicality and category coherence as factors controlling the speed of semantic processing, changing patterns of inductive projection over development, and the conservation of semantic similarity in neural representations across species. Thus, surprisingly, our simple neural model qualitatively recapitulates many diverse regularities underlying semantic development, while providing analytic insight into how the statistical structure of an environment can interact with nonlinear deep learning dynamics to give rise to these regularities.},
archivePrefix = {arXiv},
arxivId = {1810.10531},
author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
doi = {arXiv:1810.10531v1},
eprint = {1810.10531},
file = {:home/andrew/Documents/grad/Papers/1810.10531.pdf:pdf},
issn = {21535965},
journal = {arXiv preprint},
title = {{A mathematical theory of semantic development in deep neural networks}},
url = {http://arxiv.org/abs/1810.10531},
year = {2018}
}
@article{Schapiro2012,
abstract = {Regularities are gradually represented in cortex after extensive experience [1], and yet they can influence behavior after minimal exposure [2, 3]. What kind of representations support such rapid statistical learning? The medial temporal lobe (MTL) can represent information from even a single experience [4], making it a good candidate system for assisting in initial learning about regularities. We combined anatomical segmentation of the MTL, high-resolution fMRI, and multivariate pattern analysis to identify representations of objects in cortical and hippocampal areas of human MTL, assessing how these representations were shaped by exposure to regularities. Subjects viewed a continuous visual stream containing hidden temporal relationships - pairs of objects that reliably appeared nearby in time. We compared the pattern of blood oxygen level-dependent activity evoked by each object before and after this exposure, and found that perirhinal cortex, parahippocampal cortex, subiculum, CA1, and CA2/CA3/dentate gyrus (CA2/3/DG) encoded regularities by increasing the representational similarity of their constituent objects. Most regions exhibited bidirectional associative shaping, whereas CA2/3/DG represented regularities in a forward-looking predictive manner. These findings suggest that object representations in MTL come to mirror the temporal structure of the environment, supporting rapid and incidental statistical learning. {\textcopyright} 2012 Elsevier Ltd. All rights reserved.},
author = {Schapiro, Anna C. and Kustner, Lauren V. and Turk-Browne, Nicholas B.},
doi = {10.1016/j.cub.2012.06.056},
file = {:home/andrew/Documents/grad/Papers/Schapiro{\_}CB{\_}2012.pdf:pdf},
isbn = {0960-9822},
issn = {09609822},
journal = {Current Biology},
number = {17},
pages = {1622--1627},
pmid = {22885059},
publisher = {Elsevier},
title = {{Shaping of object representations in the human medial temporal lobe based on temporal regularities}},
url = {http://dx.doi.org/10.1016/j.cub.2012.06.056},
volume = {22},
year = {2012}
}
@inproceedings{Lampinen2019,
author = {Lampinen, Andrew K and Ganguli, Surya},
booktitle = {International Conference on Learning Representations},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lampinen, Ganguli - 2019 - An analytic theory of generalization dynamics and transfer learning in deep linear networks.pdf:pdf},
pages = {1--20},
title = {{An analytic theory of generalization dynamics and transfer learning in deep linear networks}},
year = {2019}
}
@article{Baars2005,
abstract = {Global workspace (GW) theory emerged from the cognitive architecture tradition in cognitive science. Newell and co-workers were the first to show the utility of a GW or "blackboard" architecture in a distributed set of knowledge sources, which could cooperatively solve problems that no single constituent could solve alone. The empirical connection with conscious cognition was made by Baars (1988, 2002). GW theory generates explicit predictions for conscious aspects of perception, emotion, motivation, learning, working memory, voluntary control, and self systems in the brain. It has similarities to biological theories such as Neural Darwinism and dynamical theories of brain functioning. Functional brain imaging now shows that conscious cognition is distinctively associated with wide spread of cortical activity, notably toward frontoparietal and medial temporal regions. Unconscious comparison conditions tend to activate only local regions, such as visual projection areas. Frontoparietal hypometabolism is also implicated in unconscious states, including deep sleep, coma, vegetative states, epileptic loss of consciousness, and general anesthesia. These findings are consistent with the GW hypothesis, which is now favored by a number of scientists and philosophers. Copyright {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Baars, Bernard J.},
doi = {10.1016/S0079-6123(05)50004-9},
file = {:home/andrew/Documents/grad/Papers/10.1.1.456.2829.pdf:pdf},
isbn = {9780444518514},
issn = {00796123},
journal = {Progress in Brain Research},
pages = {45--53},
pmid = {16186014},
title = {{Global workspace theory of consciousness: Toward a cognitive neuroscience of human experience}},
volume = {150},
year = {2005}
}
@article{McCloskey1989,
author = {McCloskey, Michael and Cohen, Neal J},
journal = {Psychology of learning and motivation},
title = {{Catastrophic interference in connectionist networks: The sequential learning problem}},
volume = {24},
year = {1989}
}
@article{Morcos,
abstract = {Comparing different neural network representations and determining how represen-tations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method [22]. We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with di-verse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.},
archivePrefix = {arXiv},
arxivId = {1806.05759},
author = {Morcos, Ari S and Raghu, Maithra and Bengio, Samy},
eprint = {1806.05759},
file = {:home/andrew/Documents/grad/Papers/1806.05759.pdf:pdf},
title = {{Insights on representational similarity in neural networks with canonical correlation}},
url = {https://arxiv.org/pdf/1806.05759.pdf}
}
@article{Morcos2018,
abstract = {Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.},
archivePrefix = {arXiv},
arxivId = {1803.06959},
author = {Morcos, Ari S. and Barrett, David G. T. and Rabinowitz, Neil C. and Botvinick, Matthew},
eprint = {1803.06959},
file = {:home/andrew/Documents/grad/Papers/1803.06959.pdf:pdf},
pages = {1--15},
title = {{On the importance of single directions for generalization}},
url = {http://arxiv.org/abs/1803.06959},
year = {2018}
}
@article{Reed2017,
abstract = {Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet. However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks. In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset. Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.},
archivePrefix = {arXiv},
arxivId = {1710.10304},
author = {Reed, Scott and Chen, Yutian and Paine, Thomas and van den Oord, A{\"{a}}ron and Eslami, S. M. Ali and Rezende, Danilo and Vinyals, Oriol and de Freitas, Nando},
eprint = {1710.10304},
file = {:home/andrew/Documents/grad/Papers/1710.10304.pdf:pdf},
isbn = {9783942710374},
number = {2016},
pages = {1--11},
title = {{Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions}},
url = {http://arxiv.org/abs/1710.10304},
year = {2017}
}
@article{Alonso2016,
abstract = {Multitask learning has been applied successfully to a range of tasks, mostly morphosyntactic. However, little is known on when MTL works and whether there are data characteristics that help to determine its success. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary tasks, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, significant improvements are obtained only for 1 out of 5 tasks. When successful, auxiliary tasks with compact and more uniform label distributions are preferable.},
archivePrefix = {arXiv},
arxivId = {1612.02251},
author = {Alonso, H{\'{e}}ctor Mart{\'{i}}nez and Plank, Barbara},
eprint = {1612.02251},
file = {:home/andrew/Documents/grad/Papers/1612.02251.pdf:pdf},
isbn = {9781510838604},
title = {{When is multitask learning effective? Semantic sequence prediction under varying data conditions}},
url = {http://arxiv.org/abs/1612.02251},
year = {2016}
}
@article{Yoon2018,
abstract = {We propose a meta learning algorithm utilizing a linear transformer that carries out null-space projection of neural network outputs. The main idea is to construct a classification space such that the error signals during few-shot training are zero-forced on that space. The final decision on a test sample is obtained utilizing a null-space-projected distance measure between the network output and label-dependent weights that have been trained in the initial meta learning phase. Our meta learner achieves the best or near-best accuracies among known methods in few-shot image classification tasks with Omniglot and miniImageNet. In particular, our method shows stronger relative performance by significant margins as the classification task becomes more complicated.},
archivePrefix = {arXiv},
arxivId = {1806.01010},
author = {Yoon, Sung Whan and Seo, Jun and Moon, Jaekyun},
eprint = {1806.01010},
file = {:home/andrew/Documents/grad/Papers/1806.01010.pdf:pdf},
title = {{Meta Learner with Linear Nulling}},
year = {2018}
}
@article{Bingel2017,
abstract = {Multi-task learning (MTL) in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for labeled data. While it has brought significant improvements in a number of NLP tasks, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in NLP. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups.},
archivePrefix = {arXiv},
arxivId = {1702.08303},
author = {Bingel, Joachim and S{\o}gaard, Anders},
eprint = {1702.08303},
file = {:home/andrew/Documents/grad/Papers/1702.08303.pdf:pdf},
isbn = {9781510838604},
number = {2016},
title = {{Identifying beneficial task relations for multi-task learning in deep neural networks}},
url = {http://arxiv.org/abs/1702.08303},
year = {2017}
}
@article{Frans2017,
abstract = {We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.},
archivePrefix = {arXiv},
arxivId = {1710.09767},
author = {Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
doi = {10.1016/S0733-8619(03)00096-3},
eprint = {1710.09767},
file = {:home/andrew/Documents/grad/Papers/1710.09767.pdf:pdf},
isbn = {1359-7345},
issn = {07338619},
pages = {1--11},
pmid = {24239943},
title = {{Meta Learning Shared Hierarchies}},
url = {http://arxiv.org/abs/1710.09767},
year = {2017}
}
@article{Ren2018,
abstract = {In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would.},
archivePrefix = {arXiv},
arxivId = {1803.00676},
author = {Ren, Mengye and Triantafillou, Eleni and Ravi, Sachin and Snell, Jake and Swersky, Kevin and Tenenbaum, Joshua B. and Larochelle, Hugo and Zemel, Richard S.},
doi = {10.5121/ijci.2015.4227},
eprint = {1803.00676},
file = {:home/andrew/Documents/grad/Papers/1803.00676.pdf:pdf},
pages = {1--15},
title = {{Meta-Learning for Semi-Supervised Few-Shot Classification}},
url = {http://arxiv.org/abs/1803.00676},
year = {2018}
}
@article{Munkhdalai2017,
abstract = {Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6{\%} accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.},
archivePrefix = {arXiv},
arxivId = {1703.00837},
author = {Munkhdalai, Tsendsuren and Yu, Hong},
doi = {10.1093/mnrasl/slx008},
eprint = {1703.00837},
file = {:home/andrew/Documents/grad/Papers/1703.00837.pdf:pdf},
isbn = {9781510855144},
issn = {1938-7228},
journal = {arXiv preprint},
title = {{Meta Networks}},
url = {http://arxiv.org/abs/1703.00837},
year = {2017}
}
@article{Jamrozik2016,
abstract = {Embodied cognition accounts posit that concepts are grounded in our sensory and motor systems. An important challenge for these accounts is explaining how abstract concepts, which do not directly call upon sensory or motor information, can be informed by experience. We propose that metaphor is one important vehicle guiding the development and use of abstract concepts. Metaphors allow us to draw on concrete, familiar domains to acquire and reason about abstract concepts. Additionally, repeated metaphoric use drawing on particular aspects of concrete experience can result in the development of new abstract representations. These abstractions, which are derived from embodied experience but lack much of the sensorimotor information associated with it, can then be flexibly applied to understand new situations.},
author = {Jamrozik, Anja and McQuire, Marguerite and Cardillo, Eileen R. and Chatterjee, Anjan},
doi = {10.3758/s13423-015-0861-0},
file = {:home/andrew/Documents/grad/Papers/Jamrozik2016{\_}Article{\_}MetaphorBridgingEmbodimentToAb.pdf:pdf},
isbn = {1069-9384},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
keywords = {Abstract concepts,Abstraction,Embodiment,Grounded cognition,Metaphor},
number = {4},
pages = {1080--1089},
pmid = {27294425},
publisher = {Psychonomic Bulletin {\&} Review},
title = {{Metaphor: Bridging embodiment to abstraction}},
url = {http://dx.doi.org/10.3758/s13423-015-0861-0},
volume = {23},
year = {2016}
}
@article{Xu2017a,
abstract = {In this work, we propose a novel robot learning framework called Neural Task Programming (NTP), which bridges the idea of few-shot learning from demonstration and neural program induction. NTP takes as input a task specification (e.g., video demonstration of a task) and recursively decomposes it into finer sub-task specifications. These specifications are fed to a hierarchical neural program, where bottom-level programs are callable subroutines that interact with the environment. We validate our method in three robot manipulation tasks. NTP achieves strong generalization across sequential tasks that exhibit hierarchal and compositional structures. The experimental results show that NTP learns to generalize well to- wards unseen tasks with increasing lengths, variable topologies, and changing objectives.},
archivePrefix = {arXiv},
arxivId = {1710.01813},
author = {Xu, Danfei and Nair, Suraj and Zhu, Yuke and Gao, Julian and Garg, Animesh and Fei-Fei, Li and Savarese, Silvio},
eprint = {1710.01813},
file = {:home/andrew/Documents/grad/Papers/1710.01813.pdf:pdf},
isbn = {9781538630808},
title = {{Neural Task Programming: Learning to Generalize Across Hierarchical Tasks}},
url = {http://arxiv.org/abs/1710.01813},
year = {2017}
}
@article{Karmiloff-Smith1986,
abstract = {This paper explores possible relations obtaining between unconscious meta-processes and those available to conscious access and verbal statement. It is argued that the issue of conscious access must be conceptualized within a developmental perspective, in order to understand its function in human cognition. A theoretical framework is specified, in the form of a recurrent 3-phase model (differentiated from stage models), which stresses the distinction between implicitly defined representations and progressive representational explicitation at several levels of processing, culminating in the possibility of conscious access. The role of conscious access, as well as that of negative and positive feedback, are discussed in the light of a distinction drawn between models of developmental sequence and models of information processing flow in real time. Prominence is given to a success-based model of representational change as opposed to a failure-based model of behavioural change. The data consist of a detailed comparison of children's metalinguistic responses and spontaneous repairs. It is argued that metalinguistic awareness has little or no role to play in language acquisition macrodevelopmentally, a minor role to play in linguistic processing in real time, but that verbally encoded representations have an essential role to play in overall macrodevelopment. The implications of the model are briefly examined with respect to the representational status of the fluent language of some children with low IQ and that of fluent adult speakers of a non-native language. Consideration is given to the fact that some aspects of language, but not others, are available to conscious access. This leads to speculations with respect to the plausibility of considering modularity as a product of some aspects of development, rather than restricting modularity solely to innate givens. {\textcopyright} 1986.},
author = {Karmiloff-Smith, Annette},
doi = {10.1016/0010-0277(86)90040-5},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-0010027786900405-main.pdf:pdf},
isbn = {0010-0277},
issn = {00100277},
journal = {Cognition},
number = {2},
pages = {95--147},
pmid = {3742992},
title = {{From meta-processes to conscious access: Evidence from children's metalinguistic and repair data}},
volume = {23},
year = {1986}
}
@article{Patel2018,
abstract = {Abstract Mathematical cognition research has largely emphasized concepts that can be directly perceived or grounded in visuospatial referents. These include concrete number systems like natural numbers, integers, and rational numbers. Here, we investigate how a more abstract number system, the irrationals denoted by radical expressions like ï¿¼, is understood across three tasks. Performance on a magnitude comparison task suggests that people interpret irrational numbers (specifically, the radicands of radical expressions) as natural numbers. Strategy selfâ€reports during a number line estimation task reveal that the spatial locations of irrationals are determined by referencing neighboring perfect squares. Finally, perfect squares facilitate the evaluation of arithmetic expressions. These converging results align with a constellation of related phenomena spanning tasks and number systems of varying complexity. Accordingly, we propose that the taskâ€specific recruitment of more concrete representations to make sense of more abstract concepts (referential processing) is an important mechanism for teaching and learning mathematics.},
author = {Patel, Purav and Varma, Sashank},
doi = {10.1111/cogs.12619},
file = {:home/andrew/Documents/grad/Papers/Patel{\_}et{\_}al-2018-Cognitive{\_}Science.pdf:pdf},
issn = {15516709},
journal = {Cognitive Science},
keywords = {Arithmetic,Irrational number,Magnitude,Number line,Perfect square,Radicals,Referential processing,Strategy},
number = {5},
pages = {1642--1676},
title = {{How the Abstract Becomes Concrete: Irrational Numbers Are Understood Relative to Natural Numbers and Perfect Squares}},
volume = {42},
year = {2018}
}
@article{Battaglia2018,
abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning.},
archivePrefix = {arXiv},
arxivId = {1806.01261},
author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
eprint = {1806.01261},
file = {:home/andrew/Downloads/1806.01261.pdf:pdf},
pages = {1--38},
title = {{Relational inductive biases, deep learning, and graph networks}},
url = {http://arxiv.org/abs/1806.01261},
year = {2018}
}
@article{Don2016,
abstract = {Numerous tasks in learning and cognition have demonstrated differences in response patterns that may reflect the operation of two distinct systems. For example, causal and reinforcement learning tasks each show responding that considers abstract structure as well as responding based on simple associations. Nevertheless, there has been little attempt to verify whether these tasks are measuring related processes. The current study therefore investigated the relationship between rule- and feature-based generalization in a causal learning task, and model-based and model-free responding in a reinforcement learning task, including cognitive reflection as a predictor of individual tendencies to use controlled, deliberative processes in these tasks. We found that the use of rule-based generalization in a patterning task was a significant predictor of model-based, but not model-free, choice. Individual differences in cognitive reflection were significantly correlated with performance in both tasks, although this did not predict variation in model-based choice independently of rule-based generalization. Thus, although there is evidence of stable individual differences in the use of higher order processes across tasks, there may also be differences in mechanisms that these tasks reveal.},
author = {Don, Hilary J. and Goldwater, Micah B. and Otto, A. Ross and Livesey, Evan J.},
doi = {10.3758/s13423-016-1012-y},
file = {:home/andrew/Documents/grad/Papers/Don2016{\_}Article{\_}RuleAbstractionModel-basedChoi.pdf:pdf},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
keywords = {Associative learning,Cognitive control,Individual differences,Model-based vs. model-free reinforcement learning,Rule vs. feature generalization},
number = {5},
pages = {1615--1623},
pmid = {26907600},
publisher = {Psychonomic Bulletin {\&} Review},
title = {{Rule abstraction, model-based choice, and cognitive reflection}},
url = {http://dx.doi.org/10.3758/s13423-016-1012-y},
volume = {23},
year = {2016}
}
@article{Finn2018,
abstract = {Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems.},
archivePrefix = {arXiv},
arxivId = {1806.02817},
author = {Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
eprint = {1806.02817},
file = {:home/andrew/Documents/grad/Papers/1806.02817.pdf:pdf},
journal = {arXiv preprint},
title = {{Probabilistic Model-Agnostic Meta-Learning}},
url = {http://arxiv.org/abs/1806.02817},
year = {2018}
}
@article{Stadie2018,
abstract = {We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-{\$}\backslashtext{\{}RL{\}}{\^{}}2{\$}. Results are presented on a novel environment we call `Krazy World' and a set of maze environments. We show E-MAML and E-{\$}\backslashtext{\{}RL{\}}{\^{}}2{\$} deliver better performance on tasks where exploration is important.},
archivePrefix = {arXiv},
arxivId = {1803.01118},
author = {Stadie, Bradly C. and Yang, Ge and Houthooft, Rein and Chen, Xi and Duan, Yan and Wu, Yuhuai and Abbeel, Pieter and Sutskever, Ilya},
doi = {10.1051/0004-6361/201527329},
eprint = {1803.01118},
file = {:home/andrew/Documents/grad/Papers/1803.01118.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv preprint},
pmid = {23459267},
title = {{Some Considerations on Learning to Explore via Meta-Reinforcement Learning}},
url = {http://arxiv.org/abs/1803.01118},
year = {2018}
}
@incollection{Ben-David2003,
abstract = {The approach of learning of multiple "related" tasks simultaneously has proven quite successful in practice; however, theoretical justification for this success has remained elusive. The starting point for previous work on multiple task learning has been that the tasks to be learned jointly are somehow "algorithmically related", in the sense that the results of applying a specific learning algorithm to these tasks are assumed similar. We offer an alternative approach, defining relatedness of tasks on the basis of similarity betwen the example generating distributions that underline these tasks. We provid a formal framework for this notion of task relatedness, which captures a sub-domain of the wide scope of issues in which one may apply a multiple task learning approach. Our notion of task similarity is relevant to a variety of real life multitask learning scenarios and allows the formal derivation of generalization bounds that are strictly stronger than the previously known bounds for both the learning-to-learn and the multitask learning scenarios. We give precise conditions under which our bounds guarnatee generalization on the basis of smaller sample sizes and the standard single-task approach.},
author = {Ben-David, Shai and Schuller, Reba},
booktitle = {Learning theory and kernel machines},
doi = {10.1007/978-3-540-45167-9_41},
file = {:home/andrew/Documents/grad/Papers/colt03{\_}paper.pdf:pdf},
isbn = {0302-9743},
issn = {03029743},
pages = {567--580},
title = {{Exploiting Task Relatedness for Multiple Task Learning}},
url = {http://link.springer.com/10.1007/978-3-540-45167-9{\_}41},
year = {2003}
}
@article{Liu2018,
abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evo-lution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive ex-periments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.},
archivePrefix = {arXiv},
arxivId = {1806.09055},
author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
eprint = {1806.09055},
file = {:home/andrew/Downloads/1806.09055.pdf:pdf},
title = {{DARTS: Differentiable Architecture Search}},
url = {https://arxiv.org/pdf/1806.09055.pdf},
year = {2018}
}
@article{Ruder2017a,
abstract = {Multi-task learning is motivated by the observation that humans bring to bear what they know about related problems when solving new ones. Similarly, deep neural networks can profit from related tasks by sharing parameters with other networks. However, humans do not consciously decide to transfer knowledge between tasks. In Natural Language Processing (NLP), it is hard to predict if sharing will lead to improvements, particularly if tasks are only loosely related. To overcome this, we introduce Sluice Networks, a general framework for multi-task learning where trainable parameters control the amount of sharing. Our framework generalizes previous proposals in enabling sharing of all combinations of subspaces, layers, and skip connections. We perform experiments on three task pairs, and across seven different domains, using data from OntoNotes 5.0, and achieve up to 15{\%} average error reductions over common approaches to multi-task learning. We show that a) label entropy is predictive of gains in sluice networks, confirming findings for hard parameter sharing and b) while sluice networks easily fit noise, they are robust across domains in practice.},
archivePrefix = {arXiv},
arxivId = {1705.08142},
author = {Ruder, Sebastian and Bingel, Joachim and Augenstein, Isabelle and S{\o}gaard, Anders},
eprint = {1705.08142},
file = {:home/andrew/Documents/grad/Papers/1705.08142.pdf:pdf},
title = {{Learning what to share between loosely related tasks}},
url = {http://arxiv.org/abs/1705.08142},
year = {2017}
}
@article{Kondor2018,
abstract = {Most existing neural networks for learning graphs address permutation invariance by conceiving of the network as a message passing scheme, where each node sums the feature vectors coming from its neighbors. We argue that this imposes a limitation on their representation power, and instead propose a new general architecture for representing objects consisting of a hierarchy of parts, which we call Covariant Compositional Networks (CCNs). Here, covariance means that the activation of each neuron must transform in a specific way under permutations, similarly to steerability in CNNs. We achieve covariance by making each activation transform according to a tensor representation of the permutation group, and derive the corresponding tensor aggregation rules that each neuron must implement. Experiments show that CCNs can outperform competing methods on standard graph learning benchmarks.},
archivePrefix = {arXiv},
arxivId = {1801.02144},
author = {Kondor, Risi and Son, Hy Truong and Pan, Horace and Anderson, Brandon and Trivedi, Shubhendu},
eprint = {1801.02144},
file = {:home/andrew/Documents/grad/Papers/1801.02144.pdf:pdf},
pages = {1--19},
title = {{Covariant Compositional Networks For Learning Graphs}},
url = {http://arxiv.org/abs/1801.02144},
year = {2018}
}
@article{Ruder2017,
abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
archivePrefix = {arXiv},
arxivId = {1706.05098},
author = {Ruder, Sebastian},
doi = {10.1109/CVPR.2015.7299170},
eprint = {1706.05098},
file = {:home/andrew/Documents/grad/Papers/1706.05098.pdf:pdf},
isbn = {978-1-4673-6964-0},
number = {May},
title = {{An Overview of Multi-Task Learning in Deep Neural Networks}},
url = {http://arxiv.org/abs/1706.05098},
year = {2017}
}
@article{Schield1999,
abstract = {Statistical literacy is the ability to read and interpret data: the ability to use statistics as evidence in argu- ments. Statistical literacy is a competency: the ability to think critically about statistics. This introduction defines statistical literacy as a science of method, com- pares statistical literacy with traditional statistics and reviews some of the elements in reading and interpret- ing statistics. It gives more emphasis to observational studies than to experiments and thus to using associa- tions},
author = {Schield, Milo},
file = {:home/andrew/Documents/grad/Papers/1999SchieldAPDU.pdf:pdf},
journal = {Of Significance...},
keywords = {Critical Think- ing,Epistemology,Observational Studies,Strength of Belief,Teaching},
number = {1},
pages = {15--23},
title = {{Statistical Literacy: Thinking Critically about Statistics}},
url = {www.augsburg.edu/ppages/schield{\%}0Ahttp://web.augsburg.edu/{~}schield/MiloPapers/984StatisticalLiteracy6.pdf},
volume = {1},
year = {1999}
}
@article{Gerstenberg2016,
author = {Gerstenberg, Tobias and Tenenbaum, Joshua B},
file = {:home/andrew/Documents/grad/Papers/Understanding almost Empirical and computational studies of near misses, Gerstenberg, Tenenbaum, 2016.pdf:pdf},
journal = {Proceedings of the 38th Annual Conference of the Cognitive Science Society},
keywords = {all possible,almost,as one among many,causality,counterfactuals,intuitive physics,mental simula-,possible worlds,that picture,think of the actual,tion,we have come to,we need to repaint},
pages = {2777--2782},
title = {{Understanding â€œalmostâ€: Empirical and computational studies of near misses}},
year = {2016}
}
@article{Lagnado2013,
abstract = {How do people attribute responsibility in situations where the contributions of multiple agents combine to produce a joint outcome? The prevalence of over-determination in such cases makes this a difficult problem for counterfactual theories of causal responsibility. In this article, we explore a general framework for assigning responsibility in multiple agent contexts. We draw on the structural model account of actual causation (e.g., Halpern {\&} Pearl, 2005) and its extension to responsibility judgments (Chockler {\&} Halpern, 2004). We review the main theoretical and empiri-cal issues that arise from this literature and propose a novel model of intuitive judgments of responsibility. This model is a function of both pivotality (whether an agent made a difference to the outcome) and criticality (how important the agent is perceived to be for the outcome, before any actions are taken). The model explains empirical results from previous studies and is sup-ported by a new experiment that manipulates both pivotality and criticality. We also discuss possi-ble extensions of this model to deal with a broader range of causal situations. Overall, our approach emphasizes the close interrelations between causality, counterfactuals, and responsibility attributions.},
author = {Lagnado, David A. and Gerstenberg, Tobias and Zultan, Ro'i},
doi = {10.1111/cogs.12054},
file = {:home/andrew/Documents/grad/Papers/cogs.12054.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Attribution,Causality,Counterfactuals,Criticality,Pivotality,Responsibility,Shared responsibility},
number = {6},
pages = {1036--1073},
pmid = {23855451},
title = {{Causal responsibility and counterfactuals}},
volume = {37},
year = {2013}
}
@article{Gerstenberg2015,
abstract = {How do people make causal judgments? Here, we propose a counterfactual simulation model (CSM) of causal judgment that unifies different views on causation. The CSM predicts that people's causal judgments are influenced by whether a candidate cause made a difference to whether the outcome oc-curred as well as to how it occurred. We show how whether-causation and how-causation can be implemented in terms of different counterfactual contrasts defined over the same intu-itive generative model of the domain. We test the model in an intuitive physics domain where people make judgments about colliding billiard balls. Experiment 1 shows that participants' counterfactual judgments about what would have happened if one of the balls had been removed, are well-explained by an approximately Newtonian model of physics. In Experiment 2, participants judged to what extent two balls were causally re-sponsible for a third ball going through a gate or missing the gate. As predicted by the CSM, participants' judgments in-creased with their belief that a ball was a whether-cause, a how-cause, as well as sufficient for bringing about the out-come.},
author = {Gerstenberg, Tobias and Goodman, Noah D and Lagnado, David a and Tenenbaum, Joshua B},
file = {:home/andrew/Documents/grad/Papers/How, whether, why Causal judgments as counterfactual contrasts, Gerstenberg et al., 2015.pdf:pdf},
journal = {Proceedings of the 37th Annual Conference of the Cognitive Science Society},
keywords = {causality,counterfactuals,in-,mental simulation},
number = {1},
pages = {3--8},
title = {{How , whether , why : Causal judgments as counterfactual contrasts}},
volume = {1},
year = {2015}
}
@article{Pritzel2017,
abstract = {Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.},
archivePrefix = {arXiv},
arxivId = {1703.01988},
author = {Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Puigdom{\`{e}}nech, Adri{\`{a}} and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
doi = {10.1109/CVPR.2015.7298932},
eprint = {1703.01988},
file = {:home/andrew/Documents/grad/Papers/1703.01988.pdf:pdf},
isbn = {9781510829008},
issn = {10636919},
pmid = {16873662},
title = {{Neural Episodic Control}},
url = {http://arxiv.org/abs/1703.01988},
year = {2017}
}
@inproceedings{Wu2018,
author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella X. and Lin, Dahua},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
file = {:home/andrew/Documents/grad/Papers/Wu{\_}Unsupervised{\_}Feature{\_}Learning{\_}CVPR{\_}2018{\_}paper.pdf:pdf},
title = {{Unsupervised Feature Learning via Non-Parametric Instance Discrimination}},
year = {2018}
}
@article{Schuck2015,
abstract = {Many daily behaviors require us to actively focus on the current task and ignore all other distractions. Yet, ignoring everything else might hinder the ability to discover new ways to achieve the same goal. Here, we studied the neural mechanisms that support the spontaneous change to better strategies while an established strategy is executed. Multivariate neuroimaging analyses showed that before the spontaneous change to an alternative strategy, medial prefrontal cortex (MPFC) encoded information that was irrelevant for the current strategy butnecessary for the later strategy. Importantly, this neural effect was related to future behavioralchanges: information encoding in MPFC was changed only in participants who eventually switched their strategy and started before the actual strategy change. This allowed us to predict spontaneous strategy shifts ahead of time. These findings suggest that MPFC might internally simulate alternative strategies and shed new light on the organization of PFC.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Schuck, Nicolas W. and Gaschler, Robert and Wenke, Dorit and Heinzle, Jakob and Frensch, Peter A. and Haynes, John Dylan and Reverberi, Carlo},
doi = {10.1016/j.neuron.2015.03.015},
eprint = {15334406},
file = {:home/andrew/Documents/grad/Papers/Schuck{\_}2015{\_}Neuron.pdf:pdf},
isbn = {0896-6273},
issn = {10974199},
journal = {Neuron},
number = {1},
pages = {331--340},
pmid = {25819613},
title = {{Medial prefrontal cortex predicts internally driven strategy shifts}},
volume = {86},
year = {2015}
}
@article{Brock2018,
abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick", allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Frechet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.},
archivePrefix = {arXiv},
arxivId = {1809.11096},
author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
doi = {10.1016/j.jhep.2010.09.031},
eprint = {1809.11096},
file = {:home/andrew/Documents/grad/Papers/1809.11096.pdf:pdf},
isbn = {0276-34781098-108X},
issn = {0168-8278},
pages = {1--29},
pmid = {25980551},
title = {{Large Scale GAN Training for High Fidelity Natural Image Synthesis}},
url = {http://arxiv.org/abs/1809.11096},
year = {2018}
}
@article{Chen2018,
abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
archivePrefix = {arXiv},
arxivId = {1806.07366},
author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
doi = {arXiv:1806.07366v3},
eprint = {1806.07366},
file = {:home/andrew/Documents/grad/Papers/1806.07366.pdf:pdf},
isbn = {9781139108188},
number = {Nips},
pages = {1--18},
title = {{Neural Ordinary Differential Equations}},
url = {http://arxiv.org/abs/1806.07366},
year = {2018}
}
@article{Arpit2017,
abstract = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.},
archivePrefix = {arXiv},
arxivId = {1706.05394},
author = {Arpit, Devansh and Jastrz{\c{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
eprint = {1706.05394},
file = {:home/andrew/Documents/grad/Papers/1706.05394.pdf:pdf},
isbn = {9781510855144},
issn = {1938-7228},
journal = {arXiv preprint},
title = {{A Closer Look at Memorization in Deep Networks}},
url = {http://arxiv.org/abs/1706.05394},
year = {2017}
}
@article{Gabay2015,
abstract = {Objective: Developmental dyslexia is presumed to arise from specific phonological impairments. However, an emerging theoretical framework suggests that phonological impairments may be symptoms stemming from an underlying dysfunction of procedural learning. Method: We tested procedural learning in adults with dyslexia (n = 15) and matched-controls (n = 15) using 2 versions of the weather prediction task: feedback (FB) and paired-associate (PA). In the FB-based task, participants learned associations between cues and outcomes initially by guessing and subsequently through feedback indicating the correctness of response. In the PA-based learning task, participants viewed the cue and its associated outcome simultaneously without overt response or feedback. In both versions, participants trained across 150 trials. Learning was assessed in a subsequent test without presentation of the outcome, or corrective feedback. Results: The dyslexia group exhibited impaired learning compared with the control group on both the FB and PA versions of the weather prediction task. Conclusions: The results indicate that the ability to learn by feedback is not selectively impaired in dyslexia. Rather it seems that the probabilistic nature of the task, shared by the FB and PA versions of the weather prediction task, hampers learning in those with dyslexia. Results are discussed in light of procedural learning impairments among participants with dyslexia. (PsycINFO Database Record (c) 2015 APA, all rights reserved)},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Gabay, Yafit and Vakil, Eli and Schiff, Rachel and Holt, Lori L.},
doi = {10.1037/neu0000194},
eprint = {15334406},
file = {:home/andrew/Documents/grad/Papers/2015-08352-001.pdf:pdf},
isbn = {1931-1559(Electronic);0894-4105(Print)},
issn = {19311559},
journal = {Neuropsychology},
keywords = {Developmental dyslexia,Feedback,Paired-associate,Probabilistic category learning,Weather prediction task},
number = {6},
pages = {844--854},
pmid = {25730732},
title = {{Probabilistic category learning in developmental dyslexia: Evidence from feedback and paired-associate weather prediction tasks}},
volume = {29},
year = {2015}
}
@article{Dykstra1983,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. A commonly occurring problem in statistics is that of minimizing a least squares expression subject to side con-straints. Here a simple iterative algorithm is presented and shown to converge to the desired solution. Several examples are presented, including finding the closest con-cave (convex) function to a set of points and other general quadratic programming problems. The dual problem to the basic problem is also discussed and a solution for it is given in terms of the algorithm. Finally, extensions to expressions other than least squares are given.},
author = {Dykstra, Richard L},
doi = {10.1080/01621459.1983.10477029},
file = {:home/andrew/Documents/grad/Papers/2288193.pdf:pdf},
issn = {0162-1459},
journal = {Source Journal of the American Statistical Association},
keywords = {Concave (convex) functions,Convex cones,Dual convex cones,Least squares,Linear constraints,Mahalanobis distance,Projections,Re-stricted maximum likelihood,Regression},
number = {384},
pages = {837--842},
title = {{An Algorithm for Restricted Least Squares Regression}},
volume = {78},
year = {1983}
}
@article{Murez2017,
abstract = {We propose a general framework for unsupervised domain adaptation, which allows deep neural networks trained on a source domain to be tested on a different target domain without requiring any training annotations in the target domain. This is achieved by adding extra networks and losses that help regularize the features extracted by the backbone encoder network. To this end we propose the novel use of the recently proposed unpaired image-toimage translation framework to constrain the features extracted by the encoder network. Specifically, we require that the features extracted are able to reconstruct the images in both domains. In addition we require that the distribution of features extracted from images in the two domains are indistinguishable. Many recent works can be seen as specific cases of our general framework. We apply our method for domain adaptation between MNIST, USPS, and SVHN datasets, and Amazon, Webcam and DSLR Office datasets in classification tasks, and also between GTA5 and Cityscapes datasets for a segmentation task. We demonstrate state of the art performance on each of these datasets.},
archivePrefix = {arXiv},
arxivId = {1712.00479},
author = {Murez, Zak and Kolouri, Soheil and Kriegman, David and Ramamoorthi, Ravi and Kim, Kyungnam},
doi = {10.1109/CVPR.2018.00473},
eprint = {1712.00479},
file = {:home/andrew/Documents/grad/Papers/I2IAdapt.pdf:pdf},
title = {{Image to Image Translation for Domain Adaptation}},
url = {http://arxiv.org/abs/1712.00479},
year = {2017}
}
@article{Pollock,
author = {Pollock, D.S.G},
file = {:home/andrew/Documents/grad/Papers/restricted{\_}least{\_}squares.pdf:pdf},
pages = {1--2},
title = {{Restricted least-squares regression}}
}
@article{Lampinen2018,
author = {Lampinen, Andrew K. and Mcclelland, J L and Ganguli, Surya},
journal = {arXiv preprint},
title = {{An analytic theory of generalization dynamics and transfer in deep linear neural networks}},
year = {2018}
}
@article{Montavon2017,
abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
archivePrefix = {arXiv},
arxivId = {1512.02479},
author = {Montavon, Gr{\'{e}}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"{u}}ller, Klaus Robert},
doi = {10.1016/j.patcog.2016.11.008},
eprint = {1512.02479},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S0031320316303582-main.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Deep neural networks,Heatmapping,Image recognition,Relevance propagation,Taylor decomposition},
number = {May 2016},
pages = {211--222},
publisher = {Elsevier},
title = {{Explaining nonlinear classification decisions with deep Taylor decomposition}},
url = {http://dx.doi.org/10.1016/j.patcog.2016.11.008},
volume = {65},
year = {2017}
}
@article{Meyer2014,
abstract = {Repeated viewing of an image over days and weeks induces a marked reduction in the strength with which neurons in monkey inferotemporal cortex respond to it. The processing advantage that attaches to this reduction is unknown. One possibility is that truncation of the response to a familiar image leaves neurons in a state of readiness to respond to ensuing images and thereby enhances their ability to track rapidly changing displays. We explored this possibility by assessing neuronal responses to familiar and novel images in rapid serial visual displays. Inferotemporal neurons responded more strongly to familiar than to novel images in such displays. The effect was stronger among putative inhibitory neurons than among putative excitatory neurons. A comparable effect occurred at the level of the scalp potential in humans. We conclude that long-term familiarization sharpens the response dynamics of neurons in both monkey and human extrastriate visual cortex.},
author = {Meyer, Travis and Walker, Christopher and Cho, Raymond Y. and Olson, Carl R.},
doi = {10.1038/nn.3794},
file = {:home/andrew/Documents/grad/Papers/nn.3794.pdf:pdf},
isbn = {1097-6256},
issn = {15461726},
journal = {Nature Neuroscience},
number = {10},
pages = {1388--1394},
pmid = {25151263},
publisher = {Nature Publishing Group},
title = {{Image familiarization sharpens response dynamics of neurons in inferotemporal cortex}},
url = {http://dx.doi.org/10.1038/nn.3794},
volume = {17},
year = {2014}
}
@article{Goldin-Meadow1993,
author = {Goldin-Meadow, Susan and Alibali, Martha W and Church, R B},
file = {:home/andrew/Documents/grad/Papers/10.1.1.587.3211.pdf:pdf},
journal = {Psychological Review},
keywords = {age:children,cognition:development,cognition:learning,gesture:speech:mismatch,status:not},
number = {2},
pages = {279--297},
title = {{Transitions in concept acquisition: Using the hand to reach the mind}},
volume = {100},
year = {1993}
}
@article{Wakefield2018,
abstract = {{\textcopyright} 2018 John Wiley {\&} Sons Ltd. Teaching a new concept through gestures-hand movements that accompany speech-facilitates learning above-and-beyond instruction through speech alone (e.g., Singer {\&} Goldin-Meadow, ). However, the mechanisms underlying this phenomenon are still under investigation. Here, we use eye tracking to explore one often proposed mechanism-gesture's ability to direct visual attention. Behaviorally, we replicate previous findings: Children perform significantly better on a posttest after learning through Speech+Gesture instruction than through Speech Alone instruction. Using eye tracking measures, we show that children who watch a math lesson with gesture do allocate their visual attention differently from children who watch a math lesson without gesture-they look more to the problem being explained, less to the instructor, and are more likely to synchronize their visual attention with information presented in the instructor's speech (i.e., follow along with speech) than children who watch the no-gesture lesson. The striking finding is that, even though these looking patterns positively predict learning outcomes, the patterns do not mediate the effects of training condition (Speech Alone vs. Speech+Gesture) on posttest success. We find instead a complex relation between gesture and visual attention in which gesture moderates the impact of visual looking patterns on learning-following along with speech predicts learning for children in the Speech+Gesture condition, but not for children in the Speech Alone condition. Gesture's beneficial effects on learning thus come not merely from its ability to guide visual attention, but also from its ability to synchronize with speech and affect what learners glean from that speech.},
author = {Wakefield, Elizabeth and Novack, Miriam A. and Congdon, Eliza L. and Franconeri, Steven and Goldin-Meadow, Susan},
doi = {10.1111/desc.12664},
file = {:home/andrew/Documents/grad/Papers/Wakefield{\_}et{\_}al-2017-Developmental{\_}Science.pdf:pdf},
issn = {14677687},
journal = {Developmental Science},
number = {February},
pages = {1--12},
title = {{Gesture helps learners learn, but not merely by guiding their visual attention}},
year = {2018}
}
@article{Goldin-Meadow1999,
abstract = {People move their hands as they talk - they gesture. Gesturing is a robust phenomenon, found across cultures, ages, and tasks. Gesture is even found in individuals blind from birth. But what purpose, if any, does gesture serve? In this review, I begin by examining gesture when it stands on its own, substituting for speech and clearly serving a communicative function. When called upon to carry the full burden of communication, gesture assumes a language-like form, with structure at word and sentence levels. However, when produced along with speech, gesture assumes a different form - it becomes imagistic and analog. Despite its form, the gesture that accompanies speech also communicates. Trained coders can glean substantive information from gesture - information that is not always identical to that gleaned from speech. Gesture can thus serve as a research tool, shedding light on speakers' unspoken thoughts. The controversial question is whether gesture conveys information to listeners not trained to read them. Do spontaneous gestures communicate to ordinary listeners? Or might they be produced only for speakers themselves? I suggest these are not mutually exclusive functions - gesture serves as both a tool for communication for listeners, and a tool for thinking for speakers.},
author = {Goldin-Meadow, Susan},
doi = {10.1016/S1364-6613(99)01397-2},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S1364661399013972-main.pdf:pdf},
isbn = {1364-6613},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {11},
pages = {419--429},
pmid = {10529797},
title = {{The role of gesture in communication and thinking}},
volume = {3},
year = {1999}
}
@article{Benaych-Georges2012,
abstract = {In this paper, we consider the singular values and singular vectors of finite, low rank perturbations of large rectangular random matrices. Specifically, we prove almost sure convergence of the extreme singular values and appropriate projections of the corresponding singular vectors of the perturbed matrix.As in the prequel, where we considered the eigenvalues of Hermitian matrices, the non-random limiting value is shown to depend explicitly on the limiting singular value distribution of the unperturbed matrix via an integral transform that linearizes rectangular additive convolution in free probability theory. The asymptotic position of the extreme singular values of the perturbed matrix differs from that of the original matrix if and only if the singular values of the perturbing matrix are above a certain critical threshold which depends on this same aforementioned integral transform.We examine the consequence of this singular value phase transition on the associated left and right singular eigenvectors and discuss the fluctuations of the singular values around these non-random limits. {\textcopyright} 2012 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {1103.2221},
author = {Benaych-Georges, Florent and Nadakuditi, Raj Rao},
doi = {10.1016/j.jmva.2012.04.019},
eprint = {1103.2221},
file = {:home/andrew/Documents/grad/Papers/1103.2221.pdf:pdf},
isbn = {00018708},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {Free probability,Haar measure,Phase transition,Random eigenvalues,Random eigenvectors,Random matrices,Random perturbation,Sample covariance matrices},
pages = {120--135},
title = {{The singular values and vectors of low rank perturbations of large rectangular random matrices}},
volume = {111},
year = {2012}
}
@phdthesis{McIntyre2007,
annote = {Notes a potentially important gap between level of explanation and level of questions.},
author = {McIntyre, Richard N. I.},
booktitle = {Unpublished masters thesis},
file = {:home/andrew/Documents/grad/Papers/McIntyre{\_}unpublished{\_}masters.pdf:pdf},
keywords = {Math education,Van Hiele},
title = {{Analysing geometry in the Classroom Mathematics and Mind Action Series mathematics textbooks using the van Hiele levels}},
year = {2007}
}
@inproceedings{Achille2018,
abstract = {Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.},
archivePrefix = {arXiv},
arxivId = {1808.06508},
author = {Achille, Alessandro and Eccles, Tom and Matthey, Loic and Burgess, Christopher P. and Watters, Nick and Lerchner, Alexander and Higgins, Irina},
booktitle = {Advances in Neural Information Processing Systems},
doi = {10.1007/BF01952291},
eprint = {1808.06508},
file = {:home/andrew/Documents/grad/Papers/1808.06508.pdf:pdf},
isbn = {0250-8095 (Print)},
issn = {00144754},
pmid = {3239590},
title = {{Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies}},
url = {http://arxiv.org/abs/1808.06508},
year = {2018}
}
@book{Devlin2012,
author = {Devlin, Keith},
booktitle = {Journal of Experimental Psychology: General},
file = {:home/andrew/Documents/grad/Papers/curso-Devlin.pdf:pdf},
isbn = {9780615653631},
keywords = {Math education},
number = {1},
title = {{Introduction to Mathematical Thinking}},
volume = {136},
year = {2012}
}
@article{Constantinescu2016,
author = {Constantinescu, Alexandra O and O'Reilly, Jill X and Behrens, Timothy E. J.},
doi = {10.1126/science.aaf0941.Organizing},
file = {:home/andrew/Documents/grad/Papers/emss-71012.pdf:pdf},
journal = {Science},
number = {6292},
pages = {1464--1468},
title = {{Organizing conceptual knowledge in humans with a gridlike code.}},
volume = {352},
year = {2016}
}
@article{Cang2017,
abstract = {Although deep learning approaches have had tremendous success in image, video and audio processing, computer vision, and speech recognition, their applications to three-dimensional (3D) biomolecular structural data sets have been hindered by the geometric and biological complexity. To address this problem we introduce the element-specific persistent homology (ESPH) method. ESPH represents 3D complex geometry by one-dimensional (1D) topological invariants and retains important biological information via a multichannel image-like representation. This representation reveals hidden structure-function relationships in biomolecules. We further integrate ESPH and deep convolutional neural networks to construct a multichannel topological neural network (TopologyNet) for the predictions of protein-ligand binding affinities and protein stability changes upon mutation. To overcome the deep learning limitations from small and noisy training sets, we propose a multi-task multichannel topological convolutional neural network (MM-TCNN). We demonstrate that TopologyNet outperforms the latest methods in the prediction of protein-ligand binding affinities, mutation induced globular protein folding free energy changes, and mutation induced membrane protein folding free energy changes. Availability: weilab.math.msu. edu/TDL/ Author summary The predictions of biomolecular functions and properties from biomolecular structures are of fundamental importance in computational biophysics. The structural and biological complexities of biomolecules and their interactions hinder successful predictions. Machine learning has become an important tool for such predictions. Recent advances in deep learning architectures, particularly convolutional neural network (CNN), have profoundly impacted a number of disciplines, such as image classification and voice recognition. Though CNN can be directly applied to molecular sciences by using a three-dimensional (3D) image-like brute-force representation, it is computationally intractable when applied to large biomolecules and large datasets. We propose a topological strategy PLOS Computational Biology | https://doi.},
author = {Cang, Zixuan and Wei, Guo-Wei},
doi = {10.1371/journal.pcbi.1005690},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cang, Wei - 2017 - TopologyNet Topology based deep convolutional and multi-task neural networks for biomolecular property predictions.pdf:pdf},
isbn = {1111111111},
title = {{TopologyNet: Topology based deep convolutional and multi-task neural networks for biomolecular property predictions}},
year = {2017}
}
@article{Elsayed2018,
abstract = {Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.},
archivePrefix = {arXiv},
arxivId = {1802.08195},
author = {Elsayed, Gamaleldin F. and Shankar, Shreya and Cheung, Brian and Papernot, Nicolas and Kurakin, Alex and Goodfellow, Ian and Sohl-Dickstein, Jascha},
eprint = {1802.08195},
file = {:home/andrew/Documents/grad/Papers/1802.08195.pdf:pdf},
title = {{Adversarial Examples that Fool both Computer Vision and Time-Limited Humans}},
url = {http://arxiv.org/abs/1802.08195},
year = {2018}
}
@article{Laje2013,
abstract = {The brain's ability to tell time and produce complex spatiotemporal motor patterns is critical for anticipating the next ring of a telephone or playing a musical instrument. One class of models proposes that these abilities emerge from dynamically changing patterns of neural activity generated in recurrent neural networks. However, the relevant dynamic regimes of recurrent networks are highly sensitive to noise; that is, chaotic. We developed a firing rate model that tells time on the order of seconds and generates complex spatiotemporal patterns in the presence of high levels of noise. This is achieved through the tuning of the recurrent connections. The network operates in a dynamic regime that exhibits coexisting chaotic and locally stable trajectories. These stable patterns function as 'dynamic attractors' and provide a feature that is characteristic of biological systems: the ability to 'return' to the pattern being generated in the face of perturbations.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Laje, Rodrigo and Buonomano, Dean V.},
doi = {10.1038/nn.3405},
eprint = {NIHMS150003},
file = {:home/andrew/Documents/grad/Papers/nn.3405.pdf:pdf},
isbn = {1097-6256},
issn = {10976256},
journal = {Nature Neuroscience},
number = {7},
pages = {925--933},
pmid = {23708144},
publisher = {Nature Publishing Group},
title = {{Robust timing and motor patterns by taming chaos in recurrent neural networks}},
url = {http://dx.doi.org/10.1038/nn.3405},
volume = {16},
year = {2013}
}
@article{Tai2015,
abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
archivePrefix = {arXiv},
arxivId = {1503.00075},
author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
doi = {10.1515/popets-2015-0023},
eprint = {1503.00075},
file = {:home/andrew/Documents/grad/Papers/1503.00075.pdf:pdf},
isbn = {9781941643723},
issn = {9781941643723},
pmid = {18267787},
title = {{Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks}},
url = {http://arxiv.org/abs/1503.00075},
year = {2015}
}
@article{Karpathy2017,
abstract = {We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.},
archivePrefix = {arXiv},
arxivId = {1412.2306},
author = {Karpathy, Andrej and Fei-Fei, Li},
doi = {10.1109/TPAMI.2016.2598339},
eprint = {1412.2306},
file = {:home/andrew/Documents/grad/Papers/cvpr2015.pdf:pdf},
isbn = {9781467369640},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Image captioning,deep neural networks,language model,recurrent neural network,visual-semantic embeddings},
number = {4},
pages = {664--676},
pmid = {16873662},
title = {{Deep Visual-Semantic Alignments for Generating Image Descriptions}},
volume = {39},
year = {2017}
}
@article{Kumar2015,
abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.},
archivePrefix = {arXiv},
arxivId = {1506.07285},
author = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
doi = {10.1017/CBO9781107415324.004},
eprint = {1506.07285},
file = {:home/andrew/Documents/grad/Papers/1506.07285.pdf:pdf},
isbn = {9781510829008},
issn = {1938-7228},
pmid = {25246403},
title = {{Ask Me Anything: Dynamic Memory Networks for Natural Language Processing}},
url = {http://arxiv.org/abs/1506.07285},
year = {2015}
}
@article{Shazeer2017,
abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
archivePrefix = {arXiv},
arxivId = {1701.06538},
author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
doi = {10.1051/0004-6361/201527329},
eprint = {1701.06538},
file = {:home/andrew/Documents/grad/Papers/1701.06538.pdf:pdf},
isbn = {9781611970685},
issn = {0004-6361},
pages = {1--19},
pmid = {23459267},
title = {{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}},
url = {http://arxiv.org/abs/1701.06538},
year = {2017}
}
@article{Smolensky2012,
abstract = {Is thought computation over ideas? Turing, and many cognitive scientists since, have assumed so, and formulated computational systems in which meaningful concepts are encoded by symbols which are the objects of computation. Cognition has been carved into parts, each a function defined over such symbols. This paper reports on a research program aimed at computing these symbolic functions without computing over the symbols. Symbols are encoded as patterns of numerical activation over multiple abstract neurons, each neuron simultaneously contributing to the encoding of multiple symbols. Computation is carried out over the numerical activation values of such neurons, which individually have no conceptual meaning. This is massively parallel numerical computation operating within a continuous computational medium. The paper presents an axiomatic framework for such a computational account of cognition, including a number of formal results. Within the framework, a class of recursive symbolic functions can be computed. Formal languages defined by symbolic rewrite rules can also be specified, the subsymbolic computations producing symbolic outputs that simultaneously display central properties of both facets of human language: universal symbolic grammatical competence and statistical, imperfect performance.},
author = {Smolensky, Paul},
doi = {10.1098/rsta.2011.0334},
file = {:home/andrew/Documents/grad/Papers/3543.full.pdf:pdf},
issn = {1364503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Cognitive science,Connectionism,Language,Linguistics,Neural networks,Optimality theory},
number = {1971},
pages = {3543--3569},
pmid = {22711873},
title = {{Symbolic functions from neural computation}},
volume = {370},
year = {2012}
}
@article{Smolensky1990,
abstract = {A general method, the tensor product representation, is defined for the connectionist representation of value/variable bindings. The technique is a formalization of the idea that a set of value/variable pairs can be represented by accumulating activity in a collection of units each of which computes the product of a feature of a variable and a feature of its value. The method allows the fully distributed representation of bindings and symbolic structures. Fully and partially localized special cases of the tensor product representation reduce to existing cases of connectionist representations of structured data. The representation rests on a principled analysis of structure; it saturates gracefully as larger structures are represented; it permits recursive construction of complex representations from simpler ones; it respects the independence of the capacities to generate and maintain multiple bindings in parallel; it extends naturally to continuous structures and continuous representational patterns; it permits values to also serve as variables; and it enables analysis of the interference of symbolic structures stored in associative memories. It has also served as the basis for working connectionist models of high-level cognitive tasks. {\textcopyright} 1990.},
author = {Smolensky, Paul},
doi = {10.1016/0004-3702(90)90007-M},
file = {:home/andrew/Documents/grad/Papers/Smolensky{\_}1990{\_}TensorProductVariableBinding.AI.pdf:pdf},
isbn = {9780262256360},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1-2},
pages = {159--216},
title = {{Tensor product variable binding and the representation of symbolic structures in connectionist systems}},
volume = {46},
year = {1990}
}
@article{Manning2018,
author = {Manning, Charles and Smolensky, Paul},
journal = {Annual Review of Linguistics},
title = {{Integrating symbolic and neural computation}},
year = {2018}
}
@article{Elsayed,
abstract = {Deep neural networks are susceptible to adversarial attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as identifying a panda as a gibbon or confusing a cat with a computer. Previous adversarial examples have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce adversarial attacks that instead reprogram the target model to perform a task chosen by the attackerâ€”without the attacker needing to specify or compute the desired output for each test-time input. This attack is accomplished by optimizing for a single adversarial perturbation, of unrestricted magnitude, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary when processing these inputsâ€”even if the model was not trained to do this task. These perturbations can be thus considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as two classification tasks: classification of MNIST and CIFAR-10 examples presented within the input to the ImageNet model.},
archivePrefix = {arXiv},
arxivId = {1806.11146},
author = {Elsayed, Gamaleldin F and Goodfellow, Ian and Sohl-Dickstein, Jascha and Brain, Google},
eprint = {1806.11146},
file = {:home/andrew/Documents/grad/Papers/1806.11146.pdf:pdf},
journal = {arXiv preprint},
title = {{Adversarial Reprogramming of Neural Networks}},
url = {https://arxiv.org/pdf/1806.11146.pdf},
year = {2018}
}
@article{Fernandez2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1803.03834v1},
author = {Fernandez, Roland and CelikyÄ±lmaz, Asli and Singh, Rishabh and Smolensky, Paul},
eprint = {arXiv:1803.03834v1},
file = {:home/andrew/Documents/grad/Papers/1803.03834.pdf:pdf},
journal = {Workshop paper at ICLR 2018},
pages = {1--4},
title = {{Learning and analyzing vector encoding of symbolic representations}},
year = {2018}
}
@article{Smolensky2014,
author = {Smolensky, Paul and Goldrick, Matthew},
doi = {10.1111/cogs.12047},
file = {:home/andrew/Documents/grad/Papers/cogs.12047.pdf:pdf},
journal = {Cognitive Science},
keywords = {a research pro-,combinatorial structure,distributed representation,gram that was already,harmonic grammar,optimization,path for carrying out,selection,sketched by 1986 1,speech errors,the study discussed here,was developed as one},
pages = {1102--1138},
title = {{Optimization and Quantization in Gradient Symbol Systems : A Framework for Integrating the Continuous and the Discrete in Cognition}},
volume = {38},
year = {2014}
}
@inproceedings{Huang2017,
abstract = {We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture --- the Tensor Product Generation Network (TPGN) --- is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.},
archivePrefix = {arXiv},
arxivId = {1709.09118},
author = {Huang, Qiuyuan and Smolensky, Paul and He, Xiaodong and Deng, Li and Wu, Dapeng},
booktitle = {Proceedings of the NAACL-HLT},
eprint = {1709.09118},
file = {:home/andrew/Documents/grad/Papers/N18-1114.pdf:pdf},
pages = {1263--1273},
title = {{Tensor Product Generation Networks for Deep NLP Modeling}},
url = {http://arxiv.org/abs/1709.09118},
year = {2017}
}
@article{Yu2018,
abstract = {Humans and animals are capable of learning a new behavior by observing others perform the skill just once. We consider the problem of allowing a robot to do the same -- learning from a raw video pixels of a human, even when there is substantial domain shift in the perspective, environment, and embodiment between the robot and the observed human. Prior approaches to this problem have hand-specified how human and robot actions correspond and often relied on explicit human pose detection systems. In this work, we present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated. We show experiments on both a PR2 arm and a Sawyer arm, demonstrating that after meta-learning, the robot can learn to place, push, and pick-and-place new objects using just one video of a human performing the manipulation.},
archivePrefix = {arXiv},
arxivId = {1802.01557},
author = {Yu, Tianhe and Finn, Chelsea and Xie, Annie and Dasari, Sudeep and Zhang, Tianhao and Abbeel, Pieter and Levine, Sergey},
eprint = {1802.01557},
file = {:home/andrew/Documents/grad/Papers/1802.01557.pdf:pdf},
title = {{One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning}},
url = {http://arxiv.org/abs/1802.01557},
year = {2018}
}
@article{Smolensky1988,
annote = {Can't find... yet},
author = {Smolensky, Paul},
file = {:home/andrew/Documents/grad/Papers/smolensky.proper.treat.pdf:pdf},
journal = {Behavioral and Brain Sciences1},
keywords = {cognition,computation,connectionism,dynamical systems,networks,neural models,parallel distributed processing},
pages = {1--74},
title = {{On the proper treatment of connectionism}},
volume = {11},
year = {1988}
}
@article{Burstein1986,
abstract = {This chapter presents a model of learning by analogical reasoning. The model is based on two main ideas, namely, (1) that the analogies used in learning about an unfamiliar domain depend heavily on the use of previously formed causal abstrac-tions in a familiar or base domain; (2) that these analogies are extended incremenrally to handle related situations. CARL is a computer program that learns about the semantics of assignment statements for the BASIC programming language. It is described as an illustration of causally driven analogical reasoning and learning. The model maps and debugs inferences drawn from several commonly used analogies to assignment in response to presented examples. It has often been said among A1 researchers that learning something new requires knowing a lot about it already. This is certainly true for learning by analogy. This chapter shows how prior knowledge can be applied in one specific kind of learning by analogy, namely, the formation of new concepts in an unfamiliar domain},
author = {Burstein, M H},
file = {:home/andrew/Documents/grad/Papers/f46fed78b05dd3b883ecc7ea93790b3c68ab.pdf:pdf},
journal = {Machine Learning II: An Artificial Intelligence Approach},
title = {{Concept formation by incremental analogical reasoning and debugging}},
year = {1986}
}
@article{Fisher2018,
abstract = {The current study quantified the degree to which group data are able to describe individual participants. We utilized intensive repeated-measures dataâ€”data that have been collected many times, across many individualsâ€”to compare the distributions of bivariate correlations calculated within subjects vs. those calculated between subjects. Because the vast majority of social and medical science research aggregates across subjects, we aimed to assess how closely such aggregations reflect their constituent individuals. We provide evidence that conclusions drawn from aggregated data may be worryingly imprecise. Specifically, the variance in individuals is up to four times larger than in groups. These data call for a focus on idiography and open science that may substantially alter best-practice guidelines in the medical and behavioral sciences.

Only for ergodic processes will inferences based on group-level data generalize to individual experience or behavior. Because human social and psychological processes typically have an individually variable and time-varying nature, they are unlikely to be ergodic. In this paper, six studies with a repeated-measure design were used for symmetric comparisons of interindividual and intraindividual variation. Our results delineate the potential scope and impact of nonergodic data in human subjects research. Analyses across six samples (with 87â€“94 participants and an equal number of assessments per participant) showed some degree of agreement in central tendency estimates (mean) between groups and individuals across constructs and data collection paradigms. However, the variance around the expected value was two to four times larger within individuals than within groups. This suggests that literatures in social and medical sciences may overestimate the accuracy of aggregated statistical estimates. This observation could have serious consequences for how we understand the consistency between group and individual correlations, and the generalizability of conclusions between domains. Researchers should explicitly test for equivalence of processes at the individual and group level across the social and medical sciences.},
author = {Fisher, Aaron J. and Medaglia, John D. and Jeronimus, Bertus F.},
doi = {10.1073/pnas.1711978115},
file = {:home/andrew/Documents/grad/Papers/1711978115.full.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
pages = {201711978},
pmid = {29915059},
title = {{Lack of group-to-individual generalizability is a threat to human subjects research}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1711978115},
year = {2018}
}
@article{Clark1993,
abstract = {Considers the hypothesis that there is a joint in the natural order such that humans fall on one side and many other systems, including sophisticated information processors, fall on the other. The joint marks a pivotal difference in internal organization. The representational redescription (RR) model embodies specific hypotheses about the nature of this joint. This article discusses a number of connectionist models, limits of the implicit representations they generate, and how they fall short of what the RR model calls for. The authors provide empirical support for the increasing cognitive flexibility that the RR process makes possible in human development. The discussion outlines consequences for connectionist modeling of the idea of a conservative, multileveled, redescriptive architecture. The wider philosophical implications of the RR model are also presented. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
author = {Clark, ANDY and Karmiloff-Smith, ANNETTE},
doi = {10.1111/j.1468-0017.1993.tb00299.x},
file = {:home/andrew/Documents/grad/Papers/Cognizer.pdf:pdf},
isbn = {1468-0017},
issn = {14680017},
journal = {Mind {\&} Language},
number = {4},
pages = {487--519},
title = {{The Cognizer's Innards: A Psychological and Philosophical Perspective on the Development of Thought}},
volume = {8},
year = {1993}
}
@article{Chen1999,
author = {Chen, Zhe and Klahr, David},
file = {:home/andrew/Documents/grad/Papers/1132052.pdf:pdf},
journal = {Child Development},
number = {5},
pages = {1098--1120},
title = {{All Other Things Being Equal : Acquisition and Transfer of the Control of Variables Strategy}},
volume = {70},
year = {1999}
}
@article{Gentner1998,
author = {Gentner, Dedre},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S001002779800002X-main.pdf:pdf},
keywords = {analogical learning,and that we are,cries out for explana-,rule-based processing,similarity-based processing,struc-,than anybody else,ture-sensitive comparison,we are far cleverer},
pages = {263--297},
title = {{Similarity and the development of rules}},
volume = {65},
year = {1998}
}
@article{Kemp2007,
author = {Kemp, Charles and Perfors, Amy and Tenenbaum, Joshua B},
doi = {10.1111/j.1467-7687.2007.00585.x},
file = {:home/andrew/Documents/grad/Papers/devsci07{\_}kempetal.pdf:pdf},
pages = {307--321},
title = {{Learning overhypotheses with hierarchical Bayesian models}},
volume = {3},
year = {2007}
}
@article{Barnett2002a,
author = {Barnett, Susan M and Ceci, Stephen J},
doi = {10.1037//0033-2909.128.4.612},
file = {:home/andrew/Documents/grad/Papers/barnett{\_}2002.pdf:pdf},
number = {4},
pages = {612--637},
title = {{When and Where Do We Apply What We Learn ? A Taxonomy for Far Transfer}},
volume = {128},
year = {2002}
}
@article{Aslin2012,
abstract = {Statistical learning is a rapid and robust mechanism that enables adults and infants to extract patterns of stimulation embedded in both language and visual domains. Importantly, statistical learning operates implicitly, without instruction, through mere exposure to a set of input stimuli. However, much of what learners must acquire about a structured domain consists of principles or rules that can be applied to novel inputs. Although it has been claimed that statistical learning and rule learning are separate mechanisms, here we review evidence and provide a unifying perspective that argues for a single mechanism of statistical learning that accounts for both the learning of the input stimuli and the generalization to novel instances. The balance between instance-learning and generalization is based on two factors: the strength of perceptual biases that highlight structural regularities, and the consistency of unique versus overlapping contexts in the input.},
author = {Aslin, Richard N and Newport, Elissa L},
doi = {10.1177/0963721412436806},
file = {:home/andrew/Documents/grad/Papers/0963721412436806.pdf:pdf},
isbn = {0963-7214},
issn = {09637214},
journal = {Current Directions in Psychological Science},
keywords = {generalization,infants,rule learning,statistical learning},
number = {3},
pages = {170--176},
title = {{Statistical Learning: From Acquiring Specific Items to Forming General Rules}},
volume = {21},
year = {2012}
}
@article{Hummel2011,
author = {Hummel, John E},
doi = {10.1080/09540091.2011.569880},
file = {:home/andrew/Documents/grad/Papers/Getting symbols out of a neural architecture.pdf:pdf},
number = {May},
title = {{Getting symbols out of a neural architecture}},
volume = {0091},
year = {2011}
}
@article{Advani2016,
author = {Advani, Madhu and Ganguli, Surya},
doi = {10.1103/PhysRevX.6.031034},
file = {:home/andrew/Documents/grad/Papers/PhysRevX.6.031034.pdf:pdf},
keywords = {complex systems,interdisciplinary physics},
number = {June},
pages = {1--16},
title = {{Statistical Mechanics of Optimal Convex Inference in High Dimensions}},
volume = {031034},
year = {2016}
}
@inproceedings{Schulman2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.05477v5},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter},
booktitle = {International Conference on Machine Learning},
eprint = {arXiv:1502.05477v5},
file = {:home/andrew/Documents/grad/Papers/1502.05477.pdf:pdf},
pages = {1889--1897},
title = {{Trust region policy optimization}},
year = {2015}
}
@book{Quine1969,
annote = {Green or philosophy B840 . Q49},
author = {Quine, W. V.},
title = {{Ontological relativity, and other essays}},
year = {1969}
}
@book{Goodman1972,
annote = {Philosophy library B29 .G62},
author = {Goodman, Nelson},
title = {{Problems and projects}},
year = {1972}
}
@article{Burgoon2013,
author = {Burgoon, Erin M and Henderson, Marlone D and Markman, Arthur B},
doi = {10.1177/1745691613497964},
file = {:home/andrew/Documents/grad/Papers/1745691613497964.pdf:pdf},
keywords = {abstraction,abstraction was initially a,and much,cognitive and developmental psychology,concreteness,construal,early years of,general,gist,global,hot topic in the,representation,specificity},
title = {{There Are Many Ways to See the Forest for the Trees : A Tour Guide for Abstraction}},
year = {2013}
}
@article{Tenenbaum2001,
abstract = {Shepard has argued that a universal law should govern generalization across different domains of perception and cognition, as well as across organisms from different species or even different planets. Starting with some basic assumptions about natural kinds, he derived an exponential decay function as the form of the universal generalization gradient, which accords strikingly well with a wide range of empirical data. However, his original formulation applied only to the ideal case of generalization from a single encountered stimulus to a single novel stimulus, and for stimuli that can be represented as points in a continuous metric psychological space. Here we recast Shepard's theory in a more general Bayesian framework and show how this naturally extends his approach to the more realistic situation of generalizing from multiple consequential stimuli with arbitrary representational structure. Our framework also subsumes a version of Tversky's set-theoretic model of similarity, which is conventionally thought of as the primary alternative to Shepard's continuous metric space model of similarity and generalization. This unification allows us not only to draw deep parallels between the set-theoretic and spatial approaches, but also to significantly advance the explanatory power of set-theoretic models.},
author = {Tenenbaum, Joshua B. and Griffiths, Thomas L.},
doi = {10.1017/S0140525X01000061},
file = {:home/andrew/Documents/grad/Papers/tenenbaum{\_}griffiths01.pdf:pdf},
isbn = {0140-525X (Print)$\backslash$n0140-525X (Linking)},
issn = {0140525X},
journal = {Behavioral and Brain Sciences},
keywords = {Addictive clustering,Bayesian inference,Categorization,Concept learning,Constrast model,Features,Generalization,Psychological space,Similarity},
number = {4},
pages = {629--640},
pmid = {12048947},
title = {{Generalization, similarity, and Bayesian inference}},
volume = {24},
year = {2001}
}
@article{Finn2016,
abstract = {Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.},
archivePrefix = {arXiv},
arxivId = {1603.00448},
author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
doi = {1603.00448v3},
eprint = {1603.00448},
file = {:home/andrew/Documents/grad/Papers/1603.00448.pdf:pdf},
isbn = {9781510829008},
title = {{Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization}},
url = {http://arxiv.org/abs/1603.00448},
volume = {48},
year = {2016}
}
@article{Luo2017,
abstract = {We propose a framework that learns a representation transferable across different domains and tasks in a label efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition.},
archivePrefix = {arXiv},
arxivId = {1712.00123},
author = {Luo, Zelun and Zou, Yuliang and Hoffman, Judy and Fei-Fei, Li},
eprint = {1712.00123},
file = {:home/andrew/Documents/grad/Papers/1712.00123.pdf:pdf},
issn = {10495258},
number = {Nips},
title = {{Label Efficient Learning of Transferable Representations across Domains and Tasks}},
url = {http://arxiv.org/abs/1712.00123},
year = {2017}
}
@article{Kaiser2017,
abstract = {Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.},
archivePrefix = {arXiv},
arxivId = {1706.05137},
author = {Kaiser, Lukasz and Gomez, Aidan N. and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
doi = {10.1007/s11263-015-0816-y},
eprint = {1706.05137},
file = {:home/andrew/Documents/grad/Papers/1706.05137.pdf:pdf},
isbn = {0920-5691},
issn = {0920-5691},
pmid = {16190471},
title = {{One Model To Learn Them All}},
url = {http://arxiv.org/abs/1706.05137},
year = {2017}
}
@article{Kokkinos2017,
abstract = {In this work we introduce a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a unified architecture that is trained end-to-end. Such a universal network can act like a `swiss knife' for vision tasks; we call this architecture an UberNet to indicate its overarching nature. We address two main technical challenges that emerge when broadening up the range of tasks handled by a single CNN: (i) training a deep architecture while relying on diverse training sets and (ii) training many (potentially unlimited) tasks with a limited memory budget. Properly addressing these two problems allows us to train accurate predictors for a host of tasks, without compromising accuracy. Through these advances we train in an end-to-end manner a CNN that simultaneously addresses (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) human part segmentation (f) semantic boundary detection, (g) region proposal generation and object detection. We obtain competitive performance while jointly addressing all of these tasks in 0.7 seconds per frame on a single GPU. A demonstration of this system can be found at http://cvn.ecp.fr/ubernet/.},
archivePrefix = {arXiv},
arxivId = {1609.02132},
author = {Kokkinos, Iasonas},
doi = {10.1109/CVPR.2017.579},
eprint = {1609.02132},
file = {:home/andrew/Documents/grad/Papers/1609.02132.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {5454--5463},
title = {{UberNet: Training a universal convolutional neural network for Low-, Mid-, and high-level vision using diverse datasets and limited memory}},
volume = {2017-Janua},
year = {2017}
}
@article{Niculescu-Mizil2007,
abstract = {We consider the problem of learning Bayes Net$\backslash$n$\backslash$nstructures for related tasks. We present an algorithm for learning$\backslash$nBayes Net structures that takes$\backslash$n$\backslash$nadvantage of the similarity between tasks by biasing learning toward$\backslash$nsimilar structures for each$\backslash$n$\backslash$ntask. Heuristic search is used to ï¬nd a high scoring set of structures$\backslash$n(one for each task), where the$\backslash$n$\backslash$nscore for a set of structures is computed in a principled way. Experiments$\backslash$non problems generated$\backslash$n$\backslash$nfrom the ALARM and INSURANCE networks$\backslash$n$\backslash$nshow that learning the structures for related tasks$\backslash$n$\backslash$nusing the proposed method yields better results$\backslash$n$\backslash$nthan learning the structures independently.},
author = {Niculescu-Mizil, Alexandru and Caruana, Rich},
file = {:home/andrew/Documents/grad/Papers/niculescu.mtlbnets.aistats07.pdf:pdf},
journal = {International Conference on Artificial Intelligence and Statistics},
number = {1},
pages = {339--346},
title = {{Inductive Transfer for Bayesian Network Structure Learning.}},
volume = {14853},
year = {2007}
}
@article{Duan2016,
abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL{\$}{\^{}}2{\$}, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL{\$}{\^{}}2{\$} experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL{\$}{\^{}}2{\$} is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL{\$}{\^{}}2{\$} on a vision-based navigation task and show that it scales up to high-dimensional problems.},
archivePrefix = {arXiv},
arxivId = {1611.02779},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
doi = {10.1051/0004-6361/201527329},
eprint = {1611.02779},
file = {:home/andrew/Documents/grad/Papers/1611.02779.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv preprint},
pages = {1--14},
pmid = {23459267},
title = {{RL{\$}{\^{}}2{\$}: Fast Reinforcement Learning via Slow Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.02779},
year = {2016}
}
@article{Silver2008,
abstract = {Inductive transfer or transfer learning refers to the problem of retaining and applying the knowledge learned in one or more tasks to develop efficiently an effective hypothesis for a new task. While all learning involves generalization across problem instances, transfer learning emphasizes the transfer of knowledge across domains, tasks, and distributions that are similar but not the same. For example, learning to recognize chairs might help to recognize tables; or learning to play checkers might improve the learning of chess. While people are adept at inductive transfer, even across widely disparate domains, we have only begun to develop associated computational learning theory and there are few machine learning systems that exhibit knowledge transfer. Inductive transfer invokes some of the most important questions in artificial intelligence. Amongst its challenges are questions such as: What is the best representation and method for retaining learned background knowledge? How does one index into such knowledge? What is the best representation and method for transferring prior knowledge to a new task? How does the use of prior knowledge affect hypothesis search heuristics? What is the nature of similarity or relatedness between tasks for the purposes of learning? Can it be measured? What role does curriculum play in the sequential learning of tasks? The papers in this special issue consider several of these problems. (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
author = {Silver, Daniel L. and Bennett, Kristin P.},
doi = {10.1007/s10994-008-5087-1},
file = {:home/andrew/Documents/grad/Papers/Silver-Bennett2008{\_}Article{\_}GuestEditorSIntroductionSpecia.pdf:pdf},
isbn = {1099400850871},
issn = {08856125},
journal = {Machine Learning},
number = {3},
pages = {215--220},
title = {{Guest editor's introduction: Special issue on inductive transfer learning}},
volume = {73},
year = {2008}
}
@article{Wang2016a,
abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
archivePrefix = {arXiv},
arxivId = {1611.05763},
author = {Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
eprint = {1611.05763},
file = {:home/andrew/Documents/grad/Papers/1611.05763.pdf:pdf},
journal = {arXiv preprint},
pages = {1--17},
title = {{Learning to reinforcement learn}},
url = {http://arxiv.org/abs/1611.05763},
year = {2016}
}
@article{Tessler2016,
abstract = {We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two techniques: (1) a deep skill array and (2) skill distillation, our novel variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill distillation enables the HDRLN to efficiently retain knowledge and therefore scale in lifelong learning, by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network. The H-DRLN exhibits superior performance and lower learning sample complexity compared to the regular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft.},
archivePrefix = {arXiv},
arxivId = {1604.07255},
author = {Tessler, Chen and Givony, Shahar and Zahavy, Tom and Mankowitz, Daniel J. and Mannor, Shie},
eprint = {1604.07255},
file = {:home/andrew/Documents/grad/Papers/1604.07255.pdf:pdf},
journal = {arXiv preprint},
title = {{A Deep Hierarchical Approach to Lifelong Learning in Minecraft}},
url = {http://arxiv.org/abs/1604.07255},
year = {2016}
}
@article{Finn2016a,
abstract = {Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while deployed. However, this learning requires access to a reward function, which is often hard to measure in real-world domains, where the reward could depend on, for example, unknown positions of objects or the emotional state of the user. Conversely, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present or in a controlled setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect on its own? In this paper, we formalize this problem as semisupervised reinforcement learning, where the reward function can only be evaluated in a set of "labeled" MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of "unlabeled" MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent's own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.},
archivePrefix = {arXiv},
arxivId = {1612.00429},
author = {Finn, Chelsea and Yu, Tianhe and Fu, Justin and Abbeel, Pieter and Levine, Sergey},
doi = {10.1051/0004-6361/201527329},
eprint = {1612.00429},
file = {:home/andrew/Documents/grad/Papers/1612.00429.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pages = {1--11},
pmid = {23459267},
title = {{Generalizing Skills with Semi-Supervised Reinforcement Learning}},
url = {http://arxiv.org/abs/1612.00429},
year = {2016}
}
@inproceedings{Finn2017,
abstract = {In order for a robot to be a generalist that can perform a wide range of jobs, it must be able to acquire a wide variety of skills quickly and efficiently in complex unstructured environments. High-capacity models such as deep neural networks can enable a robot to represent complex skills, but learning each skill from scratch then becomes infeasible. In this work, we present a meta-imitation learning method that enables a robot to learn how to learn more efficiently, allowing it to acquire new skills from just a single demonstration. Unlike prior methods for one-shot imitation, our method can scale to raw pixel inputs and requires data from significantly fewer prior tasks for effective learning of new skills. Our experiments on both simulated and real robot platforms demonstrate the ability to learn new tasks, end-to-end, from a single visual demonstration.},
archivePrefix = {arXiv},
arxivId = {1709.04905},
author = {Finn, Chelsea and Yu, Tianhe and Zhang, Tianhao and Abbeel, Pieter and Levine, Sergey},
booktitle = {1st Conference on Robot Learning},
eprint = {1709.04905},
file = {:home/andrew/Documents/grad/Papers/1709.04905.pdf:pdf},
isbn = {9781510829008},
issn = {1938-7228},
keywords = {deep learning,imitation learning,meta-learning,one-shot learning},
pages = {1--12},
pmid = {9404875},
title = {{One-Shot Visual Imitation Learning via Meta-Learning}},
url = {http://arxiv.org/abs/1709.04905},
year = {2017}
}
@article{Zamir2018,
abstract = {Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity. We proposes a fully computational approach for modeling the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3 (compared to training independently) while keeping the performance nearly the same. We provide a set of tools for computing and probing this taxonomical structure including a solver that users can employ to devise efficient supervision policies for their use cases.},
archivePrefix = {arXiv},
arxivId = {1804.08328},
author = {Zamir, Amir and Sax, Alexander and Shen, William and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
doi = {10.1109/CVPR.2018.00391},
eprint = {1804.08328},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zamir et al. - 2018 - Taskonomy Disentangling Task Transfer Learning.pdf:pdf;:home/andrew/Documents/grad/Papers/1804.08328.pdf:pdf},
title = {{Taskonomy: Disentangling Task Transfer Learning}},
url = {http://arxiv.org/abs/1804.08328},
year = {2018}
}
@article{Gorban,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.01547v2},
author = {Gorban, Alexander N and Sofeikov, Konstantin I},
eprint = {arXiv:1709.01547v2},
file = {:home/andrew/Documents/grad/Papers/1709.01547.pdf:pdf},
keywords = {approximation,learning,measure concentration,neural networks},
title = {{Knowledge Transfer Between Artificial Intelligence Systems}}
}
@article{Bourne1970,
abstract = {In 2 experiments, 78 and 48 undergraduates, respectively, solved several problems in which the object was to find the correct conceptual rule (out of 4 rules conjunctive, disjunctive, conditional, or biconditional) for sorting of geometrical designs. Sizable general positive intra- and interrule transfer effects were observed. The effects were traceable to the acquisition by Ss of a simple yet general problem-solving strategy based on the bidimensional logical truth table, and suggest a tentative hierarchical model of the sophisticated S's knowledge and skill (cognition and competence) based on the embeddedness and generative character of concepts. (PsycINFO Database Record (c) 2006 APA, all rights reserved)},
author = {Bourne, Lyle E.},
doi = {10.1037/h0030000},
file = {:home/andrew/Documents/grad/Papers/1971-03496-001.pdf:pdf},
issn = {0033295X},
journal = {Psychological Review},
keywords = {problem solving using correct conceptual rule, int},
number = {6},
pages = {546--556},
title = {{Knowing and using concepts}},
volume = {77},
year = {1970}
}
@article{Scholkopf2012,
abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernelfunctions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinearmap; for instance the space of all possible d pixel products in images.We give the derivation of the method and present experimental resultson polynomial feature extraction for pattern recognition.},
author = {Scholkopf, B and Smola, a J and Muller, K R},
doi = {10.1162/089976698300017467},
file = {:home/andrew/Documents/grad/Papers/scholkopf{\_}kernel.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Computer Vision And Mathematical Methods In Medical And Biomedical Image Analysis},
pages = {583--588},
pmid = {21939901},
title = {{Kernel Principal Component Analysis}},
url = {http://www.tribesandclimatechange.org/docs/tribes{\_}450.pdf{\%}5Cnpapers2://publication/uuid/D79004C6-EBB3-42CE-9F15-120270D985BE},
volume = {1327},
year = {2012}
}
@article{Ovsjanikov2012,
abstract = {We present a novel representation of maps between pairs of shapes that allows for efficient inference and manipulation. Key to our approach is a generalization of the notion of map that puts in correspondence real-valued functions rather than points on the shapes. By choosing a multi-scale basis for the function space on each shape, such as the eigenfunctions of its Laplace-Beltrami operator, we obtain a representation of a map that is very compact, yet fully suitable for global inference. Perhaps more remarkably, most natural constraints on a map, such as descriptor preservation, landmark correspondences, part preservation and operator commutativity become linear in this formulation. Moreover, the representation naturally supports certain algebraic operations such as map sum, difference and composition, and enables a number of applications, such as function or annotation transfer without establishing point-to-point correspondences. We exploit these properties to devise an efficient shape matching method, at the core of which is a single linear solve. The new method achieves state-of-the-art results on an isometric shape matching benchmark. We also show how this representation can be used to improve the quality of maps produced by existing shape matching methods, and illustrate its usefulness in segmentation transfer and joint analysis of shape collections.},
author = {Ovsjanikov, Maks and Ben-Chen, Mirela and Solomon, Justin and Butscher, Adrian and Guibas, Leonidas},
doi = {10.1145/2185520.2185526},
file = {:home/andrew/Documents/grad/Papers/obsbg{\_}fmaps.pdf:pdf},
isbn = {0730-0301},
issn = {0730-0301},
journal = {ACM Trans. Graph.},
keywords = {correspondence,representation,shape matching},
pages = {30:1----30:11},
title = {{Functional maps: a flexible representation of maps between shapes}},
url = {http://doi.acm.org/10.1145/2185520.2185526},
volume = {31},
year = {2012}
}
@article{Arora2018a,
abstract = {Conventional wisdom in deep learning states that increasing depth improves expressiveness but complicates optimization. This paper suggests that, sometimes, increasing depth can speed up optimization. The effect of depth on optimization is decoupled from expressiveness by focusing on settings where additional layers amount to overparameterization - linear neural networks, a well-studied model. Theoretical analysis, as well as experiments, show that here depth acts as a preconditioner which may accelerate convergence. Even on simple convex problems such as linear regression with {\$}\backslashell{\_}p{\$} loss, {\$}p{\textgreater}2{\$}, gradient descent can benefit from transitioning to a non-convex overparameterized objective, more than it would from some common acceleration schemes. We also prove that it is mathematically impossible to obtain the acceleration effect of overparametrization via gradients of any regularizer.},
archivePrefix = {arXiv},
arxivId = {1802.06509},
author = {Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
eprint = {1802.06509},
file = {:home/andrew/Documents/grad/Papers/1802.06509.pdf:pdf},
title = {{On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization}},
url = {http://arxiv.org/abs/1802.06509},
year = {2018}
}
@article{Bennett2015,
abstract = {We show how the wide range in strengths of intensifying de- gree adverbs (e.g. very and extremely) could be explained by pragmatic inference based on differing cost, rather than differ- ing semantics. This predicts a linear relationship between the meaning of intensifiers and their length and log-frequency. We test this prediction in two studies, using two different depen- dent measures, finding that higher cost does predict stronger meanings. We discuss the implications for adverbial meaning and the more general question of how extensive non-arbitrary form-meaning association may be in language. Keywords:},
author = {Bennett, Erin and Goodman, Noah D},
doi = {10.1016/j.cognition.2018.05.011},
file = {:home/andrew/Documents/grad/Papers/BennettGoodman2018.pdf:pdf},
issn = {0010-0277},
journal = {CogSci},
keywords = {degree adverbs,intensifiers,m-implicature,pragmatics,scalar adjectives},
number = {August 2015},
pages = {226--231},
publisher = {Elsevier},
title = {{Extremely costly intensifiers are stronger than quite costly ones}},
url = {https://doi.org/10.1016/j.cognition.2018.05.011},
volume = {178},
year = {2015}
}
@article{Zhang2006,
abstract = {Despite the tiny brain of the honeybee, some remarkable higher cognitive functions have emerged from this assembly of about one million neurons. Work on the honeybee over the past decade is beginning to suggest that insects may not be the simple, reflexive creatures that they were once believed to be. Bees display perceptual and â€œcognitiveâ€ capacities that are surprisingly rich, complex and flexible. This article reviews the recent progress on the honeybee's ability to learn and use abstract rules and concepts, to categorize visual objects in various ways, and to memorize task-specific information while navigating through their environment. This review is not intended to be exhaustive. Rather, it highlights important advances in our understanding of the processes underlying the bee's remarkable behaviors.},
author = {Zhang, Shaowu},
file = {:home/andrew/Documents/grad/Papers/qt3nr4d3r6.pdf:pdf},
isbn = {6126125509},
journal = {International Journal of Comparative Psychology},
number = {3},
title = {{Learning of abstract concepts and rules by the honeybee}},
url = {http://escholarship.org/uc/item/3nr4d3r6.pdf{\%}5Cnhttp://escholarship.org/uc/item/3nr4d3r6},
volume = {19},
year = {2006}
}
@article{Rosholm2017,
annote = {This paper sucks. Fucking non-randomized "experiment," and not as far of transfer as the title/abstract would make it sound.},
author = {Rosholm, Michael and Bj, Mai and Gumede, Kamilla},
file = {:home/andrew/Documents/grad/Papers/journal.pone.0177257.pdf:pdf},
isbn = {1111111111},
pages = {1--18},
title = {{Your move : The effect of chess on mathematics test scores}},
year = {2017}
}
@article{Green2014,
author = {Green, C Shawn and Strobach, Tilo and Schubert, Torsten},
doi = {10.1007/s00426-013-0535-3},
file = {:home/andrew/Documents/grad/Papers/10.1007{\%}2Fs00426-013-0535-3.pdf:pdf},
journal = {Psychological Research},
pages = {756--772},
title = {{On methodological standards in training and transfer experiments}},
volume = {78},
year = {2014}
}
@article{Vapnik2017,
author = {Vapnik, Vladimir and Izmailov, Rauf},
doi = {10.1007/s10472-017-9538-x},
file = {:home/andrew/Documents/grad/Papers/10.1007{\%}2Fs10472-017-9538-x.pdf:pdf},
isbn = {9550151050},
keywords = {2010,68q32,68t05,68t30,83c32,Intelligent teacher,Privileged information,Similar,classification,frames,intelligent teacher,knowledge,knowledge representation,learning theory,mathematics subject classification,neural network,privileged information,regression,similarity control,support vector machine,transfer},
number = {February},
pages = {3--19},
publisher = {Annals of Mathematics and Artificial Intelligence},
title = {{Knowledge transfer in SVM and neural networks}},
year = {2017}
}
@article{Zetzsche2009,
abstract = {We investigate the relation between the physical world and its mental representation in the 'cognitive map', and test if this representation is image-like and complies with the laws of Euclidean geometry. We have developed a new experimental technique using 'impossible' virtual environments (VE) to directly influence the representational development. Subjects explore a number of VEs -- some 'normal', others with severe violations of Euclidean metrics or planar topology. We check if these manipulated properties cause problems in navigation performance. A consistent VE should be easily represented mentally in a map-like fashion, while a VE with severe violations should prove difficult. Surprisingly, we found no substantial influence of the impossible VEs on navigation performance, and forced-choice tests showed little evidence that subjects were aware of manipulations. This suggests that the representation does not resemble a two-dimensional image-like map. Alternatives to consider are sensorimotor and graph-like representations.},
author = {Zetzsche, Christoph and Wolter, Johannes and Galbraith, Christopher and Schill, Kerstin},
doi = {10.1163/156856809789476074},
file = {:home/andrew/Documents/grad/Papers/v22n5{\_}splitsection5.pdf:pdf},
isbn = {0169-1015},
issn = {01691015},
journal = {Spatial Vision},
keywords = {Cognitive map,Navigation,Sensorimotor,Spatial representation,Virtual reality},
number = {5},
pages = {409--424},
pmid = {19814904},
title = {{Representation of space: Image-like or sensorimotor?}},
volume = {22},
year = {2009}
}
@article{Melby-Lervag2016,
abstract = {Background To maintain the balance between the demand of the body and supply (cardiac output), cardiac performance is tightly regulated via the parasympathetic and sympathetic nervous systems. In heart failure, cardiac output (supply) is decreased due to pathologic remodeling of the heart. To meet the demands of the body, the sympathetic system is activated and catecholamines stimulate $\beta$-adrenergic receptors ($\beta$-ARs) to increase contractile performance and cardiac output. Although this is beneficial in the acute phase, chronic $\beta$-ARs stimulation initiates a cascade of alterations at the cellular level, resulting in a diminished contractile performance of the heart.},
annote = {-- explanation of why working memory training doesn't help: we're training the weakest link ,WM is a very strong system that's been trained over many years of experience},
author = {Melby-Lerv{\aa}g, Monica and Redick, Thomas S. and Hulme, Charles},
doi = {10.1177/1745691616635612},
file = {:home/andrew/Documents/grad/Papers/1745691616635612.pdf:pdf},
isbn = {1972952943},
issn = {17456924},
journal = {Perspectives on Psychological Science},
keywords = {meta-analysis,training,transfer,working memory},
number = {4},
pages = {512--534},
pmid = {25071666},
title = {{Working Memory Training Does Not Improve Performance on Measures of Intelligence or Other Measures of â€œFar Transferâ€: Evidence From a Meta-Analytic Review}},
volume = {11},
year = {2016}
}
@article{Kubricht2017,
abstract = {Research on analogical problem solving has shown that people often fail to spontaneously notice the relevance of a semantically remote source analog when solving a target problem, although they are able to form mappings and derive inferences when given a hint to recall the source. Relatively little work has investigated possible individual differences that predict spontaneous transfer, or how such differences may interact with interventions that facilitate transfer. In this study, fluid intelligence was measured for participants in an analogical problem-solving task, using an abridged version of the Raven's Progressive Matrices (RPM) test. In two experiments, we systematically compared the effect of augmenting verbal descriptions of the source with animations or static diagrams. Solution rates to Duncker's radiation problem were measured across varying source presentation conditions, and participants' understanding of the relevant source material was assessed. The pattern of transfer was best fit by a moderated mediation model: the positive impact of fluid intelligence on spontaneous transfer was mediated by its influence on source comprehension; however, this path was in turn modulated by provision of a supplemental animation via its influence on comprehension of the source. Animated source depictions were most beneficial in facilitating spontaneous transfer for those participants with low scores on the fluid intelligence measure.},
annote = {Greater "fluid intelligence" (raven's progressive matrices) results in more transfer, especially with a weaker source presentation (text or text and static diagram vs. text + animation) . (With stronger source, everyone does fairly well.)},
author = {Kubricht, James R. and Lu, Hongjing and Holyoak, Keith J.},
doi = {10.3758/s13421-016-0687-7},
file = {:home/andrew/Documents/grad/Papers/10.3758{\%}2Fs13421-016-0687-7.pdf:pdf},
isbn = {1532-5946},
issn = {15325946},
journal = {Memory and Cognition},
keywords = {Analogy,Animation,Fluid intelligence,Multimedia learning,Transfer},
number = {4},
pages = {576--588},
publisher = {Memory {\&} Cognition},
title = {{Individual differences in spontaneous analogical transfer}},
volume = {45},
year = {2017}
}
@article{Schunn1996,
abstract = {The mechanisms by which a concept used in solving one complex task can influence performance on another complex task were investigated. We tested the hypothesis that even when subjects do not spontaneously make an analogy between two domains, knowledge of one domain can still spontaneously influence reasoning about the other domain via the mechanism of priming. Four groups of subjects (two experimental and two control) were given a simulated biochemistry problem on Day 1 and a simulated molecular genetics problem on Day 2. For the two experimental groups, the solution to the biochemistry problem involved inhibition. For the two control groups, the solution did not involve inhibition. On Day 2, all subjects received the same version of the molecular genetics problem in which the solution involved the concept of inhibition. Subjects in the experimental conditions were more likely to attain the correct answer, to propose inhibition, and to propose inhibition early in the problem-solving session than were subjects in the control conditions. However, subjects in the experimental conditions made no reference to the biochemistry problem either in their verbal protocols or in a post-task questionnaire. The results are interpreted as demonstrating that an implicit process--priming--can make old knowledge available for current problem solving.},
author = {Schunn, Christian D. and Dunbar, Kevin},
doi = {10.3758/BF03213292},
file = {:home/andrew/Documents/grad/Papers/schunndunbar96.pdf:pdf},
isbn = {0090-502X},
issn = {0090502X},
journal = {Memory and Cognition},
number = {3},
pages = {271--284},
pmid = {8718762},
title = {{Priming, analogy, and awareness in complex reasoning}},
volume = {24},
year = {1996}
}
@article{Sternberg2012,
abstract = {How do humans learn contingencies between events? Both pathway-strengthening and inference-based process models have been proposed to explain contingency learning. We propose that each of these processes is used in different conditions. Participants viewed displays that contained single or paired objects and learned which displays were usually followed by the appearance of a dot. Some participants predicted whether the dot would appear before seeing the outcome, whereas other participants were required to respond quickly if the dot appeared shortly after the display. In the prediction task, instructions guiding participants to infer which objects caused the dot to appear were necessary in order for contingencies associated with one object to influence participants' predictions about the object with which it had been paired. In the response task, contingencies associated with one object affected responses to its pair mate irrespective of whether or not participants were given causal instructions. Our results challenge single-mechanism accounts of contingency learning and suggest that the mechanisms underlying performance in the two tasks are distinct.},
author = {Sternberg, Daniel A. and McClelland, James L.},
doi = {10.1177/0956797611429577},
file = {:home/andrew/Documents/grad/Papers/SternbergMcC12.pdf:pdf},
isbn = {0956797614679280},
issn = {14679280},
journal = {Psychological Science},
keywords = {associative processes,cognitive processes,learning,reasoning},
number = {1},
pages = {59--68},
pmid = {22198929},
title = {{Two mechanisms of human contingency learning}},
volume = {23},
year = {2012}
}
@article{Lou,
author = {Lou, Zhengzheng and Ye, Yangdong and Yan, Xiaoqiang},
file = {:home/andrew/Documents/grad/Papers/225.pdf:pdf},
keywords = {Machine Learning},
pages = {1508--1515},
title = {{The Multi-Feature Information Bottleneck with Application to Unsupervised Image Categorization}}
}
@article{Vera,
archivePrefix = {arXiv},
arxivId = {arXiv:1711.07099v1},
author = {Vera, Matias and Member, Student and Vega, Leonardo Rey},
eprint = {arXiv:1711.07099v1},
file = {:home/andrew/Documents/grad/Papers/1711.07099.pdf:pdf},
number = {1},
pages = {1--13},
title = {{Compression-Based Regularization with an Application to Multi-Task Learning}}
}
@article{Leordeanu2006,
abstract = {We present an efficient method for maximizing energy functions with first and second order potentials, suitable for MAP labeling estimation problems that arise in undirected graphical models. Our approach is to relax the integer constraints on the solution in two steps. First we efficiently obtain the relaxed global optimum following a procedure similar to the iterative power method for finding the largest eigenvector of a matrix. Next, we map the relaxed optimum on a simplex and show that the new energy obtained has a certain optimal bound. Starting from this energy we follow an efficient coordinate ascent procedure that is guaranteed to increase the energy at every step and converge to a solution that obeys the initial integral constraints. We also present a sufficient condition for ascent procedures that guarantees the increase in energy at every step.},
author = {Leordeanu, Marius and Hebert, Martial},
doi = {10.1145/1143844.1143913},
file = {:home/andrew/Documents/grad/Papers/leordeanu{\_}marius{\_}2006{\_}1.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning  - ICML '06},
pages = {545--552},
title = {{Efficient MAP approximation for dense energy functions}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143913},
year = {2006}
}
@article{Hwang2011,
abstract = {This report outlines an approach to learning generative models from data. We express models as probabilistic programs, which allows us to capture abstract patterns within the examples. By choosing our language for programs to be an extension of the algebraic data type of the examples, we can begin with a program that generates all and only the examples. We then introduce greater abstraction, and hence generalization, incrementally to the extent that it improves the posterior probability of the examples given the program. Motivated by previous approaches to model merging and program induction, we search for such explanatory abstractions using program transformations. We consider two types of transformation: Abstraction merges common subexpressions within a program into new functions (a form of anti-unification). Deargumentation simplifies functions by reducing the number of arguments. We demonstrate that this approach finds key patterns in the domain of nested lists, including parameterized sub-functions and stochastic recursion.},
archivePrefix = {arXiv},
arxivId = {1110.5667},
author = {Hwang, Irvin and Stuhlm{\"{u}}ller, Andreas and Goodman, Noah D.},
eprint = {1110.5667},
file = {:home/andrew/Documents/grad/Papers/1110.5667.pdf:pdf},
title = {{Inducing Probabilistic Programs by Bayesian Program Merging}},
url = {http://arxiv.org/abs/1110.5667},
year = {2011}
}
@inproceedings{Pennington2018,
abstract = {Recent work has shown that tight concentration of the entire spectrum of singular values of a deep network's input-output Jacobian around one at initialization can speed up learning by orders of magnitude. Therefore, to guide important design choices, it is important to build a full theoretical understanding of the spectra of Jacobians at initialization. To this end, we leverage powerful tools from free probability theory to provide a detailed analytic understanding of how a deep network's Jacobian spectrum depends on various hyperparameters including the nonlinearity, the weight and bias distributions, and the depth. For a variety of nonlinearities, our work reveals the emergence of new universal limiting spectral distributions that remain concentrated around one even as the depth goes to infinity.},
archivePrefix = {arXiv},
arxivId = {1802.09979},
author = {Pennington, Jeffrey and Schoenholz, Samuel S. and Ganguli, Surya},
booktitle = {AISTATS 2018},
eprint = {1802.09979},
file = {:home/andrew/Documents/grad/Papers/1802.09979.pdf:pdf},
title = {{The Emergence of Spectral Universality in Deep Networks}},
url = {http://arxiv.org/abs/1802.09979},
year = {2018}
}
@inproceedings{Schoenholz2016,
abstract = {We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.},
archivePrefix = {arXiv},
arxivId = {1611.01232},
author = {Schoenholz, Samuel S. and Gilmer, Justin and Ganguli, Surya and Sohl-Dickstein, Jascha},
booktitle = {International Conference on Learning Representations (ICLR),},
eprint = {1611.01232},
file = {:home/andrew/Documents/grad/Papers/1611.01232.pdf:pdf},
number = {2016},
pages = {1--18},
title = {{Deep Information Propagation}},
url = {http://arxiv.org/abs/1611.01232},
year = {2016}
}
@article{Pennington2017,
abstract = {It is well known that the initialization of weights in deep neural networks can have a dramatic impact on learning speed. For example, ensuring the mean squared singular value of a network's input-output Jacobian is {\$}O(1){\$} is essential for avoiding the exponential vanishing or explosion of gradients. The stronger condition that all singular values of the Jacobian concentrate near {\$}1{\$} is a property known as dynamical isometry. For deep linear networks, dynamical isometry can be achieved through orthogonal weight initialization and has been shown to dramatically speed up learning; however, it has remained unclear how to extend these results to the nonlinear setting. We address this question by employing powerful tools from free probability theory to compute analytically the entire singular value distribution of a deep network's input-output Jacobian. We explore the dependence of the singular value distribution on the depth of the network, the weight initialization, and the choice of nonlinearity. Intriguingly, we find that ReLU networks are incapable of dynamical isometry. On the other hand, sigmoidal networks can achieve isometry, but only with orthogonal weight initialization. Moreover, we demonstrate empirically that deep nonlinear networks achieving dynamical isometry learn orders of magnitude faster than networks that do not. Indeed, we show that properly-initialized deep sigmoidal networks consistently outperform deep ReLU networks. Overall, our analysis reveals that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning.},
archivePrefix = {arXiv},
arxivId = {1711.04735},
author = {Pennington, Jeffrey and Schoenholz, Samuel S. and Ganguli, Surya},
eprint = {1711.04735},
file = {:home/andrew/Documents/grad/Papers/7064-resurrecting-the-sigmoid-in-deep-learning-through-dynamical-isometry-theory-and-practice.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 30},
number = {Nips},
pages = {1--11},
title = {{Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice}},
url = {http://arxiv.org/abs/1711.04735},
year = {2017}
}
@article{Golowich2017,
abstract = {We study the sample complexity of learning neural networks, by providing new bounds on their Rademacher complexity assuming norm constraints on the parameter matrix of each layer. Compared to previous work, these complexity bounds have improved dependence on the network depth, and under some additional assumptions, are fully independent of the network size (both depth and width). These results are derived using some novel techniques, which may be of independent interest.},
archivePrefix = {arXiv},
arxivId = {1712.06541},
author = {Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
eprint = {1712.06541},
file = {:home/andrew/Documents/grad/Papers/1712.06541.pdf:pdf},
journal = {arXiv preprint},
number = {1},
pages = {1--26},
title = {{Size-Independent Sample Complexity of Neural Networks}},
url = {http://arxiv.org/abs/1712.06541},
year = {2017}
}
@article{Neyshabur2017a,
abstract = {We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. The generalization bound is derived using a PAC-Bayes analysis.},
archivePrefix = {arXiv},
arxivId = {1707.09564},
author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Srebro, Nathan},
eprint = {1707.09564},
file = {:home/andrew/Documents/grad/Papers/1707.09564.pdf:pdf},
journal = {arXiv preprint},
number = {2017},
pages = {1--9},
title = {{A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks}},
url = {http://arxiv.org/abs/1707.09564},
year = {2017}
}
@article{Dziugaite2017,
abstract = {One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous when applied to networks learned by SGD in this "deep learning" regime. Logically, in order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hidden-unit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.},
archivePrefix = {arXiv},
arxivId = {1703.11008},
author = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
eprint = {1703.11008},
file = {:home/andrew/Documents/grad/Papers/1703.11008.pdf:pdf},
journal = {arXiv preprint},
title = {{Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data}},
url = {http://arxiv.org/abs/1703.11008},
year = {2017}
}
@article{Arora2018,
abstract = {Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The current paper shows generalization bounds that're orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net --- a compression that is explicit and efficient. These yield generalization bounds via a simple compression-based framework introduced here. Our results also provide some theoretical justification for widespread empirical success in compressing deep nets. Analysis of correctness of our compression relies upon some newly identified $\backslash$textquotedblleft noise stability$\backslash$textquotedblright properties of trained deep nets, which are also experimentally verified. The study of these properties and resulting generalization bounds are also extended to convolutional nets, which had eluded earlier attempts on proving generalization.},
archivePrefix = {arXiv},
arxivId = {1802.05296},
author = {Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
eprint = {1802.05296},
file = {:home/andrew/Documents/grad/Papers/1802.05296.pdf:pdf},
journal = {arXiv preprint},
pages = {1--39},
title = {{Stronger generalization bounds for deep nets via a compression approach}},
url = {http://arxiv.org/abs/1802.05296},
year = {2018}
}
@article{Neyshabur2015,
abstract = {We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.},
archivePrefix = {arXiv},
arxivId = {1503.00036},
author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
eprint = {1503.00036},
file = {:home/andrew/Documents/grad/Papers/1503.00036.pdf:pdf},
issn = {15337928},
journal = {Conference on Learning Theory (COLT)},
keywords = {deep learning,feed-forward neural networks,scale-sensitive capacity control},
pages = {1376--1401},
title = {{Norm-based capacity control in neural networks}},
url = {http://arxiv.org/abs/1503.00036},
year = {2015}
}
@article{Bartlett2017,
abstract = {This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized "spectral complexity": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the mnist and cifar10 datasets, with both original and random labels; the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity.},
archivePrefix = {arXiv},
arxivId = {1706.08498},
author = {Bartlett, Peter and Foster, Dylan J. and Telgarsky, Matus},
eprint = {1706.08498},
file = {:home/andrew/Documents/grad/Papers/1706.08498.pdf:pdf},
journal = {arXiv preprint},
pages = {1--24},
title = {{Spectrally-normalized margin bounds for neural networks}},
url = {http://arxiv.org/abs/1706.08498},
year = {2017}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, I.J. and Pouget-Abadie, J and Mirza, Mehdi},
doi = {10.1001/jamainternmed.2016.8245},
eprint = {arXiv:1406.2661v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow, Pouget-Abadie, Mirza - 2014 - Generative Adversarial Networks.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {arXiv preprint},
pages = {1--9},
pmid = {15040217},
title = {{Generative Adversarial Networks}},
year = {2014}
}
@inproceedings{Aggarwal2001,
author = {Aggarwal, Charu C and Hinneburg, Alexander and Keim, Daniel A},
booktitle = {International conference on database theory},
file = {:home/andrew/Documents/grad/Papers/c4590ac7288e722bc154cbfd73be1f575b58.pdf:pdf},
title = {{On the Surprising Behavior of Distance Metrics in High Dimensional Space}},
year = {2001}
}
@article{Silver2016,
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Driessche, George Van Den and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray},
doi = {10.1038/nature16961},
file = {:home/andrew/Documents/grad/Papers/AlphaGoNaturePaper.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7585},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Silver,
archivePrefix = {arXiv},
arxivId = {arXiv:1712.01815v1},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
eprint = {arXiv:1712.01815v1},
file = {:home/andrew/Documents/grad/Papers/1712.01815.pdf:pdf},
pages = {1--19},
title = {{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm arXiv : 1712 . 01815v1 [ cs . AI ] 5 Dec 2017}}
}
@article{Zhang2016,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
doi = {10.1109/TKDE.2015.2507132},
eprint = {1611.03530},
file = {:home/andrew/Documents/grad/Papers/1611.03530.pdf:pdf},
isbn = {1506.02142},
issn = {10414347},
journal = {arXiv preprint},
pmid = {88045},
title = {{Understanding deep learning requires rethinking generalization}},
url = {http://arxiv.org/abs/1611.03530},
year = {2016}
}
@article{DiSessa1993,
annote = {p. 134-135 "[This suggests that] expert intuitions might develop for reasons of increasing coherence of the knowledge system (Systematicity D, mutual plausibility) even though such intuitions may not be instructed or even have any direct instrumental role in problem solving."

p. 189 - "in mathematics, definitions are supposed categorically to determine the set of examples of a class. But if students possess structurally limited knowledge systems, judgments will frequently be made on the basis of different and inarticulate knowledge, even if definitions are overtly end},
author = {DiSessa, Andrea A.},
file = {:home/andrew/Documents/grad/Papers/3233725.pdf:pdf},
journal = {Cognition and Instruction},
number = {2/3},
pages = {105--225},
title = {{Toward an Epistemology of Physics}},
url = {http://www.jstor.org/stable/3233725},
volume = {10},
year = {1993}
}
@article{Duncan2016,
abstract = {Using memory to guide decisions allows past experience to improve future outcomes. However, the circumstances that modulate how and when memory influences decisions are not well understood. Here, we report that the use of memories to guide decisions depends on the context in which these decisions are made. We show that decisions made in the context of familiar images are more likely to be influenced by past events than are decisions made in the context of novel images (Experiment 1), that this bias persists even when a temporal gap is introduced between the image presentation and the decision (Experiment 2), and that contextual novelty facilitates value learning whereas familiarity facilitates the retrieval and use of previously learned values (Experiment 3). These effects are consistent with neurobiological and computational models of memory, which propose that familiar images evoke a lingering â€œretrieval stateâ€ that facilitates the recollection of other episodic memories. Together, these experiments highlight the importance of episodic memory for decision-making and provide an example of how computational and neurobiological theories can lead to new insights into how and when different types of memories guide our choices.},
author = {Duncan, Katherine D. and Shohamy, Daphna},
doi = {10.1037/xge0000231},
file = {:home/andrew/Documents/grad/Papers/Duncan{\_}2016.pdf:pdf},
issn = {00963445},
journal = {Journal of Experimental Psychology: General},
keywords = {Context,Decision-making,Episodic memory,Memory states,Novelty},
number = {11},
pages = {1420--1426},
pmid = {27797556},
title = {{Memory states influence value-based decisions}},
volume = {145},
year = {2016}
}
@article{Smith2017,
abstract = {Visual learning depends on both the algorithms and the training material. This essay considers the natural statistics of infant- and toddler-egocentric vision. These natural training sets for human visual object recognition are very different from the training data fed into machine vision systems. Rather than equal experiences with all kinds of things, toddlers experience extremely skewed distributions with many repeated occurrences of a very few things. And though highly variable when considered as a whole, individual views of things are experienced in a specific order â€“ with slow, smooth visual changes moment-to-moment, and developmentally-ordered transitions in scene content. We propose that the skewed, ordered, biased visual experiences of infants and toddlers are the training data that allow human learners to develop a way to recognize everything, both the pervasively present entities and the rarely encountered ones. The joint consideration of real-world statistics for learning by researchers of human and machine learning seems likely to bring advances in both disciplines.},
author = {Smith, Linda B. and Slone, Lauren K.},
doi = {10.3389/fpsyg.2017.02124},
file = {:home/andrew/Documents/grad/Papers/fpsyg-08-02124.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Active vision,Development,Egocentric vision,Natural environment,Object recognition},
number = {DEC},
pages = {1--10},
pmid = {29259573},
title = {{A developmental approach to machine learning?}},
volume = {8},
year = {2017}
}
@article{Li2012,
abstract = {Efficient exploration is widely recognized as a fundamental challenge in- herent in reinforcement-learning problems. Algorithms that explore efficiently con- verge faster to better policies. While heuristics techniques are popular in practice, they lack formal guarantees and may not work well in general. This chapter studies principled algorithms, both model-based and model-free ones, in a unified manner based on a powerful theorem. These so-called PAC-MDP algorithms enjoy polyno- mial sample complexity of exploration, and hence behave near-optimally except in a â€œsmallâ€ number of steps with high probability. Furthermore, a new learning model known as KWIK is used to unify most existing model-based PAC-MDP algorithms as well as to facilitate development of new ones for various subclasses of MDPs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.07669v1},
author = {Li, Lihong},
doi = {10.1007/978-3-642-27645-3_6},
eprint = {arXiv:1512.07669v1},
file = {:home/andrew/Documents/grad/Papers/Li.12.SampleComplexity.pdf:pdf},
isbn = {9783642015267},
issn = {18674542},
journal = {Adaptation, Learning, and Optimization},
keywords = {Covariance,Nite},
pages = {175--204},
pmid = {17082042},
title = {{Sample complexity bounds of exploration}},
volume = {12},
year = {2012}
}
@article{Andreas2016,
abstract = {We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them---specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor--critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.},
archivePrefix = {arXiv},
arxivId = {1611.01796},
author = {Andreas, Jacob and Klein, Dan and Levine, Sergey},
eprint = {1611.01796},
file = {:home/andrew/Documents/grad/Papers/1611.01796.pdf:pdf},
issn = {1938-7228},
journal = {arXiv preprint},
title = {{Modular Multitask Reinforcement Learning with Policy Sketches}},
url = {http://arxiv.org/abs/1611.01796},
year = {2016}
}
@article{Andreasb,
archivePrefix = {arXiv},
arxivId = {arXiv:1704.06960v4},
author = {Andreas, Jacob and Dragan, Anca and Klein, Dan},
eprint = {arXiv:1704.06960v4},
file = {:home/andrew/Documents/grad/Papers/1704.06960.pdf:pdf},
title = {{Translating Neuralese}}
}
@article{Gorea2000,
author = {Gorea, Andrei and Sagi, Dov},
file = {:home/andrew/Documents/grad/Papers/12380.full.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {22},
title = {{Failure to handle more than one internal representation in visual detection tasks}},
volume = {97},
year = {2000}
}
@article{Pereira2018,
author = {Pereira, Francisco and Lou, Bin and Pritchett, Brianna and Ritter, Samuel and Gershman, Samuel J. and Kanwisher, Nancy and Botvinick, Matthew and Fedorenko, Evelina},
doi = {10.1038/s41467-018-03068-4},
file = {:home/andrew/Documents/grad/Papers/s41467-018-03068-4.pdf:pdf},
issn = {2041-1723},
journal = {Nature Communications},
number = {1},
pages = {963},
pmid = {29511192},
publisher = {Springer US},
title = {{Toward a universal decoder of linguistic meaning from brain activation}},
url = {http://www.nature.com/articles/s41467-018-03068-4},
volume = {9},
year = {2018}
}
@article{Nakamura2018,
author = {Nakamura, Kimihiro and Makuuchi, Michiru and Oga, Tatsuhide and Mizuochi-Endo, Tomomi and Iwabuchi, Toshiki and Nakajima, Yasoichi and Dehaene, Stanislas},
doi = {10.1111/ejn.13890},
file = {:home/andrew/Documents/grad/Papers/Nakamura{\_}et{\_}al-2018-European{\_}Journal{\_}of{\_}Neuroscience.pdf:pdf},
issn = {0953816X},
journal = {European Journal of Neuroscience},
keywords = {N400,eventâ€related potentials,sentence comprehension,subliminal language processing,visual masking},
pages = {0--2},
pmid = {29512843},
title = {{Neural capacity limits during unconscious semantic processing}},
url = {http://doi.wiley.com/10.1111/ejn.13890},
year = {2018}
}
@article{Hannagan2017,
abstract = {Number sense, a spontaneous ability to process approximate numbers, has been documented in human adults, infants and newborns, and many other animals. Species as distant as monkeys and crows exhibit very similar neurons tuned to specific numerosities. How number sense can emerge in the absence of learning or fine tuning is currently unknown. We introduce a random-matrix theory of self-organized neural states where numbers are coded by vectors of activation across multiple units, and where the vector codes for successive integers are obtained through multiplication by a fixed but random matrix. This cortical implementation of the 'von Mises' algorithm explains many otherwise disconnected observations ranging from neural tuning curves in monkeys to looking times in neonates and cortical numerotopy in adults. The theory clarifies the origin of Weber-Fechner's Law and yields a novel and empirically validated prediction of multi-peak number neurons. Random matrices constitute a novel mechanism for the emergence of brain states coding for quantity.This article is part of a discussion meeting issue 'The origins of numerical abilities'.},
author = {Hannagan, T and Nieder, A and Viswanathan, P and Dehaene, S},
doi = {10.1098/rstb.2017.0253},
file = {:home/andrew/Documents/grad/Papers/20170253.full.pdf:pdf},
issn = {1471-2970},
journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
keywords = {Weberâ€“Fechner Law,neonates,number neurons,number sense,numerotopy,random-matrix theory},
number = {1740},
pages = {20170253},
pmid = {29292354},
title = {{A random-matrix theory of the number sense.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29292354},
volume = {373},
year = {2017}
}
@article{Jadhav2012,
author = {Jadhav, Shantanu P and Kemere, Caleb and German, P Walter and Frank, Loren M},
doi = {10.1126/science.1217230},
file = {:home/andrew/Documents/grad/Papers/science.1217230.full.pdf:pdf},
isbn = {0036807510959203},
issn = {0036-8075, 1095-9203},
journal = {Science},
number = {May},
pmid = {22555434},
title = {{Awake Hippocampal Sharp-Wave Ripples Support Spatial Memory}},
year = {2012}
}
@article{Carr2011,
abstract = {The hippocampus is required for the encoding, consolidation and retrieval of event memories. Although the neural mechanisms that underlie these processes are only partially understood, a series of recent papers point to awake memory replay as a potential contributor to both consolidation and retrieval. Replay is the sequential reactivation of hippocampal place cells that represent previously experienced behavioral trajectories and occurs frequently in the awake state, particularly during periods of relative immobility. Awake replay may reflect trajectories through either the current environment or previously visited environments that are spatially remote. The repetition of learned sequences on a compressed time scale is well suited to promote memory consolidation in distributed circuits beyond the hippocampus, suggesting that consolidation occurs in both the awake and sleeping animal. Moreover, sensory information can influence the content of awake replay, suggesting a role for awake replay in memory retrieval.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Carr, Margaret F. and Jadhav, Shantanu P. and Frank, Loren M.},
doi = {10.1038/nn.2732},
eprint = {NIHMS150003},
file = {:home/andrew/Documents/grad/Papers/nn.2732.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$r1097-6256 (Linking)},
issn = {10976256},
journal = {Nature Neuroscience},
number = {2},
pages = {147--153},
pmid = {21270783},
publisher = {Nature Publishing Group},
title = {{Hippocampal replay in the awake state: A potential substrate for memory consolidation and retrieval}},
url = {http://dx.doi.org/10.1038/nn.2732},
volume = {14},
year = {2011}
}
@article{Ram2007,
abstract = {Growth curve modeling has become a mainstay in the study of development. In this article we review some of the flexibility provided by this technique for describing and testing hypotheses about: (1) intraindividual change across multiple occasions of measurement, and (2) interindividual differences in intraindividual change. Through empirical example we demonstrate how linear, quadratic, latent basis, exponential, and multiphase versions of the model can be specified using commonly available SEM/multilevel modeling software and illustrate and discuss how results are obtained and interpreted. Particularly, we underscore the â€œdevelopmental theoryâ€ articulated by each model.},
author = {Ram, Nilam and Grimm, Kevin},
doi = {10.1177/0165025407077751},
file = {:home/andrew/Documents/grad/Papers/0165025407077751.pdf:pdf},
isbn = {0165-0254},
issn = {01650254},
journal = {International Journal of Behavioral Development},
keywords = {Developmental change,Growth curve modeling,Intraindividual change},
number = {4},
pages = {303--316},
title = {{Using simple and complex growth models to articulate developmental change: Matching theory to method}},
volume = {31},
year = {2007}
}
@article{Ram2009,
abstract = {Growth mixture modeling (GMM) is a method for identifying multiple unobserved sub-populations, describing longitudinal change within each unobserved sub-population, and examining differences in change among unobserved sub-populations. We provide a practical primer that may be useful for researchers beginning to incorporate GMM analysis into their research. We briefly review basic elements of the standard latent basis growth curve model, introduce GMM as an extension of multiple-group growth modeling, and describe a four-step approach to conducting a GMM analysis. Example data from a cortisol stress-response paradigm are used to illustrate the suggested procedures. },
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Ram, Nilam and Grimm, Kevin J.},
doi = {10.1177/0165025409343765},
eprint = {NIHMS150003},
file = {:home/andrew/Documents/grad/Papers/0165025409343765.pdf:pdf},
isbn = {0165025409},
issn = {01650254},
journal = {International Journal of Behavioral Development},
keywords = {Aging,Cortisol,Development,Dynamic process,Growth mixture modeling,Latent growth,Longitudinal methods},
number = {6},
pages = {565--576},
pmid = {23885133},
title = {{Methods and Measures: Growth mixture modeling: A method for identifying differences in longitudinal change among unobserved groups}},
volume = {33},
year = {2009}
}
@article{Botvinick2009,
abstract = {Research on human and animal behavior has long emphasized its hierarchical structure-the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andrew C.},
doi = {10.1016/j.cognition.2008.08.011},
file = {:home/andrew/Documents/grad/Papers/BotvinickEtAl09Hierarchically.pdf:pdf},
isbn = {1873-7838 (Electronic)$\backslash$n0010-0277 (Linking)},
issn = {00100277},
journal = {Cognition},
keywords = {Prefrontal cortex,Reinforcement learning},
number = {3},
pages = {262--280},
pmid = {18926527},
publisher = {Elsevier B.V.},
title = {{Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective}},
url = {http://dx.doi.org/10.1016/j.cognition.2008.08.011},
volume = {113},
year = {2009}
}
@article{Dicarlo2007,
author = {Dicarlo, James J and Cox, David D},
doi = {10.1016/j.tics.2007.06.010},
file = {:home/andrew/Documents/grad/Papers/dicarlo and cox 2007.pdf:pdf},
journal = {Trends in Cognitive Sciences},
number = {8},
title = {{Untangling invariant object recognition}},
volume = {11},
year = {2007}
}
@incollection{Stewart1990,
abstract = {abstract The singular value decomposition has a number of applications in dig-ital signal processing. However, the the decomposition must be com-puted from a matrix consisting of both signal and noise. It is therefore important to be able to assess the eeects of the noise on the singular values and singular vectors | a problem in classical perturbation the-ory. In this paper we survey the perturbation theory of the singular value decomposition. Abstract The singular value decomposition has a number of applications in digital signal processing. However, the the decomposition must be computed from a matrix consisting of both signal and noise. It is therefore important to be able to assess the eeects of the noise on the singular values and singular vectors | a problem in classical perturbation theory. In this paper we survey the perturbation theory of the singular value decomposition.},
author = {Stewart, G W and Stewart, G W},
booktitle = {SVD AND SIGNAL PROCESSING, II: ALGORITHMS, ANALYSIS AND APPLICATIONS},
file = {:home/andrew/Documents/grad/Papers/CS-TR-2539.pdf:pdf},
number = {September},
title = {{Perturbation Theory for the Singular Value Decomposition}},
url = {https://pdfs.semanticscholar.org/0a2b/0c431bd68c6a67926f44a07a0d575f2957f1.pdf{\%}0Ahttp://users.math.msu.edu/users/markiwen/Teaching/MTH995/Papers/SVD{\_}Stewart.pdf},
year = {1990}
}
@article{Li1998,
author = {Li, Ren-cang},
file = {:home/andrew/Documents/grad/Papers/relpthyI.pdf:pdf},
isbn = {0895479896298},
journal = {SIAM J. Matrix Anal. Appl.},
keywords = {15a18,15a42,65f15,65f35,65g99,ams subject classifications,eigenvector,graded matrix,multiplicative perturbation,pii,relative gap,relative perturbation theory,s0895479896298506,singular vector,structured sylvester equation},
number = {4},
pages = {956--982},
title = {{Relative Perturbation Theory: Eigenvalue and Singular Value Variations}},
volume = {19},
year = {1998}
}
@book{Naumann2017,
annote = {Good overview of difficulties of homology and importance of comparative neuroanatomy, if you're into that sort of thing},
author = {Naumann, Robert K. and Laurent, Gilles},
booktitle = {Evolution of Nervous Systems},
doi = {10.1016/B978-0-12-804042-3.00022-1},
edition = {Second Edi},
file = {:home/andrew/Documents/grad/Papers/3-s2.0-B9780128040423000221-main.pdf:pdf},
isbn = {9780128040423},
pages = {491--512},
publisher = {Elsevier},
title = {{Function and Evolution of the Reptilian Cerebral Cortex}},
url = {http://dx.doi.org/10.1016/B978-0-12-804042-3.00022-1},
volume = {1},
year = {2017}
}
@article{Suhr2017,
abstract = {We present a new visual reasoning lan-guage dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sen-tences. We describe a method of crowd-sourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phe-nomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.},
author = {Suhr, Alane and Lewis, Mike and Yeh, James and Artzi, Yoav},
doi = {10.18653/v1/P17-2034},
file = {:home/andrew/Documents/grad/Papers/suhr2017.pdf:pdf},
isbn = {9781945626760},
journal = {Acl},
pages = {217----223},
title = {{A Corpus of Natural Language for Visual Reasoning}},
url = {http://yoavartzi.com/pub/slya-acl.2017.pdf},
year = {2017}
}
@article{Johnson,
archivePrefix = {arXiv},
arxivId = {arXiv:1612.06890v1},
author = {Johnson, Justin and Fei-fei, Li and Hariharan, Bharath and Zitnick, C Lawrence and Maaten, Laurens Van Der and Girshick, Ross},
eprint = {arXiv:1612.06890v1},
file = {:home/andrew/Documents/grad/Papers/1612.06890.pdf:pdf},
title = {{Compositional Language and Elementary Visual Reasoning}}
}
@article{Forbus2017,
author = {Forbus, Kenneth D and Ferguson, Ronald W and Lovett, Andrew},
doi = {10.1111/cogs.12377},
file = {:home/andrew/Documents/grad/Papers/Forbus{\_}et{\_}al-2016-Cognitive{\_}Science.pdf:pdf},
keywords = {2133 sheri-,analogical learning,analogical reasoning,analogy,artificial intelligence,cognitive psychology,cognitive simulation,correspondence should be sent,eecs department,northwestern university,similarity,symbolic modeling,to ken forbus},
pages = {1152--1201},
title = {{Extending SME to Handle Large-Scale Cognitive Modeling}},
volume = {41},
year = {2017}
}
@article{Pandarinath2017,
author = {Pandarinath, Chethan and O'Shea, Daniel J. and Collins, Jasmine and Jozefowicz, Rafal and Stavisky, Sergey D. and Kao, Johnathan C. and Trautmann, Eric M. and Kaufman, Matthew T. and Ryu, Stephen I. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shenoy, Krishna V. and Abbott, L. F. and Sussillo, David},
file = {:home/andrew/Documents/grad/Papers/152884.full.pdf:pdf},
journal = {bioRxiv},
pages = {1--4},
title = {{Inferring single-trial neural population dynamics using sequential auto-encoders}},
year = {2017}
}
@article{Vyas2018,
author = {Vyas, Saurabh and Even-Chen, Nir and Stavisky, Sergey D. and Ryu, Stephen I. and Nuyujukian, Paul and Shenoy, Krishna V.},
doi = {10.1016/j.neuron.2018.01.040},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S0896627318300655-main.pdf:pdf},
issn = {08966273},
journal = {Neuron},
pages = {1--10},
pmid = {29456026},
publisher = {Elsevier Inc.},
title = {{Neural Population Dynamics Underlying Motor Learning Transfer}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627318300655},
year = {2018}
}
@article{Gentner2017,
author = {Gentner, Dedre and Asmuth, Jennifer},
doi = {10.1080/23273798.2017.1410560},
file = {:home/andrew/Documents/grad/Papers/Metaphoric extension relational categories and abstraction.pdf:pdf},
issn = {2327-3798},
journal = {Language, Cognition and Neuroscience},
keywords = {analogical abstraction,career,concept,of metaphor,progressive alignment},
number = {0},
pages = {1--10},
publisher = {Taylor {\&} Francis},
title = {{Metaphoric extension , relational categories , and abstraction}},
url = {https://doi.org/10.1080/23273798.2017.1410560},
volume = {0},
year = {2017}
}
@article{Lahiri2016,
abstract = {Interesting data often concentrate on low dimensional smooth manifolds inside a high dimensional ambient space. Random projections are a simple, powerful tool for dimensionality reduction of such data. Previous works have studied bounds on how many projections are needed to accurately preserve the geometry of these manifolds, given their intrinsic dimensionality, volume and curvature. However, such works employ definitions of volume and curvature that are inherently difficult to compute. Therefore such theory cannot be easily tested against numerical simulations to understand the tightness of the proven bounds. We instead study typical distortions arising in random projections of an ensemble of smooth Gaussian random manifolds. We find explicitly computable, approximate theoretical bounds on the number of projections required to accurately preserve the geometry of these manifolds. Our bounds, while approximate, can only be violated with a probability that is exponentially small in the ambient dimension, and therefore they hold with high probability in cases of practical interest. Moreover, unlike previous work, we test our theoretical bounds against numerical experiments on the actual geometric distortions that typically occur for random projections of random smooth manifolds. We find our bounds are tighter than previous results by several orders of magnitude.},
archivePrefix = {arXiv},
arxivId = {1607.04331},
author = {Lahiri, Subhaneil and Gao, Peiran and Ganguli, Surya},
eprint = {1607.04331},
file = {:home/andrew/Documents/grad/Papers/1607.04331.pdf:pdf},
isbn = {0001406108},
journal = {arXiv},
pages = {1--45},
title = {{Random projections of random manifolds}},
url = {http://arxiv.org/abs/1607.04331},
year = {2016}
}
@article{Haber2018,
abstract = {Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to replicate some of these abilities with a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which the agent can move and interact with objects it sees, the agent learns a world model predicting the dynamic consequences of its actions. Simultaneously, the agent learns to take actions that adversarially challenge the developing world model, pushing the agent to explore novel and informative interactions with its environment. We demonstrate that this policy leads to the self-supervised emergence of a spectrum of complex behaviors, including ego motion prediction, object attention, and object gathering. Moreover, the world model that the agent learns supports improved performance on object dynamics prediction and localization tasks. Our results are a proof-of-principle that computational models of intrinsic motivation might account for key features of developmental visuomotor learning in infants.},
archivePrefix = {arXiv},
arxivId = {1802.07461},
author = {Haber, Nick and Mrowca, Damian and Fei-Fei, Li and Yamins, Daniel L. K.},
eprint = {1802.07461},
file = {:home/andrew/Documents/grad/Papers/1802.07461.pdf:pdf},
journal = {arXiv},
title = {{Emergence of Structured Behaviors from Curiosity-Based Intrinsic Motivation}},
url = {http://arxiv.org/abs/1802.07461},
year = {2018}
}
@article{Gentner2017a,
abstract = {A central question in human development is how young children gain knowledge so fast. We propose that analogical generalization drives much of this early learning and allows children to generate new abstractions from experience. In this paper, we review evidence for analogical gen-eralization in both children and adults. We discuss how analogical processes interact with the child's changing knowledge base to predict the course of learning, from conservative to domain-general understanding. This line of research leads to challenges to existing assumptions about learning. It shows that (a) it is not enough to consider the distribution of examples given to learn-ers; one must consider the processes learners are applying; (b) contrary to the general assumption, maximizing variability is not always the best route for maximizing generalization and transfer.},
author = {Gentner, Dedre and Hoyos, Christian},
doi = {10.1111/tops.12278},
file = {:home/andrew/Documents/grad/Papers/Gentner{\_}et{\_}al-2017-Topics{\_}in{\_}Cognitive{\_}Science.pdf:pdf},
issn = {17568765},
journal = {Topics in Cognitive Science},
keywords = {Abstraction,Analogy,Overhypotheses},
number = {3},
pages = {672--693},
title = {{Analogy and Abstraction}},
volume = {9},
year = {2017}
}
@article{Birnbaum2013,
abstract = {Kornell and Bjork (Psychological science 19:585-592, 2008) found that interleaving exemplars of different categories enhanced inductive learning of the concepts based on those exemplars. They hypothesized that the benefit of mixing exemplars from different categories is that doing so highlights differences between the categories. Kang and Pashler (Applied cognitive psychology 26:97-103, 2012) obtained results consistent with this discriminative-contrast hypothesis: Interleaving enhanced inductive learning, but temporal spacing, which does not highlight category differences, did not. We further tested the discriminative-contrast hypothesis by examining the effects of interleaving and spacing, as well as their combined effects. In three experiments, using photographs of butterflies and birds as the stimuli, temporal spacing was harmful when it interrupted the juxtaposition of interleaved categories, even when total spacing was held constant, supporting the discriminative-contrast hypothesis. Temporal spacing also had value, however, when it did not interrupt discrimination processing.},
author = {Birnbaum, Monica S. and Kornell, Nate and Bjork, Elizabeth Ligon and Bjork, Robert A.},
doi = {10.3758/s13421-012-0272-7},
file = {:home/andrew/Documents/grad/Papers/10.3758{\%}2Fs13421-012-0272-7.pdf:pdf},
isbn = {0090-502X},
issn = {0090502X},
journal = {Memory and Cognition},
keywords = {Categorization,Induction,Interleaving,Metacognition,Spacing},
number = {3},
pages = {392--402},
pmid = {23138567},
title = {{Why interleaving enhances inductive learning: The roles of discrimination and retrieval}},
volume = {41},
year = {2013}
}
@article{Carpenter2013,
abstract = {Many studies have shown that students learn better when they are given repeated exposures to different concepts in a way that is shuffled or interleaved, rather than blocked (e.g., Rohrer Educational Psychology Review, 24, 355-367, 2012). The present study explored the effects of interleaving versus blocking on learning French pronunciations. Native English speakers learned several French words that conformed to specific pronunciation rules (e.g., the long "o" sound formed by the letter combination "eau," as in bateau), and these rules were presented either in blocked fashion (bateau, carreau, fardeau . . . mouton, genou, verrou . . . tandis, verglas, admis) or in interleaved fashion (bateau, mouton, tandis, carreau, genou, verglas . . .). Blocking versus interleaving was manipulated within subjects (Experiments 1-3) or between subjects (Experiment 4), and participants' pronunciation proficiency was later tested through multiple-choice tests (Experiments 1, 2, and 4) or a recall test (Experiment 3). In all experiments, blocking benefited the learning of pronunciations more than did interleaving, and this was true whether participants learned only 4 words per rule (Experiments 1-3) or 15 words per rule (Experiment 4). Theoretical implications of these findings are discussed.},
author = {Carpenter, Shana K. and Mueller, Frank E.},
doi = {10.3758/s13421-012-0291-4},
file = {:home/andrew/Documents/grad/Papers/10.3758{\%}2Fs13421-012-0291-4.pdf:pdf},
isbn = {1342101202914},
issn = {0090502X},
journal = {Memory and Cognition},
keywords = {Discriminative contrast,Interleaving,Pronunciation learning},
number = {5},
pages = {671--682},
pmid = {23322358},
title = {{The effects of interleaving versus blocking on foreign language pronunciation learning}},
volume = {41},
year = {2013}
}
@article{Price2009,
abstract = {Examinations of the cognitive neuroscience of category learning frequently rely on probabilistic classification-learning tasks-namely, the weather prediction task (WPT)-to study the neural mechanisms of implicit learning. Accumulating evidence suggests that the task also depends on explicit-learning processes. The present investigation manipulated the WPT to assess the specific contributions of implicit- and explicit-learning processes to performance, with a particular focus on how the contributions of these processes change as the task progresses. In Experiment 1, a manipulation designed to disrupt implicit-learning processes had no effect on classification accuracy or the distribution of individual response strategies. In Experiment 2, by contrast, a manipulation designed to disrupt explicit-learning processes substantially reduced classification accuracy and reduced the number of participants who relied on a correct response strategy. The present findings suggest that WPT learning is not an effective tool for investigating nondeclarative learning processes.},
author = {Price, Amanda L.},
doi = {10.3758/MC.37.2.210},
file = {:home/andrew/Documents/grad/Papers/weatherprediction{\_}Explicit.pdf:pdf},
isbn = {0090-502X (Print)},
issn = {0090502X},
journal = {Memory and Cognition},
number = {2},
pages = {210--222},
pmid = {19223570},
title = {{Distinguishing the contributions of implicit and explicit processes to performance of the weather prediction task}},
volume = {37},
year = {2009}
}
@article{Kriegeskorte2015,
author = {Kriegeskorte, Nikolaus},
file = {:home/andrew/Documents/grad/Papers/nikokri-2015-39700.pdf:pdf},
journal = {Annual Review of Vision Science},
keywords = {artificial intelligence,biological vision,computational neuroscience,computer vision,deep learning,neural network,object recognition},
pages = {417--446},
title = {{Deep neural networks: a new framework for modelling biological vision and brain information processing}},
year = {2015}
}
@inproceedings{Rahimi2017,
author = {Rahimi, Ali},
booktitle = {Neural Information Processing Systems},
title = {{Test of time award presentation}},
url = {https://www.youtube.com/watch?v=Qi1Yry33TQE},
year = {2017}
}
@incollection{Mickey2017,
author = {Mickey, Kevin and Mcclelland, James L},
booktitle = {Acquisition of Complex Arithmetic Skills and Higher-Order Mathematics Concepts.},
editor = {Geary, D. C. and Berch, D. B. and Ochsendorf, R. and {Mann Koepke}, K.},
file = {:home/andrew/Documents/grad/Papers/MickeyMcCIPUCasGroundedCS.pdf:pdf},
keywords = {conceptual grounding,learning,learning trigonometry,visuospatial representation},
pages = {1--47},
publisher = {Elsevier/Academic Press},
title = {{The Unit Circle as a Grounded Conceptual Structure in Pre-Calculus Trigonometry}},
year = {2017}
}
@article{Saxe2013a,
author = {Saxe, Andrew M and Mcclelland, James L and Ganguli, Surya},
file = {:home/andrew/Documents/grad/Papers/Saxe, McClelland, Ganguli - 2013 - Learning hierarchical category structure in deep neural networks.pdf:pdf},
journal = {Proceedings of the 35th annual meeting of the Cognitive Science Society},
keywords = {hierarchical generative models,learning dynamics,neural networks,semantic cognition},
pages = {1271--1276},
title = {{Learning hierarchical category structure in deep neural networks}},
year = {2013}
}
@article{Dauphin2014b,
author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
file = {:home/andrew/Documents/grad/Papers/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization.pdf:pdf},
journal = {Neural Information Processing Systems},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
year = {2014}
}
@article{Lampinen2017b,
author = {Lampinen, Andrew K. and McClelland, James L.},
doi = {10.1037/edu0000235},
file = {:home/andrew/Documents/grad/Papers/mine/Lampinen{\_}McClelland{\_}2017{\_}PrePrint.pdf:pdf},
issn = {1939-2176},
journal = {Journal of Educational Psychology},
title = {{Different Presentations of a Mathematical Concept Can Support Learning in Complementary Ways.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/edu0000235},
year = {2018}
}
@article{Hok2007,
author = {Hok, Vincent and Hok, Vincent and Save, Etienne and Muller, Robert U and Poucet, Bruno},
doi = {10.1523/JNEUROSCI.2864-06.2007},
file = {:home/andrew/Documents/grad/Papers/Hoketal2007a.pdf:pdf},
journal = {The Journal of Neuroscience},
keywords = {goal coding,hippocampus,place cells,rat,spatial processing,unit recordings},
number = {January},
pages = {472--478},
title = {{Goal-related firing in hippocampal place cells Goal-Related Activity in Hippocampal Place Cells}},
volume = {27},
year = {2007}
}
@article{Bartlett2002,
author = {Bartlett, Peter L and Mendelson, Shahar},
file = {:home/andrew/Documents/grad/Papers/bartlett02a.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {data-dependent complexity,error bounds,maxi-,rademacher averages},
pages = {463--482},
title = {{Rademacher and Gaussian Complexities: Risk Bounds and Structural Results}},
volume = {3},
year = {2002}
}
@article{Herbelot2017,
abstract = {Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn 'a good vector' for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences' worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.},
archivePrefix = {arXiv},
arxivId = {1707.06556},
author = {Herbelot, Aurelie and Baroni, Marco},
eprint = {1707.06556},
file = {:home/andrew/Documents/grad/Papers/D17-1030.pdf:pdf},
journal = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
pages = {304--309},
title = {{High-risk learning: acquiring new word vectors from tiny data}},
url = {http://arxiv.org/abs/1707.06556},
year = {2017}
}
@article{Serdyuk2017,
abstract = {Being able to model long-term dependencies in sequential data, such as text, has been among the long-standing challenges of recurrent neural networks (RNNs). This issue is strictly related to the absence of explicit planning in current RNN architectures. More explicitly, the RNNs are trained to predict only the next token given previous ones. In this paper, we introduce a simple way of encouraging the RNNs to plan for the future. In order to accomplish this, we introduce an additional neural network which is trained to generate the sequence in reverse order, and we require closeness between the states of the forward RNN and backward RNN that predict the same token. At each step, the states of the forward RNN are required to match the future information contained in the backward states. We hypothesize that the approach eases modeling of long-term dependencies thus helping in generating more globally consistent samples. The model trained with conditional generation for a speech recognition task achieved 12$\backslash${\%} relative improvement (CER of 6.7 compared to a baseline of 7.6).},
archivePrefix = {arXiv},
arxivId = {1708.06742},
author = {Serdyuk, Dmitriy and Ke, Rosemary Nan and Sordoni, Alessandro and Pal, Chris and Bengio, Yoshua},
eprint = {1708.06742},
file = {:home/andrew/Documents/grad/Papers/1708.06742.pdf:pdf},
journal = {arXiv},
pages = {3--6},
title = {{Twin Networks: Using the Future as a Regularizer}},
url = {http://arxiv.org/abs/1708.06742},
year = {2017}
}
@article{Langendoen1973,
author = {Langendoen, D. T.},
file = {:home/andrew/Documents/grad/Papers/1421865.pdf:pdf},
journal = {The American Journal of Psychology1},
number = {1},
pages = {207--212},
title = {{Review of "Phrase and Paraphrase : Some Innovative Uses of Language" by Lila R . Gleitman and Henry Gleitman}},
volume = {86},
year = {1973}
}
@inproceedings{Siddharth2017,
abstract = {Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.},
archivePrefix = {arXiv},
arxivId = {1706.00400},
author = {Siddharth, N. and Paige, Brooks and van de Meent, Jan-Willem and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood, Frank and Torr, Philip H. S.},
booktitle = {Neural Information Processing Systems},
eprint = {1706.00400},
file = {:home/andrew/Documents/grad/Papers/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf:pdf},
title = {{Learning Disentangled Representations with Semi-Supervised Deep Generative Models}},
url = {http://arxiv.org/abs/1706.00400},
year = {2017}
}
@inproceedings{Devlin2017,
abstract = {Most recently proposed methods for Neural Program Induction work under the assumption of having a large set of input/output (I/O) examples for learning any underlying input-output mapping. This paper aims to address the problem of data and computation efficiency of program induction by leveraging information from related tasks. Specifically, we propose two approaches for cross-task knowledge transfer to improve program induction in limited-data scenarios. In our first proposal, portfolio adaptation, a set of induction models is pretrained on a set of related tasks, and the best model is adapted towards the new task using transfer learning. In our second approach, meta program induction, a {\$}k{\$}-shot learning approach is used to make a model generalize to new tasks without additional training. To test the efficacy of our methods, we constructed a new benchmark of programs written in the Karel programming language. Using an extensive experimental evaluation on the Karel benchmark, we demonstrate that our proposals dramatically outperform the baseline induction method that does not use knowledge transfer. We also analyze the relative performance of the two approaches and study conditions in which they perform best. In particular, meta induction outperforms all existing approaches under extreme data sparsity (when a very small number of examples are available), i.e., fewer than ten. As the number of available I/O examples increase (i.e. a thousand or more), portfolio adapted program induction becomes the best approach. For intermediate data sizes, we demonstrate that the combined method of adapted meta program induction has the strongest performance.},
archivePrefix = {arXiv},
arxivId = {1710.04157},
author = {Devlin, Jacob and Bunel, Rudy and Singh, Rishabh and Hausknecht, Matthew and Kohli, Pushmeet},
booktitle = {Neural Information Processing Systems},
eprint = {1710.04157},
file = {:home/andrew/Documents/grad/Papers/6803-neural-program-meta-induction.pdf:pdf},
title = {{Neural Program Meta-Induction}},
url = {http://arxiv.org/abs/1710.04157},
year = {2017}
}
@inproceedings{Raghu2017,
abstract = {With the continuing empirical successes of deep networks, it becomes increasingly important to develop better methods for understanding training of models and the representations learned within. In this paper we propose Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.},
archivePrefix = {arXiv},
arxivId = {1706.05806},
author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
booktitle = {Neural Information Processing Systems},
doi = {1706.05806},
eprint = {1706.05806},
file = {:home/andrew/Documents/grad/Papers/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-learning-dynamics-and-interpretability.pdf:pdf},
pages = {1--10},
title = {{SVCCA: Singular Vector Canonical Correlation Analysis for Deep Understanding and Improvement}},
url = {http://arxiv.org/abs/1706.05806},
year = {2017}
}
@article{Ha2016,
abstract = {This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.},
archivePrefix = {arXiv},
arxivId = {1609.09106},
author = {Ha, David and Dai, Andrew and Le, Quoc V.},
eprint = {1609.09106},
file = {:home/andrew/Documents/grad/Papers/1609.09106.pdf:pdf},
journal = {arXiv},
title = {{HyperNetworks}},
url = {http://arxiv.org/abs/1609.09106},
year = {2016}
}
@article{Yang2016,
author = {Yang, Chunliang and Shanks, David R.},
file = {:home/andrew/Documents/grad/Papers/Yang {\%}26 Shanks (JEPLMC).pdf:pdf},
title = {{The forward testing effect: interim testing enhances inductive learning}},
year = {2016}
}
@article{Anzulewicz2015,
abstract = {Recently, Windey, Gevers, and Cleeremans (2013) proposed a level of processing (LoP) hypothesis claiming that the transition from unconscious to conscious perception is influenced by the level of processing imposed by task requirements. Here, we carried out two experiments to test the LoP hypothesis. In both, participants were asked to classify briefly presented pairs of letters as same or different, based either on the letters' physical features (a low-level task), or on a semantic rule (a high-level task). Stimulus awareness was measured by means of the four-point Perceptual Awareness Scale (PAS). The results showed that low or moderate stimulus visibility was reported more frequently in the low-level task than in the high-level task, suggesting that the transition from unconscious to conscious perception is more gradual in the former than in the latter. Therefore, although alternative interpretations remain possible, the results of the present study fully support the LoP hypothesis.},
author = {Anzulewicz, Anna and Asanowicz, Dariusz and Windey, Bert and Paulewicz, Borys{\l}aw and Wierzcho{\'{n}}, Micha{\l} and Cleeremans, Axel},
doi = {10.1016/j.concog.2015.05.004},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S1053810015001129-main.pdf:pdf},
issn = {10902376},
journal = {Consciousness and Cognition},
keywords = {Awareness,Consciousness,Dichotomous,Gradual,Levels of processing,Vision},
pages = {1--11},
pmid = {26057402},
title = {{Does level of processing affect the transition from unconscious to conscious perception?}},
volume = {36},
year = {2015}
}
@article{Stickgold2013,
abstract = {The brain does not retain all the information it encodes in a day. Much is forgotten, and of those memories retained, their subsequent evolution can follow any of a number of pathways. Emerging data makes clear that sleep is a compelling candidate for performing many of these operations. But how does the sleeping brain know which information to preserve and which to forget? What should sleep do with that information it chooses to keep? For information that is retained, sleep can integrate it into existing memory networks, look for common patterns and distill overarching rules, or simply stabilize and strengthen the memory exactly as it was learned. We suggest such 'memory triage' lies at the heart of a sleep-dependent memory processing system that selects new information, in a discriminatory manner, and assimilates it into the brain's vast armamentarium of evolving knowledge, helping guide each organism through its own, unique life.},
author = {Stickgold, Robert and Walker, Matthew P},
doi = {10.1038/nn.3303},
file = {:home/andrew/Documents/grad/Papers/Walker{\_}RV{\_}NatNeuro{\_}2013.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$r1097-6256 (Linking)},
issn = {1097-6256},
journal = {Nature Neuroscience},
number = {2},
pages = {139--145},
pmid = {23354387},
title = {{Sleep-dependent memory triage: evolving generalization through selective processing}},
url = {http://www.nature.com/doifinder/10.1038/nn.3303},
volume = {16},
year = {2013}
}
@article{Gomez2006,
abstract = {Infants engage in an extraordinary amount of learning during their waking hours even though much of their day is consumed by sleep. What role does sleep play in infant learning? Fifteen-month-olds were familiarized with an artificial language 4 hr prior to a lab visit. Learning the language involved relating initial and final words in auditory strings by remembering the exact word dependencies or by remembering an abstract relation be- tween initial and final words. One group napped during the interval between familiarization and test. Another group did not nap. Infants who napped appeared to re- member a more abstract relation, one they could apply to stimuli that were similar but not identical to those from familiarization. Infants whodid not nap showed amemory effect. Naps appear to promote a qualitative change in memory, one involving greater flexibility in learning.},
author = {Gomez, R L and Bootzin, Richard R. and Nadel, L},
doi = {10.1111/j.1467-9280.2006.01764.x},
file = {:home/andrew/Documents/grad/Papers/j.1467-9280.2006.01764.x.pdf:pdf},
issn = {0956-7976},
journal = {Psychological Science},
number = {8},
pages = {670--674},
pmid = {16913948},
title = {{Naps promote abstraction in language learning infants}},
volume = {17},
year = {2006}
}
@article{Doyen2012,
abstract = {The perspective that behavior is often driven by unconscious determinants has become widespread in social psychology. Bargh, Chen, and Burrows' (1996) famous study, in which participants unwittingly exposed to the stereotype of age walked slower when exiting the laboratory, was instrumental in defining this perspective. Here, we present two experiments aimed at replicating the original study. Despite the use of automated timing methods and a larger sample, our first experiment failed to show priming. Our second experiment was aimed at manipulating the beliefs of the experimenters: Half were led to think that participants would walk slower when primed congruently, and the other half was led to expect the opposite. Strikingly, we obtained a walking speed effect, but only when experimenters believed participants would indeed walk slower. This suggests that both priming and experimenters' expectations are instrumental in explaining the walking speed effect. Further, debriefing was suggestive of awareness of the primes. We conclude that unconscious behavioral priming is real, while real, involves mechanisms different from those typically assumed to cause the effect.},
author = {Doyen, St{\'{e}}phane and Klein, Olivier and Pichon, Cora Lise and Cleeremans, Axel},
doi = {10.1371/journal.pone.0029081},
file = {:home/andrew/Documents/grad/Papers/journal.pone.0029081.PDF:PDF},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {1},
pmid = {22279526},
title = {{Behavioral priming: It's all in the mind, but whose mind?}},
volume = {7},
year = {2012}
}
@article{Cleeremans2014,
abstract = {Consciousness remains a mystery-"a phenomenon that people do not know how to think about-yet" (Dennett, , p. 21). Here, I consider how the connectionist perspective on information processing may help us progress toward the goal of understanding the computational principles through which conscious and unconscious processing differ. I begin by delineating the conceptual challenges associated with classical approaches to cognition insofar as understanding unconscious information processing is concerned, and to highlight several contrasting computational principles that are constitutive of the connectionist approach. This leads me to suggest that conscious and unconscious processing are fundamentally connected, that is, rooted in the very same computational principles. I further develop a perspective according to which the brain continuously and unconsciously learns to redescribe its own activity itself based on constant interaction with itself, with the world, and with other minds. The outcome of such interactions is the emergence of internal models that are metacognitive in nature and that function so as to make it possible for an agent to develop a (limited, implicit, practical) understanding of itself. In this light, plasticity and learning are constitutive of what makes us conscious, for it is in virtue of our own experiences with ourselves and with other people that our mental life acquires its subjective character. The connectionist framework continues to be uniquely positioned in the Cognitive Sciences to address the challenge of identifying what one could call the "computational correlates of consciousness" (Mathis {\&} Mozer, ) because it makes it possible to focus on the mechanisms through which information processing takes place.},
author = {Cleeremans, Axel},
doi = {10.1111/cogs.12149},
file = {:home/andrew/Documents/grad/Papers/Cleeremans-2014-Cognitive{\_}Science.pdf:pdf},
isbn = {9780080430768},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Connectionist modeling,Consciousness,Learning,Metacognition},
number = {6},
pages = {1286--1315},
pmid = {25087683},
title = {{Connecting conscious and unconscious processing}},
volume = {38},
year = {2014}
}
@article{Newell2014,
abstract = {{\textless}p{\textgreater}To what extent do we know our own minds when making decisions? Variants of this question have preoccupied researchers in a wide range of domains, from mainstream experimental psychology (cognition, perception, social behavior) to cognitive neuroscience and behavioral economics. A pervasive view places a heavy explanatory burden on an intelligent cognitive unconscious, with many theories assigning causally effective roles to unconscious influences. This article presents a novel framework for evaluating these claims and reviews evidence from three major bodies of research in which unconscious factors have been studied: multiple-cue judgment, deliberation without attention, and decisions under uncertainty. Studies of priming (subliminal and primes-to-behavior) and the role of awareness in movement and perception (e.g., timing of willed actions, blindsight) are also given brief consideration. The review highlights that inadequate procedures for assessing awareness, failures to consider artifactual explanations of â€œlandmarkâ€ results, and a tendency to uncritically accept conclusions that fit with our intuitions have all contributed to unconscious influences being ascribed inflated and erroneous explanatory power in theories of decision making. The review concludes by recommending that future research should focus on tasks in which participants' attention is diverted away from the experimenter's hypothesis, rather than the highly reflective tasks that are currently often employed.{\textless}/p{\textgreater}},
author = {Newell, Ben R. and Shanks, David R.},
doi = {10.1017/S0140525X12003214},
file = {:home/andrew/Documents/grad/Papers/unconscious{\_}influences{\_}on{\_}decision{\_}making{\_}a{\_}critical{\_}review.pdf:pdf},
isbn = {0140-525X, 1469-1825},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
keywords = {awareness,conscious,decision making,deliberation,intuition,judgment,perceptual-motor skills,unconscious},
number = {01},
pages = {1--19},
pmid = {24461214},
title = {{Unconscious influences on decision making: A critical review}},
url = {http://www.journals.cambridge.org/abstract{\_}S0140525X12003214},
volume = {38},
year = {2014}
}
@inproceedings{Lake2017,
abstract = {Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.},
archivePrefix = {arXiv},
arxivId = {1711.00350},
author = {Lake, Brenden M. and Baroni, Marco},
booktitle = {International Conference on Machine Learning},
eprint = {1711.00350},
file = {:home/andrew/Documents/grad/Papers/LakeBaroniArXivNotSystematic.pdf:pdf},
pages = {1--12},
title = {{Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks}},
url = {http://arxiv.org/abs/1711.00350},
year = {2018}
}
@article{Bowers2017,
author = {Bowers, Jeffrey S.},
doi = {10.1016/j.tics.2017.09.013},
file = {:home/andrew/Documents/grad/Papers/Bowers17TiCSPDPInAgeOFDeepNetworks.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
pages = {1--12},
pmid = {29100738},
publisher = {Elsevier Ltd},
title = {{Parallel Distributed Processing Theory in the Age of Deep Networks}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661317302164},
year = {2017}
}
@article{Mezzadri2006,
abstract = {We discuss how to generate random unitary matrices from the classical compact groups U(N), O(N) and USp(N) with probability distributions given by the respective invariant measures. The algorithm is straightforward to implement using standard linear algebra packages. This approach extends to the Dyson circular ensembles too. This article is based on a lecture given by the author at the summer school on Number Theory and Random Matrix Theory held at the University of Rochester in June 2006. The exposition is addressed to a general mathematical audience.},
archivePrefix = {arXiv},
arxivId = {math-ph/0609050},
author = {Mezzadri, Francesco},
eprint = {0609050},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mezzadri - 2006 - How to generate random matrices from the classical compact groups.pdf:pdf},
issn = {00029920},
primaryClass = {math-ph},
title = {{How to generate random matrices from the classical compact groups}},
url = {http://arxiv.org/abs/math-ph/0609050},
year = {2006}
}
@article{Schapiro2013,
author = {Schapiro, Anna C and Rogers, Timothy T and Cordova, Natalia I and Turk-, Nicholas B and Botvinick, Matthew M},
doi = {10.1038/nn.3331.Neural},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schapiro et al. - 2013 - Neural representations of events arise from temporal community structure.pdf:pdf},
journal = {Nature Neuroscience},
number = {4},
pages = {486--492},
title = {{Neural representations of events arise from temporal community structure}},
volume = {16},
year = {2013}
}
@article{Schapiro2017,
abstract = {A growing literature suggests that the hippocampus is critical for the rapid extraction of regularities from the environment. Although this fits with the known role of the hippocampus in rapid learning, it seems at odds with the idea that the hippocampus specializes in memorizing individual episodes. In particular, the Complementary Learning Systems theory argues that there is a computational trade-off between learning the specifics of individual experiences and regularities that hold across those experiences. We asked whether it is possible for the hippocampus to handle both statistical learning and memorization of individual episodes. We exposed a neural network model that instantiates known properties of hippocampal projections and subfields to sequences of items with temporal regularities. We found that the monosynaptic pathway - the pathway connecting entorhinal cortex directly to region CA1 - was able to support statistical learning, while the trisynaptic pathway - connecting entorhinal cortex to CA1 through dentate gyrus and CA3 - learned only individual episodes, with apparent representations of regularities resulting from associative reactivation through recurrence. Thus, in paradigms involving rapid learning, the computational trade-off between learning episodes and regularities may be handled by separate anatomical pathways within the hippocampus itself.},
author = {Schapiro, Anna C. and Turk-Browne, Nicholas B. and Botvinick, Matthew M. and Norman, Kenneth A.},
doi = {10.1098/rstb.2016.0049},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schapiro et al. - 2017 - Complementary learning systems within the hippocampus a neural network modelling approach to reconciling episod.pdf:pdf},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
keywords = {behaviour,cognition,neuroscience},
number = {1711},
pages = {20160049},
pmid = {1000305518},
title = {{Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning}},
url = {http://rstb.royalsocietypublishing.org/lookup/doi/10.1098/rstb.2016.0049},
volume = {372},
year = {2017}
}
@article{Simonsohn2015,
abstract = {When studies examine true effects, they generate right-skewed p-curves, distributions of statistically significant results with more low (.01 s) than high (.04 s) p values. What else can cause a right-skewed p-curve? First, we consider the possibility that researchers report only the smallest significant p value (as conjectured by Ulrich {\&} Miller, 2015), concluding that it is a very uncommon problem. We then consider more common problems, including (a) p-curvers selecting the wrong p values, (b) fake data, (c) honest errors, and (d) ambitiously p-hacked (beyond p {\textless} .05) results. We evaluate the impact of these common problems on the validity of p-curve analysis, and provide practical solutions that substantially increase its robustness.},
author = {Simonsohn, Uri and Simmons, Joseph P. and Nelson, Leif D.},
doi = {10.1037/xge0000104},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonsohn, Simmons, Nelson - 2015 - Better P-curves Making P-curve analysis more robust to errors, fraud, and ambitious P-hacking,(2015).pdf:pdf},
isbn = {0096-3445},
issn = {1939-2222},
journal = {Journal of Experimental Psychology: General},
keywords = {are more likely to,as a consequence,be,because statistically significant results,is biased,it is difficult to,know whether a set,of,p -curve,p -hacking,publication bias,published scientific evidence,published than nonsignificant ones},
number = {6},
pages = {1146--1152},
pmid = {26595842},
title = {{Better P-curves: Making P-curve analysis more robust to errors, fraud, and ambitious P-hacking, a Reply to Ulrich and Miller (2015).}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xge0000104},
volume = {144},
year = {2015}
}
@article{Schapiro2014,
abstract = {The sensory input that we experience is highly patterned, and we are experts at detecting these regularities. Although the extraction of such regularities, or statistical learning (SL), is typically viewed as a cortical process, recent studies have implicated the medial temporal lobe (MTL), including the hippocampus. These studies have employed fMRI, leaving open the possibility that the MTL is involved but not necessary for SL. Here, we examined this issue in a case study of LSJ, a patient with complete bilateral hippocampal loss and broader MTL damage. In Experiments 1 and 2, LSJ and matched control participants were passively exposed to a continuous sequence of shapes, syllables, scenes, or tones containing temporal regularities in the co-occurrence of items. In a subsequent test phase, the control groups exhibited reliable SL in all conditions, successfully discriminating regularities from recombinations of the same items into novel foil sequences. LSJ, however, exhibited no SL, failing to discriminate regularities from foils. Experiment 3 ruled out more general explanations for this failure, such as inattention during exposure or difficulty following test instructions, by showing that LSJ could discriminate which individual items had been exposed. These findings provide converging support for the importance of the MTL in extracting temporal regularities},
archivePrefix = {arXiv},
arxivId = {1511.04103},
author = {Schapiro, Anna and Gregory, Emma and Landau, Barbara and McCloskey, Michael and Turk-Browne, Nicholas},
doi = {10.1162/jocn},
eprint = {1511.04103},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schapiro et al. - 2014 - The Necessity of the Medial Temporal Lobe for Statistical Learning.pdf:pdf},
isbn = {9780192880512},
issn = {09528229},
journal = {Journal of Cognitive Neuroscience},
number = {8},
pages = {1736--1747},
pmid = {23647519},
title = {{The Necessity of the Medial Temporal Lobe for Statistical Learning}},
volume = {26},
year = {2014}
}
@article{Simonsohn2014,
abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that "work," readers must ask, "Are these effects true, or do they merely reflect selective reporting?" We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps {\textless} .05). Because only true effects are expected to generate right-skewed p-curves-containing more low (.01s) than high (.04s) significant p values--only right-skewed p--curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses.},
author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
doi = {10.1037/a0033242},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonsohn, Nelson, Simmons - 2014 - P-curve A key to the file-drawer.pdf:pdf},
isbn = {0096-3445},
issn = {1939-2222},
journal = {Journal of Experimental Psychology: General},
keywords = {10,1037,a0033242,doi,dx,false-positive psychology,http,hypothesis testing,org,p -hacking,publication bias,selective reporting,supp,supplemental materials},
number = {2},
pages = {534--547},
pmid = {23855496},
title = {{P-curve: A key to the file-drawer.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0033242},
volume = {143},
year = {2014}
}
@article{Momennejad2017,
abstract = {Theories of reinforcement learning in neuroscience have focused on two families of algorithms. Model-free algorithms cache action values, making them cheap but inflexible: a candidate mechanism for adaptive and maladaptive habits. Model-based algorithms achieve flexibility at computational expense, by rebuilding values from a model of the environment. We examine an intermediate class of algorithms, the successor representation (SR), which caches long-run state expectancies, blending model-free efficiency with model-based flexibility. Although previous reward revaluation studies distinguish model-free from model-based learning algorithms, such designs cannot discriminate between model-based and SR-based algorithms, both of which predict sensitivity to reward revaluation. However, changing the transition structure (" transition revaluation ") should selectively impair revaluation for the SR. In two studies we provide evidence that humans are differentially sensitive to reward vs. transition revaluation, consistent with SR predictions. These results support a new neuro-computational mechanism for flexible choice, while introducing a subtler, more cognitive notion of habit.},
author = {Momennejad, I. and Russek, E. M. and Cheong, J. H. and Botvinick, M. M. and Daw, N. D. and Gershman, S. J.},
doi = {10.1038/s41562-017-0180-8},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Momennejad et al. - 2017 - The successor representation in human reinforcement learning.pdf:pdf},
issn = {2397-3374},
journal = {Nature Human Behaviour},
keywords = {decision making,human behavior,planning,reinforcement learning,retrospective revaluation,successor representation},
number = {9},
pages = {680--692},
title = {{The successor representation in human reinforcement learning}},
url = {http://www.nature.com/articles/s41562-017-0180-8},
volume = {1},
year = {2017}
}
@article{McClelland2013a,
abstract = {The complementary learning systems theory of the roles of hippocampus and neocortex (McClelland, McNaughton, {\&} O'Reilly, 1995) holds that the rapid integration of arbitrary new information into neocortical structures is avoided to prevent catastrophic interference with structured knowledge representations stored in synaptic connections among neocortical neurons. Recent studies (Tse et al., 2007, 2011) showed that neocortical circuits can rapidly acquire new associations that are consistent with prior knowledge. The findings challenge the complementary learning systems theory as previously presented. However, new simulations extending those reported in McClelland et al. (1995) show that new information that is consistent with knowledge previously acquired by a putatively cortexlike artificial neural network can be learned rapidly and without interfering with existing knowledge; it is when inconsistent new knowledge is acquired quickly that catastrophic interference ensues. Several important features of the findings of Tse et al. (2007, 2011) are captured in these simulations, indicating that the neural network model used in McClelland et al. has characteristics in common with neocortical learning mechanisms. An additional simulation generalizes beyond the network model previously used, showing how the rate of change of cortical connections can depend on prior knowledge in an arguably more biologically plausible network architecture. In sum, the findings of Tse et al. are fully consistent with the idea that hippocampus and neocortex are complementary learning systems. Taken together, these findings and the simulations reported here advance our knowledge by bringing out the role of consistency of new experience with existing knowledge and demonstrating that the rate of change of connections in real and artificial neural networks can be strongly prior-knowledge dependent.},
author = {McClelland, James L.},
doi = {10.1037/a0033812},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McClelland - 2013 - Incorporating rapid neocortical learning of new schema-consistent information into complementary learning systems(2).pdf:pdf},
isbn = {6507251232},
issn = {1939-2222},
journal = {Journal of Experimental Psychology: General},
keywords = {1932,1979,2007,2011,bartlett,bransford,consolidation,hippocampus,in two recent articles,it has long been,known that it is,learning,memory,new,relatively easy to learn,schemas,things that are consistent,tse et al,with prior knowledge},
number = {4},
pages = {1190--1210},
pmid = {23978185},
title = {{Incorporating rapid neocortical learning of new schema-consistent information into complementary learning systems theory.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0033812},
volume = {142},
year = {2013}
}
@article{McClelland1995,
abstract = {Damage to the hippocampal system disrupts recent memory but leaves remote memory intact. The account presented here suggests that memories are first stored via synaptic changes in the hippocampal system, that these changes support reinstatement of recent memories in the neocortex, that neocortical synapses change a little on each reinstatement, and that remote memory is based on accumulated neocortical changes. Models that learn via changes to connections help explain this organization. These models discover the structure in ensembles of items if learning of each item is gradual and interleaved with learning about other items. This suggests that the neocortex learns slowly to discover the structure in ensembles of experiences. The hippocampal system permits rapid learning of new items without disrupting this structure, and reinstatement of new memories interleaves them with others to integrate them into structured neocortical memory systems.},
author = {McClelland, James L. and McNaughton, Bruce L. and O'Reilly, Randall C.},
doi = {10.1037/0033-295X.102.3.419},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McClelland, McNaughton, O'Reilly - 1995 - Why there are complementary learning systems in the hippocampus and neocortex Insights from th.pdf:pdf},
isbn = {0033-295X (Print)$\backslash$r0033-295X (Linking)},
issn = {1939-1471},
journal = {Psychological Review},
number = {3},
pages = {419--457},
pmid = {7624455},
title = {{Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.102.3.419},
volume = {102},
year = {1995}
}
@article{Neyshabur2017,
abstract = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
archivePrefix = {arXiv},
arxivId = {1706.08947},
author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
eprint = {1706.08947},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neyshabur et al. - 2017 - Exploring Generalization in Deep Learning.pdf:pdf},
title = {{Exploring Generalization in Deep Learning}},
year = {2017}
}
@article{Nie2017,
abstract = {Sentence vectors represent an appealing approach to meaning: learn an embedding that encompasses the meaning of a sentence in a single vector, that can be used for a variety of semantic tasks. Existing models for learning sentence embeddings either require extensive computational resources to train on large corpora, or are trained on costly, manually curated datasets of sentence relations. We observe that humans naturally annotate the relations between their sentences with discourse markers like "but" and "because". These words are deeply linked to the meanings of the sentences they connect. Using this natural signal, we automatically collect a classification dataset from unannotated text. Training a model to predict these discourse markers yields high quality sentence embeddings. Our model captures complementary information to existing models and achieves comparable generalization performance to state of the art models.},
archivePrefix = {arXiv},
arxivId = {1710.04334},
author = {Nie, Allen and Bennett, Erin D. and Goodman, Noah D.},
eprint = {1710.04334},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nie, Bennett, Goodman - 2017 - DisSent Sentence Representation Learning from Explicit Discourse Relations.pdf:pdf},
journal = {arXiv},
title = {{DisSent: Sentence Representation Learning from Explicit Discourse Relations}},
year = {2017}
}
@article{Advani2017,
abstract = {We perform an average case analysis of the generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant "high-dimensional" regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that naive application of worst-case theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
archivePrefix = {arXiv},
arxivId = {1710.03667},
author = {Advani, Madhu S. and Saxe, Andrew M.},
eprint = {1710.03667},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Advani, Saxe - 2017 - High-dimensional dynamics of generalization error in neural networks.pdf:pdf},
journal = {arXiv},
pages = {1--32},
title = {{High-dimensional dynamics of generalization error in neural networks}},
year = {2017}
}
@article{Berry1991,
author = {Berry, Diane C.},
doi = {10.1080/14640749108400961},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berry - 1991 - The role of action in implicit learning.pdf:pdf},
journal = {The Quarterly Journal of Experimental Psychology},
number = {4},
pages = {881--906},
title = {{The role of action in implicit learning}},
volume = {43},
year = {1991}
}
@article{Gibson1996,
author = {Gibson, Faison P},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gibson - 1996 - The Sugar Production Factory--A Dynamic Decision Task.pdf:pdf},
keywords = {dynamic decision making},
pages = {49--60},
title = {{The Sugar Production Factory--A Dynamic Decision Task}},
volume = {1},
year = {1996}
}
@article{Andreasa,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.02799v4},
author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
eprint = {arXiv:1511.02799v4},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andreas et al. - 2017 - Deep Compositional Question Answering with Neural Module Networks.pdf:pdf},
title = {{Deep Compositional Question Answering with Neural Module Networks}},
year = {2017}
}
@misc{Campbell1980,
author = {Campbell, G. and Geller, S.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Campbell, Geller - 1980 - Balanced Latin Squares.pdf:pdf},
title = {{Balanced Latin Squares}},
year = {1980}
}
@article{Gauthier2016,
abstract = {A distinguishing property of human intelligence is the ability to flexibly use language in order to communicate complex ideas with other humans in a variety of contexts. Research in natural language dialogue should focus on designing communicative agents which can integrate themselves into these contexts and productively collaborate with humans. In this abstract, we propose a general situated language learning paradigm which is designed to bring about robust language agents able to cooperate productively with humans.},
archivePrefix = {arXiv},
arxivId = {1610.03585},
author = {Gauthier, Jon and Mordatch, Igor},
eprint = {1610.03585},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gauthier, Mordatch - 2016 - A Paradigm for Situated and Goal-Driven Language Learning.pdf:pdf},
journal = {arXiv},
title = {{A Paradigm for Situated and Goal-Driven Language Learning}},
year = {2016}
}
@article{Mcnaughton2006,
author = {Mcnaughton, Bruce L and Battaglia, Francesco P and Jensen, Ole and Moser, Edvard I},
doi = {10.1038/nrn1932},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcnaughton et al. - 2006 - Path integration and the neural basis of the â€˜ cognitive map '.pdf:pdf},
number = {August},
pages = {663--678},
title = {{Path integration and the neural basis of the â€˜ cognitive map '}},
volume = {7},
year = {2006}
}
@article{Chaplot2017,
abstract = {To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding. We propose an end-to-end trainable neural architecture for task-oriented language grounding in 3D environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input. The proposed model combines the image and text representations using a Gated-Attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods. We show the effectiveness of the proposed model on unseen instructions as well as unseen maps, both quantitatively and qualitatively. We also introduce a novel environment based on a 3D game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states.},
archivePrefix = {arXiv},
arxivId = {1706.07230},
author = {Chaplot, Devendra Singh and Sathyendra, Kanthashree Mysore and Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov, Ruslan},
eprint = {1706.07230},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaplot et al. - 2017 - Gated-Attention Architectures for Task-Oriented Language Grounding.pdf:pdf},
journal = {arXiv},
title = {{Gated-Attention Architectures for Task-Oriented Language Grounding}},
year = {2017}
}
@article{Marcus1993,
abstract = {There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom- ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor- pora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valu- able for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investi- gation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.},
author = {Marcus, Mitchell P and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
doi = {10.1162/coli.2010.36.1.36100},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marcus, Santorini, Marcinkiewicz - 1993 - Building a large annotated corpus of English The Penn Treebank.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
keywords = {POS-Tagging},
number = {2},
pages = {313--330},
title = {{Building a large annotated corpus of English: The Penn Treebank}},
volume = {19},
year = {1993}
}
@article{Lampinen2017a,
author = {Lampinen, Andrew and Hsu, Shaw and Mcclelland, James L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lampinen, Hsu, Mcclelland - 2017 - Analogies Emerge from Learning Dyamics in Neural Networks.pdf:pdf},
journal = {Proceedings of the 39th Annual Conference of the Cognitive Science Society},
keywords = {analogy,neural networks,representa-,structure learning,tion,transfer},
pages = {2512--2517},
title = {{Analogies Emerge from Learning Dyamics in Neural Networks}},
year = {2017}
}
@inproceedings{Cotterell2016,
author = {Cotterell, Ryan and Schuetze, Hinrich and Eisner, Jason},
booktitle = {Proceedings of the 54th Annual Meeting of the ACL},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cotterell, Schuetze, Eisner - 2016 - Morphological Smoothing and Extrapolation of Word Embeddings.pdf:pdf},
title = {{Morphological Smoothing and Extrapolation of Word Embeddings}},
year = {2016}
}
@inproceedings{Kaliszyk2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.00426v1},
author = {Kaliszyk, Cezary and Chollet, Fran{\c{c}}ois and Szegedy, Christian},
booktitle = {ICLR},
eprint = {arXiv:1703.00426v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaliszyk, Chollet, Szegedy - 2017 - HOLSTEP A MACHINE LEARNING DATASET FOR HIGHER-ORDER LOGIC THEOREM PROVING.pdf:pdf},
pages = {1--12},
title = {{HOLSTEP: A MACHINE LEARNING DATASET FOR HIGHER-ORDER LOGIC THEOREM PROVING}},
year = {2017}
}
@article{Machens,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.08026v2},
author = {Machens, Christian K},
eprint = {arXiv:1705.08026v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Machens - 2017 - Learning arbitrary dynamics in efficient, balanced spiking networks using local plasticity rules.pdf:pdf},
journal = {arXiv preprint},
title = {{Learning arbitrary dynamics in efficient, balanced spiking networks using local plasticity rules}},
year = {2017}
}
@article{Liu2016,
abstract = {An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.},
archivePrefix = {arXiv},
arxivId = {1611.02770},
author = {Liu, Yanpei and Chen, Xinyun and Liu, Chang and Song, Dawn},
eprint = {1611.02770},
journal = {arXiv},
number = {2},
pages = {1--24},
title = {{Delving into Transferable Adversarial Examples and Black-box Attacks}},
year = {2016}
}
@article{Christiano2017,
abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
archivePrefix = {arXiv},
arxivId = {1706.03741},
author = {Christiano, Paul and Leike, Jan and Brown, Tom B and Martic, Miljan and Legg, Shane and Amodei, Dario},
eprint = {1706.03741},
journal = {arXi},
title = {{Deep reinforcement learning from human preferences}},
year = {2017}
}
@article{Nguyen2017,
abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [36] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227 Ã— 227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models â€œPlug and Play Generative Networks.â€ PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable â€œconditionâ€ network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization [39], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.},
archivePrefix = {arXiv},
arxivId = {arXiv:submit/1738978},
author = {Nguyen, Anh and Yosinski, Jason and Bengio, Yoshua and Dosovitskiy, Alexey and Clune, Jeff},
doi = {10.1109/CVPR.2017.374},
eprint = {1738978},
journal = {ICCV},
number = {3},
pages = {33},
primaryClass = {arXiv:submit},
title = {{Plug {\{}{\&}{\}} Play Generative Networks: Conditional Iterative Generation of Images in Latent Space}},
year = {2017}
}
@article{Nguyen2016,
abstract = {We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.},
archivePrefix = {arXiv},
arxivId = {1602.03616},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
eprint = {1602.03616},
journal = {arXiv},
title = {{Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks}},
year = {2016}
}
@article{Szegedy2016,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible. We benchmark our methods on the ILSVRC 2012 classification challenge validation set and demonstrate substantial gains over the state of the art via to carefully factorized convolutions and aggressive regularization: 21.2{\{}{\%}{\}} top-1 and 5.6{\{}{\%}{\}} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1002/2014GB005021},
eprint = {1512.00567},
isbn = {9781617796029},
issn = {08866236},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2818--2826},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
year = {2016}
}
@article{Odena2016,
abstract = {Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7{\{}{\%}{\}} of the classes have samples exhibiting diversity comparable to real ImageNet data.},
archivePrefix = {arXiv},
arxivId = {1610.09585},
author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
eprint = {1610.09585},
issn = {1938-7228},
journal = {arXiv},
pages = {1--14},
title = {{Conditional Image Synthesis With Auxiliary Classifier GANs}},
year = {2016}
}
@article{Yosinski2014,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99{\{}{\%}{\}} confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh},
doi = {10.1109/CVPR.2015.7298640},
eprint = {1412.1897},
isbn = {9781467369640},
journal = {arXiv},
title = {{Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}},
year = {2014}
}
@article{Sun2017,
abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
archivePrefix = {arXiv},
arxivId = {1707.02968},
author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
doi = {1707.02968},
eprint = {1707.02968},
journal = {arXiv},
title = {{Revisiting Unreasonable Effectiveness of Data in Deep Learning Era}},
year = {2017}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P and Ba, Jimmy},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
isbn = {9781450300728},
issn = {09252312},
journal = {Iclr},
pages = {1--15},
pmid = {172668},
title = {{Adam: A Method for Stochastic Optimization}},
year = {2015}
}
@article{Parisotto2015,
abstract = {The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed "Actor-Mimic", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.},
archivePrefix = {arXiv},
arxivId = {1511.06342},
author = {Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
eprint = {1511.06342},
journal = {arXiv},
pages = {1--16},
title = {{Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning}},
year = {2015}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
doi = {10.1063/1.4931082},
eprint = {1503.02531},
isbn = {3531207857},
issn = {0022-2488},
journal = {arXiv},
pages = {1--9},
pmid = {18249735},
title = {{Distilling the Knowledge in a Neural Network}},
year = {2015}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
isbn = {0950-5849},
issn = {09505849},
journal = {arXiv},
pages = {1--10},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
year = {2014}
}
@article{Yamins2014,
abstract = {The ventral visual stream underlies key human visual object recognition abilities. However, neural encoding in the higher areas of the ventral stream remains poorly understood. Here, we describe a modeling approach that yields a quantitatively accurate model of inferior temporal (IT) cortex, the highest ventral cortical area. Using high-throughput computational techniques, we discovered that, within a class of biologically plausible hierarchical neural network models, there is a strong correlation between a model's categorization performance and its ability to predict individual IT neural unit response data. To pursue this idea, we then identified a high-performing neural network that matches human performance on a range of recognition tasks. Critically, even though we did not constrain this model to match neural data, its top output layer turns out to be highly predictive of IT spiking responses to complex naturalistic images at both the single site and population levels. Moreover, the model's intermediate layers are highly predictive of neural responses in the V4 cortex, a midlevel visual area that provides the dominant cortical input to IT. These results show that performance optimization--applied in a biologically appropriate model class--can be used to build quantitative predictive models of neural processing.},
archivePrefix = {arXiv},
arxivId = {0706.1062v1},
author = {Yamins, Daniel L K and Hong, Ha and Cadieu, Charles F and Solomon, Ethan A and Seibert, Darren and DiCarlo, James J},
doi = {10.1073/pnas.1403112111},
eprint = {0706.1062v1},
isbn = {0902262106},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {array electrophysiology,computational neuroscience,computer vision},
number = {23},
pages = {8619--8624},
pmid = {24812127},
title = {{Performance-optimized hierarchical models predict neural responses in higher visual cortex.}},
volume = {111},
year = {2014}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
pmid = {16190471},
publisher = {Springer US},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {Computer Visionâ€“ECCV 2014},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and Understanding Convolutional Networks arXiv:1311.2901v3 [cs.CV] 28 Nov 2013}},
volume = {8689},
year = {2014}
}
@article{Elgammal2017,
abstract = {We propose a new system for generating art. The system generates art by look-ing at art and learning about style; and becomes creative by increasing the arousal potential of the generated art by deviating from the learned styles. We build over Generative Adversarial Networks (GAN), which have shown the ability to learn to generate novel images simulating a given distribution. We argue that such net-works are limited in their ability to generate creative products in their original design. We propose modifications to its objective to make it capable of generating creative art by maximizing deviation from established styles and minimizing de-viation from art distribution. We conducted experiments to compare the response of human subjects to the generated art with their response to art created by artists. The results show that human subjects could not distinguish art generated by the proposed system from art generated by contemporary artists and shown in top art fairs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.07068},
author = {Elgammal, Ahmed and Liu, Bingchen and Elhoseiny, Mohamed and Mazzone, Marian},
eprint = {arXiv:1706.07068},
journal = {arXiv},
number = {Iccc},
pages = {1--22},
title = {{CAN: Creative Adversarial Networks Generating "Art" by Learning About Styles and Deviating from Style Norms}},
year = {2017}
}
@book{Rogers2004,
author = {Rogers, Timothy T and McClelland, James L.},
publisher = {MIT Press},
title = {{Semantic Cognition: A Parallel Distributed Processing Approach}},
year = {2004}
}
@article{Zaremba2014a,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
archivePrefix = {arXiv},
arxivId = {1409.2329},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
doi = {ng},
eprint = {1409.2329},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaremba, Sutskever, Vinyals - 2014 - Recurrent Neural Network Regularization.pdf:pdf},
isbn = {078036404X},
issn = {0157244X},
journal = {arXiv},
pages = {1--8},
pmid = {23259955},
title = {{Recurrent Neural Network Regularization}},
year = {2014}
}
@article{Wang2017,
abstract = {We test whether distributional models can do one-shot learning of definitional properties from text only. Using Bayesian models, we find that first learning overarching structure in the known data, regularities in textual contexts and in properties, helps one-shot learning, and that individual context items can be highly informative.},
archivePrefix = {arXiv},
arxivId = {1704.04550},
author = {Wang, Su and Roller, Stephen and Erk, Katrin},
eprint = {1704.04550},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Roller, Erk - 2017 - Distributional model on a diet One-shot word learning from text only.pdf:pdf},
journal = {arXiv preprint},
title = {{Distributional model on a diet: One-shot word learning from text only}},
year = {2017}
}
@article{Rumelhart1993,
abstract = {(From the chapter) a bewildering variety of connectionist applications has cropped up throughout the cognitive sciences and engineering one of the central issues in all of these models is the representation of knowledge in the connectionist network getting a coherent picture of "what goes on" inside a network as it develops, manipulates, and alters the representation of the knowledge it processes is vital for our understanding of connectionist information processing, and likely for our understanding of the minds these systems model explore the sorts of representations that connectionist systems employ and the crucial role learning plays in constructing them distributed versus localist representations learning representations in connectionist networks autoencoders representing semantic networks in connectionist systems connectionist representations and human judgments of similarity (PsycINFO Database Record (c) 2003 APA},
author = {Rumelhart, David E and Todd, Peter M},
isbn = {0262132842},
issn = {0-262-13284-2},
journal = {Attention and performance XIV: Synergies in experimental psychology, artificial intelligence, and cognitive neuroscience},
pages = {3--30},
title = {{Learning and connectionist representations}},
year = {1993}
}
@article{Lazaridou2017,
abstract = {By the time they reach early adulthood, English speakers are familiar with the meaning of thousands of words. In the last decades, computational simulations known as distributional semantic models have demonstrated that it is possible to induce word meaning representations solely from word co-occurrence statis- tics extracted from a large amount of text. However, while these models learn in batch mode from large corpora, human word learning proceeds incrementally after minimal exposure to new words. In this study, we run a set of experiments investigating whether minimal distributional evidence from very short passages suffices to trigger successful word learning in subjects, testing their linguistic and visual intuitions about the concepts associated to new words. After con- firming that subjects are indeed very efficient distributional learners even from small amounts of evidence, we test a distributional semantic model on the same multimodal task, finding that it behaves in a remarkable human-like way. We conclude that distributional semantic models provide a convincing computa- tional account of word learning even at the early stages in which a word is first encountered, and the way they build meaning representations can offer new in- sights into human language acquisition.},
author = {Lazaridou, Angeliki and Marelli, Marco and Baroni, Marco},
doi = {10.1111/cogs.12481},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazaridou, Marelli, Baroni - 2017 - Multimodal Word Meaning Induction From Minimal Exposure to Natural Text.pdf:pdf},
issn = {15516709},
journal = {Cognitive Science},
keywords = {Distributional semantics,Language and the visual world,Multimodality,One-shot learning,Word learning},
pages = {677--705},
title = {{Multimodal Word Meaning Induction From Minimal Exposure to Natural Text}},
volume = {41},
year = {2017}
}
@inproceedings{Santoro2017,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
archivePrefix = {arXiv},
arxivId = {1706.01427},
author = {Santoro, Adam and Raposo, David and Barrett, David G.T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.01427},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Santoro et al. - 2017 - A simple neural network module for relational reasoning.pdf:pdf},
title = {{A simple neural network module for relational reasoning}},
year = {2017}
}
@article{Sutton1992,
abstract = {We introduce the first temporal-difference learning algorithm that is stable with linear function approximation and off-policy training, for any finite Markov de-cision process, behavior policy, and target policy, and whose complexity scales linearly in the number of parameters. We consider an i.i.d. policy-evaluation set-ting in which the data need not come from on-policy experience. The gradient temporal-difference (GTD) algorithm estimates the expected update vector of the TD(0) algorithm and performs stochastic gradient descent on its L 2 norm. We prove that this algorithm is stable and convergent under the usual stochastic ap-proximation conditions to the same least-squares solution as found by the LSTD, but without LSTD's quadratic computational complexity. GTD is online and in-cremental, and does not involve multiplying by products of likelihood ratios as in importance-sampling methods.},
author = {Sutton, Richard S and Szepesv, Csaba},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton, Szepesv - 1992 - A Convergent O(n) Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation.pdf:pdf},
journal = {Computing},
pages = {1--8},
title = {{A Convergent O(n) Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation}},
year = {1992}
}
@article{Gershman2015,
abstract = {Computational models of semantics have emerged as powerful tools for natural language processing. Recent work has developed models to handle compositionality, but these models have typically been evaluated on large, uncontrolled corpora. In this paper, we constructed a controlled set of phrase pairs and collected phrase similarity judgments, revealing novel insights into human semantic representation. None of the computational models that we considered were able to capture the pattern of human judgments. The results of a second experiment, using the same stimuli with a transformational judgment task, support a transformational account of similarity, according to which the similarity between phrases is inversely related to the number of edits required to transform one mental model into another. Taken together, our results indicate that popular models of compositional semantics do not capture important facets of human semantic representation.},
author = {Gershman, Samuel J and Tenenbaum, Joshua B},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gershman, Tenenbaum - 2015 - Phrase similarity in humans and machines.pdf:pdf},
journal = {Proceedings of the 37th Annual Conference of the Cognitive Science Society},
keywords = {neural networks,semantics,similarity},
pages = {776--781},
title = {{Phrase similarity in humans and machines}},
year = {2015}
}
@article{Trench2017,
author = {Trench, M{\'{a}}ximo and Taverini, Lucia M and Goldstone, Robert L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Trench, Taverini, Goldstone - 2017 - Promoting Spontaneous Analogical Transfer by Idealizing Target Representations.pdf:pdf},
journal = {Proceedings of the 39th Annual Conference of the Cognitive Science Society},
keywords = {analogy,idealization,retrieval,transfer},
pages = {1206--1211},
title = {{Promoting Spontaneous Analogical Transfer by Idealizing Target Representations}},
year = {2017}
}
@article{DeLeeuw2015,
author = {de Leeuw, J. R.},
journal = {Behavior Research Methods},
number = {1},
pages = {1--12},
title = {{jsPsych: A JavaScript library for creating behavioral experiments in a web browser.}},
volume = {47},
year = {2015}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to var-ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re-quirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
doi = {10.1.1.231.4614},
eprint = {1103.0398},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf:pdf},
isbn = {1532-4435},
issn = {0891-2017},
journal = {Journal of Machine Learning Research},
keywords = {natural language processing,neural networks},
pages = {2493--2537},
pmid = {1000183096},
title = {{Natural Language Processing (Almost) from Scratch}},
volume = {12},
year = {2011}
}
@article{Hermann2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.06551v2},
author = {Hermann, Karl Moritz and Hill, Felix and Green, Simon and Wang, Fumin and Faulkner, Ryan and Soyer, Hubert and Szepesvari, David and Czarnecki, Wojciech Marian and Jaderberg, Max and Teplyashin, Denis and Wainwright, Marcus and Apps, Chris and Hassabis, Demis},
eprint = {arXiv:1706.06551v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hermann et al. - 2017 - Grounded Language Learning in a Simulated 3D World.pdf:pdf},
journal = {arXiv preprint},
pages = {1--22},
title = {{Grounded Language Learning in a Simulated 3D World}},
year = {2017}
}
@article{Vaswani,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.03762v4},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
eprint = {arXiv:1706.03762v4},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention Is All You Need.pdf:pdf},
journal = {arXiv},
title = {{Attention Is All You Need}},
year = {2017}
}
@article{Rothe2016,
author = {Rothe, Sascha and Schuetze, Hinrich},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rothe, Schuetze - 2016 - Word Embedding Calculus in Meaningful Ultradense Subspaces.pdf:pdf},
pages = {512--517},
title = {{Word Embedding Calculus in Meaningful Ultradense Subspaces}},
year = {2016}
}
@article{Higgins2017,
author = {Higgins, Irina and Sonnerar, Nicolas and Et al., ,},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Higgins, Sonnerar, Et al. - 2017 - SCAN Learning Abstract Hierarchical Compositional Visual Concepts.pdf:pdf},
journal = {arXiv},
title = {{SCAN: Learning Abstract Hierarchical
Compositional Visual Concepts}},
year = {2017}
}
@article{Brochu2010,
abstract = {Eurographics/ ACM SIGGRAPH Symposium on Computer Animation (2010)},
author = {Brochu, Eric and Brochu, Tyson and Freitas, Nando De},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brochu, Brochu, Freitas - 2010 - A Bayesian Interactive Optimization Approach to Procedural Animation Design.pdf:pdf},
journal = {Symposium on Computer Animation},
pages = {103--12},
title = {{A Bayesian Interactive Optimization Approach to Procedural Animation Design}},
year = {2010}
}
@article{Elgammal2017,
abstract = {We propose a new system for generating art. The system generates art by looking at art and learning about style; and becomes creative by increasing the arousal potential of the generated art by deviating from the learned styles. We build over Generative Adversarial Networks (GAN), which have shown the ability to learn to generate novel images simulating a given distribution. We argue that such networks are limited in their ability to generate creative products in their original design. We propose modifications to its objective to make it capable of generating creative art by maximizing deviation from established styles and minimizing deviation from art distribution. We conducted experiments to compare the response of human subjects to the generated art with their response to art created by artists. The results show that human subjects could not distinguish art generated by the proposed system from art generated by contemporary artists and shown in top art fairs. Human subjects even rated the generated images higher on various scales.},
archivePrefix = {arXiv},
arxivId = {1706.07068},
author = {Elgammal, Ahmed and Liu, Bingchen and Elhoseiny, Mohamed and Mazzone, Marian},
eprint = {1706.07068},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elgammal et al. - 2017 - CAN Creative Adversarial Networks, Generating {\&}quotArt{\&}quot by Learning About Styles and Deviating from Style N.pdf:pdf},
number = {Iccc},
pages = {1--22},
title = {{CAN: Creative Adversarial Networks, Generating "Art" by Learning About Styles and Deviating from Style Norms}},
year = {2017}
}
@article{Ericsson2017,
author = {Ericsson, K. Anders},
doi = {10.1002/wcs.1382},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ericsson - 2017 - Expertise and individual differences the search for the structure and acquisition of experts superior performance.pdf:pdf},
issn = {19395086},
journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
number = {1-2},
pages = {1--6},
title = {{Expertise and individual differences: the search for the structure and acquisition of experts' superior performance}},
volume = {8},
year = {2017}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
year = {2015}
}
@article{Ericsson1993,
abstract = {The theoretical framework presented in this article explains expert performance as the end result of individuals' prolonged efforts to improve performance while negotiating motivational and external constraints. In most domains of expertise, individuals begin in their childhood a regimen of effortful activities (deliberate practice) designed to optimize improvement. Individual differences, even among elite performers, are closely related to assessed amounts of deliberate practice. Many characteristics once believed to reflect innate talent are actually the result of intense practice extended for a minimum of 10 yrs. Analysis of expert performance provides unique evidence on the potential and limits of extreme environmental adaptation and learning.},
archivePrefix = {arXiv},
arxivId = {http://doi.apa.org/psycinfo/1993-40718-001},
author = {Ericsson, K. Anders and Krampe, Ralf T. and Tesch-R{\"{o}}mer, Clemens},
doi = {10.1037/0033-295X.100.3.363},
eprint = {/doi.apa.org/psycinfo/1993-40718-001},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ericsson, Krampe, Tesch-R{\"{o}}mer - 1993 - The role of deliberate practice in the acquisition of expert performance.pdf:pdf},
isbn = {1939-1471(Electronic);0033-295X(Print)},
issn = {1939-1471},
journal = {Psychological Review},
number = {3},
pages = {363--406},
pmid = {2140},
primaryClass = {http:},
title = {{The role of deliberate practice in the acquisition of expert performance.}},
volume = {100},
year = {1993}
}
@article{Ritter2017,
abstract = {Deep neural networks (DNNs) have advanced performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. While past work sought to advance our understanding of these models, none has made use of the rich history of problem descriptions, theories, and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.},
archivePrefix = {arXiv},
arxivId = {1706.08606},
author = {Ritter, Samuel and Barrett, David G. T. and Santoro, Adam and Botvinick, Matt M.},
eprint = {1706.08606},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ritter et al. - 2017 - Cognitive Psychology for Deep Neural Networks A Shape Bias Case Study.pdf:pdf},
title = {{Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study}},
year = {2017}
}
@article{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
pmid = {24920543},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@article{Odena2016,
abstract = {Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7{\%} of the classes have samples exhibiting diversity comparable to real ImageNet data.},
archivePrefix = {arXiv},
arxivId = {1610.09585},
author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
eprint = {1610.09585},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Odena, Olah, Shlens - 2016 - Conditional Image Synthesis With Auxiliary Classifier GANs.pdf:pdf},
journal = {arXiv},
pages = {1--16},
title = {{Conditional Image Synthesis With Auxiliary Classifier GANs}},
year = {2016}
}
@article{Christiano2017,
abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
archivePrefix = {arXiv},
arxivId = {1706.03741},
author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
eprint = {1706.03741},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:pdf},
journal = {arXi},
title = {{Deep reinforcement learning from human preferences}},
year = {2017}
}
@article{Mundy2009,
author = {Mundy, Peter and Sullivan, Lisa and Mastergeorge, Ann M},
doi = {10.1002/aur.61},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mundy, Sullivan, Mastergeorge - 2009 - A Parallel and Distributed-Processing Model of Joint Attention, Social Cognition and Autism.pdf:pdf},
journal = {Autism Research},
keywords = {early development,neural connectivity,social symptoms},
number = {1},
pages = {2--21},
title = {{A Parallel and Distributed-Processing Model of Joint Attention, Social Cognition and Autism}},
volume = {2},
year = {2009}
}
@article{Loughlin2000,
author = {Loughlin, Claire O and Thagard, Paul},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Loughlin, Thagard - 2000 - Autism and Coherence A Computational Model.pdf:pdf},
journal = {Mind {\&} Language},
number = {4},
pages = {375--392},
title = {{Autism and Coherence: A Computational Model}},
volume = {15},
year = {2000}
}
@article{Rajendran2007,
author = {Rajendran, Gnanathusharan and Mitchell, Peter},
doi = {10.1016/j.dr.2007.02.001},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rajendran, Mitchell - 2007 - Cognitive theories of autism.pdf:pdf},
journal = {Developmental Review},
keywords = {asperger syndrome,autism,autistic spectrum disorders,cognitive theories,considers these theories by,describing the research which,dominated psychological research into,gave life to them,introduction and the history,of cognitive theories of,studies which,this article,three cognitive theories have},
number = {2},
pages = {224--260},
title = {{Cognitive theories of autism}},
volume = {27},
year = {2007}
}
@article{Brunsdon2014,
author = {Brunsdon, Victoria E A and Happ{\'{e}}, Francesca},
doi = {10.1177/1362361313499456},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunsdon, Happ{\'{e}} - 2014 - Exploring the â€˜fractionation' of autism at the cognitive level.pdf:pdf},
isbn = {1362361313},
journal = {Autism},
keywords = {autism spectrum disorder,central coherence,cognitive theories,executive function,fractionable triad,theory of mind},
number = {1},
pages = {17--30},
title = {{Exploring the â€˜fractionation' of autism at the cognitive level}},
volume = {18},
year = {2014}
}
@article{Kalish2004,
author = {Kalish, Michael L and Lewandowsky, Stephan and Kruschke, John K},
doi = {10.1037/0033-295X.111.4.1072},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalish, Lewandowsky, Kruschke - 2004 - Population of Linear Experts Knowledge Partitioning and Function Learning.pdf:pdf},
number = {4},
pages = {1072--1099},
title = {{Population of Linear Experts : Knowledge Partitioning and Function Learning}},
volume = {111},
year = {2004}
}
@article{Frith1991,
author = {Frith, Uta and Morton, John and Leslie, Alan M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frith, Morton, Leslie - 1991 - The cognitive basis of a biological disorder autism.pdf:pdf},
journal = {Trends in Neurosciences},
number = {10},
pages = {433--438},
title = {{The cognitive basis of a biological disorder: autism}},
volume = {14},
year = {1991}
}
@article{Solstad2006,
author = {Solstad, Trygve and Moser, Edvard I and Einevoll, Gaute T},
doi = {10.1002/hipo},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Solstad, Moser, Einevoll - 2006 - From Grid Cells to Place Cells A Mathematical Model.pdf:pdf},
journal = {Hippocampus},
keywords = {1978,a,a widespread brain network,entorhinal cortex,for spatial representation and,grid cells,hippocampus,keefe and nadel,memory,navigation,o,place cells,spatial representation,the hippocampus is thought,to be part of},
pages = {1026--1031},
title = {{From Grid Cells to Place Cells: A Mathematical Model}},
volume = {1031},
year = {2006}
}
@article{Moser2008,
author = {Moser, Edvard I and Kropff, Emilio and Moser, May-britt},
doi = {10.1146/annurev.neuro.31.061307.090723},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moser, Kropff, Moser - 2008 - Place Cells, Grid Cells, and the Brain's Spatial Representation System.pdf:pdf},
journal = {Annual Review of Neuroscience},
keywords = {attractor,entorhinal cortex,hippocampus,memory,path integration},
pages = {69--89},
title = {{Place Cells, Grid Cells, and the Brain's Spatial Representation System}},
volume = {31},
year = {2008}
}
@article{Schulz,
author = {Schulz, Eric and Tenenbaum, Joshua B and Duvenaud, David and Speekenbrink, Maarten and Gershman, Samuel J.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulz et al. - 2016 - Compositional Inductive Biases in Function Learning.pdf:pdf},
journal = {bioRxiv},
title = {{Compositional Inductive Biases in Function Learning}},
year = {2016}
}
@article{Parthemore2010,
author = {Parthemore, Joel and Morse, Anthony F},
doi = {10.1075/p},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parthemore, Morse - 2010 - Representations reclaimed Accounting for the co-emergence of concepts and experience.pdf:pdf},
journal = {Pragmatics {\&} Cognition},
keywords = {concept,conceptual spaces,enaction,mental representation,representation,sensorimotor,sensorimotor profile,symbol},
number = {2},
pages = {273--312},
title = {{Representations reclaimed: Accounting for the co-emergence of concepts and experience}},
volume = {18},
year = {2010}
}
@book{Gardenfors2004,
author = {G{\"{a}}rdenfors, Peter},
booktitle = {Conceptual Spaces},
doi = {10.1007/978-1-4020-9877-2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/G{\"{a}}rdenfors - 2004 - Conceptual spaces.pdf:pdf},
isbn = {9781402098772},
title = {{Conceptual spaces}},
year = {2004}
}
@article{Dauphin2014a,
abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
archivePrefix = {arXiv},
arxivId = {1406.2572},
author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
eprint = {1406.2572},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem in high-dimensional non-convex optimization(2).pdf:pdf},
isbn = {1406.2572},
issn = {10495258},
journal = {Neural Information Processing Systems},
pages = {1--9},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
year = {2014}
}
@article{Baldi1989,
abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed. {\textcopyright} 1989.},
author = {Baldi, Pierre and Hornik, Kurt},
doi = {10.1016/0893-6080(89)90014-2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baldi, Hornik - 1989 - Neural networks and principal component analysis Learning from examples without local minima.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back propagation,Learning,Neural networks,Principal component analysis},
number = {1},
pages = {53--58},
title = {{Neural networks and principal component analysis: Learning from examples without local minima}},
volume = {2},
year = {1989}
}
@article{Fodor1988,
abstract = {This paper explores differences between Connectionist proposals for cognitive architecture and the sorts of models that have traditionally been assumed in cognitive science. We claim that the major distinction is that, while both Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a 'language of thought': i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the 'systematicity' of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or 'abstract neurological') structures in which Classical cognitive architecture is implemented. We survey a number of the standard arguments that have been offered in favor of Connectionism, and conclude that they are coherent only on this interpretation. ?? 1988.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fodor, Jerry A. and Pylyshyn, Zenon W.},
doi = {10.1016/0010-0277(88)90031-5},
eprint = {arXiv:1011.1669v3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fodor, Pylyshyn - 1988 - Connectionism and cognitive architecture A critical analysis.pdf:pdf},
isbn = {9788578110796},
issn = {00100277},
journal = {Cognition},
number = {1-2},
pages = {3--71},
pmid = {25246403},
title = {{Connectionism and cognitive architecture: A critical analysis}},
volume = {28},
year = {1988}
}
@article{Shenhav2013,
author = {Shenhav, Amitai and Botvinick, Matthew M and Cohen, Jonathan D},
doi = {10.1016/j.neuron.2013.07.007},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shenhav, Botvinick, Cohen - 2013 - The Expected Value of Control An Integrative Theory of Anterior Cingulate Cortex Function.pdf:pdf},
issn = {0896-6273},
journal = {Neuron},
number = {2},
pages = {217--240},
publisher = {Elsevier Inc.},
title = {{The Expected Value of Control: An Integrative Theory of Anterior Cingulate Cortex Function}},
volume = {79},
year = {2013}
}
@article{Shenhav2014,
abstract = {Nature Neuroscience, (2014). doi:10.1038/nn.3771},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Shenhav, Amitai and Straccia, Mark A and Cohen, Jonathan D and Botvinick, Matthew M},
doi = {10.1038/nn.3771},
eprint = {NIHMS150003},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shenhav et al. - 2014 - Anterior cingulate engagement in a foraging context reflects choice difficulty, not foraging value.pdf:pdf},
isbn = {6176321972},
issn = {1097-6256},
journal = {Nature Neuroscience},
number = {9},
pages = {1249--1254},
pmid = {25064851},
publisher = {Nature Publishing Group},
title = {{Anterior cingulate engagement in a foraging context reflects choice difficulty, not foraging value}},
volume = {17},
year = {2014}
}
@article{Melby-Lervag2013,
author = {Melby-Lerv{\aa}g, Monica and Hulme, Charles},
doi = {10.1037/a0028228},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Melby-Lerv{\aa}g, Hulme - 2013 - Is working memory training effective A meta-analytic review.pdf:pdf},
isbn = {1939-0599 (Electronic)0012-1649 (Linking)},
issn = {1939-0599},
journal = {Developmental Psychology},
keywords = {adhd,at least,attention,constructs in cognitive psychology,from links between measures,in part,learning disabilities,of the most influential,of working memory capacity,theoretical,this influence derives,working memory is one,working memory training},
number = {2},
pages = {270--291},
pmid = {22612437},
title = {{Is working memory training effective? A meta-analytic review.}},
volume = {49},
year = {2013}
}
@article{Gentner2016,
author = {Gentner, Dedre},
doi = {10.1037/amp0000082},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gentner - 2016 - Language as Cognitive Tool Kit How Language Supports Relational Thought.pdf:pdf},
issn = {0003066X},
journal = {American Psychologist},
keywords = {language and cognition,relational learning,relational meaning},
number = {8},
pages = {650--657},
title = {{Language as Cognitive Tool Kit : How Language Supports Relational Thought}},
volume = {71},
year = {2016}
}
@article{Forbus2016,
abstract = {Analogy and similarity are central phenomena in human cognition, involved in processes ranging from visual perception to conceptual change. To capture this centrality requires that a model of comparison must be able to integrate with other processes and handle the size and complexity of the representations required by the tasks being modeled. This paper describes extensions to Structure-Mapping Engine (SME) since its inception in 1986 that have increased its scope of operation. We first review the basic SME algorithm, describe psychological evidence for SME as a process model, and summarize its role in simulating similarity-based retrieval and generalization. Then we describe five techniques now incorporated into the SME that have enabled it to tackle large-scale modeling tasks: (a) Greedy merging rapidly constructs one or more best interpretations of a match in polynomial time: O(n2 log(n)); (b) Incremental operation enables mappings to be extended as new information is retrieved or derived about the base or target, to model situations where information in a task is updated over time; (c) Ubiquitous predicates model the varying degrees to which items may suggest alignment; (d) Structural evaluation of analogical inferences models aspects of plausibility judgments; (e) Match filters enable large-scale task models to communicate constraints to SME to influence the mapping process. We illustrate via examples from published studies how these enable it to capture a broader range of psychological phenomena than before.},
author = {Forbus, Kenneth D. and Ferguson, Ronald W. and Lovett, Andrew and Gentner, Dedre},
doi = {10.1111/cogs.12377},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Forbus et al. - 2016 - Extending SME to Handle Large-Scale Cognitive Modeling.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Analogical learning,Analogical reasoning,Analogy,Artificial intelligence,Cognitive psychology,Cognitive simulation,Similarity,Symbolic modeling},
number = {4},
pages = {1--109},
pmid = {27322750},
title = {{Extending SME to Handle Large-Scale Cognitive Modeling}},
year = {2016}
}
@article{Goodfellow2015,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow, Shlens, Szegedy - 2015 - Explaining and Harnessing Adversarial Examples.pdf:pdf},
isbn = {1412.6572},
journal = {Iclr 2015},
pages = {1--11},
pmid = {729514},
title = {{Explaining and Harnessing Adversarial Examples}},
year = {2015}
}
@article{Xu2017,
abstract = {Although deep neural networks (DNNs) have achieved great success in many computer vision tasks, recent studies have shown they are vulnerable to adversarial examples. Such examples, typically generated by adding small but purposeful distortions, can frequently fool DNN models. Previous studies to defend against adversarial examples mostly focused on refining the DNN models. They have either shown limited success or suffer from the expensive computation. We propose a new strategy, $\backslash$emph{\{}feature squeezing{\}}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on the squeezed input, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two instances of feature squeezing: reducing the color bit depth of each pixel and smoothing using a spatial filter. These strategies are straightforward, inexpensive, and complementary to defensive methods that operate on the underlying model, such as adversarial training.},
archivePrefix = {arXiv},
arxivId = {1704.01155},
author = {Xu, Weilin and Evans, David and Qi, Yanjun},
eprint = {1704.01155},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Evans, Qi - 2017 - Feature Squeezing Detecting Adversarial Examples in Deep Neural Networks.pdf:pdf},
title = {{Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks}},
year = {2017}
}
@article{Papernot2016,
abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95{\%} to less than 0.5{\%} on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10{\^{}}30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800{\%} on one of the DNNs we tested.},
archivePrefix = {arXiv},
arxivId = {1511.04508},
author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
doi = {10.1109/SP.2016.41},
eprint = {1511.04508},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks.pdf:pdf},
isbn = {9781509008247},
journal = {Proceedings - 2016 IEEE Symposium on Security and Privacy, SP 2016},
pages = {582--597},
pmid = {7546524},
title = {{Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks}},
year = {2016}
}
@article{Moosavi-Dezfooli2016,
abstract = {State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool framework to efficiently compute perturbations that fools deep network and thus reliably quantify the robustness of arbitrary classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust. To encourage reproducible research, the code of DeepFool will be available online.},
archivePrefix = {arXiv},
arxivId = {1511.04599},
author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
doi = {10.1109/CVPR.2016.282},
eprint = {1511.04599},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moosavi-Dezfooli, Fawzi, Frossard - 2016 - DeepFool a simple and accurate method to fool deep neural networks.pdf:pdf},
isbn = {9781467388511},
issn = {10636919},
journal = {Cvpr},
pages = {2574--2582},
title = {{DeepFool: a simple and accurate method to fool deep neural networks}},
year = {2016}
}
@article{Carlini2016,
abstract = {We show that defensive distillation is not secure: it is no more resistant to targeted misclassification attacks than unprotected neural networks.},
archivePrefix = {arXiv},
arxivId = {1607.04311},
author = {Carlini, Nicholas and Wagner, David},
eprint = {1607.04311},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlini, Wagner - 2016 - Defensive Distillation is Not Robust to Adversarial Examples.pdf:pdf},
journal = {Arxiv},
pages = {1--3},
title = {{Defensive Distillation is Not Robust to Adversarial Examples}},
volume = {0},
year = {2016}
}
@article{Papernot2016a,
abstract = {Deep Learning is increasingly used in several machine learning tasks as Deep Neural Networks (DNNs) frequently outperform other techniques. Yet, previous work showed that, once deployed, DNNs are vulnerable to integrity attacks. Indeed, adversaries can control DNN outputs and for instance force them to misclassify inputs by adding a carefully crafted and undistinguishable perturbation. However, these attacks assumed knowledge of the targeted DNN's architecture and parameters. In this paper however, we release these assumptions and introduce an attack conducted under the more realistic, yet more complex, threat model of an oracle: adversaries are only capable of accessing DNN label predictions for chosen inputs. We evaluate our attack in real-world settings by successfully forcing an oracle served by MetaMind, an online API for DNN classifiers, to misclassify inputs at a 84.24{\%} rate. We also perform an evaluation to fine-tune our attack strategy and maximize the oracle's misclassification rate for adversarial samples.},
archivePrefix = {arXiv},
arxivId = {1602.02697},
author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
eprint = {1602.02697},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papernot et al. - 2016 - Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples.pdf:pdf},
isbn = {9781450349444},
journal = {arXiv},
title = {{Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples}},
year = {2016}
}
@inproceedings{Lampinen2017,
author = {Lampinen, Andrew and Hsu, Shaw and Mcclelland, James L},
booktitle = {Proceedings of the Cognitive Science Society 2017},
keywords = {analogy,neural networks,representa-,structure learning,tion,transfer},
title = {{Analogies Emerge from Learning Dyamics in Neural Networks}},
year = {2017}
}
@article{Levy2014,
abstract = {Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.'s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations.},
author = {Levy, Omer and Goldberg, Yoav},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy, Goldberg - 2014 - Linguistic regularities in sparse and explicit word representations.pdf:pdf},
isbn = {9781941643020},
journal = {CoNLL},
pages = {171--180},
title = {{Linguistic regularities in sparse and explicit word representations}},
year = {2014}
}
@inproceedings{Drozd2016,
author = {Drozd, Aleksandr and Gladkova, Anna and Matsuoka, Satoshi},
booktitle = {Proceedings of COLING 2016},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Drozd, Gladkova, Matsuoka - 2016 - Word Embeddings, Analogies, and Machine Learning Beyond King - M an W oman = Queen.pdf:pdf},
pages = {3519--3530},
title = {{Word Embeddings, Analogies, and Machine Learning: Beyond King - M an + W oman = Queen}},
year = {2016}
}
@article{Evans2013,
abstract = {Dual-process and dual-system theories in both cognitive and social psychology have been subjected to a number of recently published criticisms. However, they have been attacked as a category, incorrectly assuming there is a generic version that applies to all. We identify and respond to 5 main lines of argument made by such critics. We agree that some of these arguments have force against some of the theories in the literature but believe them to be overstated. We argue that the dual-processing distinction is supported by much recent evidence in cognitive science. Our preferred theoretical approach is one in which rapid autonomous processes (Type 1) are assumed to yield default responses unless intervened on by distinctive higher order reasoning processes (Type 2). What defines the difference is that Type 2 processing supports hypothetical thinking and load heavily on working memory.},
author = {Evans, Jonathan and Stanovich, K E},
doi = {10.1177/1745691612460685},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans, Stanovich - 2013 - Dual-process theories of higher cognition Advancing the debate.pdf:pdf},
isbn = {1745-6916$\backslash$n1745-6924},
issn = {1745-6916},
journal = {Perspectives on Psychological Science},
keywords = {Dual process theory,Dual systems,Individual differences,Rationality,Working memory},
number = {3},
pages = {223--241},
pmid = {26172965},
title = {{Dual-process theories of higher cognition: Advancing the debate}},
volume = {8},
year = {2013}
}
@article{Rogers2008a,
author = {Rogers, Timothy T and McClelland, James L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rogers, McClelland - 2008 - A simple model from a powerful framework that spans levels of analysis.pdf:pdf},
journal = {Behavioral and Brain Sciences},
pages = {729--750},
title = {{A simple model from a powerful framework that spans levels of analysis}},
volume = {31},
year = {2008}
}
@article{Reed2016,
author = {Reed, Stephen K},
doi = {10.1177/1745691616646304},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reed - 2016 - A Taxonomic Analysis of Abstraction.pdf:pdf},
journal = {Perspectives on Psychological Science},
keywords = {a vivid illustration of,abstraction,attributes,by an engineer describing,categories,hierarchies,his,his weekly meeting with,instances,is revealed,the power of abstraction},
number = {1968},
title = {{A Taxonomic Analysis of Abstraction}},
year = {2016}
}
@article{Evans2003,
abstract = {Researchers in thinking and reasoning have proposed recently that there are two distinct cognitive systems underlying reasoning. System 1 is old in evolutionary terms and shared with other animals: it comprises a set of autonomous subsystems that include both innate input modules and domain-specific knowledge acquired by a domain-general learning mechanism. System 2 is evolutionarily recent and distinctively human: it permits abstract reasoning and hypothetical thinking, but is constrained by working memory capacity and correlated with measures of general intelligence. These theories essentially posit two minds in one brain with a range of experimental psychological evidence showing that the two systems compete for control of our inferences and actions.},
author = {Evans, Jonathan St B T},
doi = {10.1016/j.tics.2003.08.012},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans - 2003 - In two minds Dual-process accounts of reasoning.pdf:pdf},
isbn = {13646613},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {10},
pages = {454--459},
pmid = {14550493},
publisher = {Elsevier Ltd},
title = {{In two minds: Dual-process accounts of reasoning}},
volume = {7},
year = {2003}
}
@article{He2014,
author = {He, Kaiming},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He - 2014 - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
title = {{Delving Deep into Rectifiers : Surpassing Human-Level Performance on ImageNet Classification}},
year = {2014}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ï¬rst observe the inï¬‚uence of the non-linear activations functions. We ï¬nd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ï¬nd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ï¬nd that a new non-linearity that saturates less can often be beneï¬cial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difï¬cult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bengio - 2010 - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
isbn = {9781937284275},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
volume = {9},
year = {2010}
}
@inproceedings{Emruli2013,
abstract = {Analogy-making is a key function of human cognition. Therefore, the development of computational models of analogy that automatically learn from examples can lead to significant advances in cognitive systems. Analogies require complex, relational representations of learned structures, which is challenging for both symbolic and neurally inspired models. Vector symbolic architectures (VSAs) are a class of connectionist models for the representation and manipulation of compositional structures, which can be used to model analogy. We study a novel VSA network for the analogical mapping of compositional structures, which integrates an associative memory known as sparse distributed memory (SDM). The SDM enables non-commutative binding of compositional structures, which makes it possible to predict novel patterns in sequences. To demonstrate this property we apply the network to a commonly used intelligence test called Raven's Progressive Matrices. We present results of simulation experiments for the Raven's task and calculate the probability of prediction error at 95{\%} confidence level. We find that non-commutative binding requires sparse activation of the SDM and that 10â€“20{\%} concept-specific activation of neurons is optimal. The optimal dimensionality of the binary distributed representations of the VSA is of the order 10{\^{}}4, which is comparable with former results and the average synapse count of neurons in the cerebral cortex.},
author = {Emruli, Blerim and Gayler, Ross W and Sandin, Fredrik},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2013.6706829},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Emruli, Gayler, Sandin - 2013 - Analogical mapping and inference with binary spatter codes and sparse distributed memory.pdf:pdf},
isbn = {9781467361293},
title = {{Analogical mapping and inference with binary spatter codes and sparse distributed memory}},
year = {2013}
}
@article{Lu2012,
author = {Lu, Hongjing and Chen, Dawn and Holyoak, Keith J and Anderson, N John and Doumas, Alex and Hummel, John},
doi = {10.1037/a0028719},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2012 - Bayesian Analogy With Relational Transformations.pdf:pdf},
keywords = {10,1037,a0028719,analogy,bayesian models,doi,dx,generalization,http,org,relation learning,supp,supplemental materials},
number = {3},
pages = {617--648},
title = {{Bayesian Analogy With Relational Transformations}},
volume = {119},
year = {2012}
}
@article{Penn2008,
abstract = {Over the last quarter century, the dominant tendency in comparative cognitive psychology has been to emphasize the similarities between human and nonhuman minds and to downplay the differences as "one of degree and not of kind" (Darwin 1871). In the present target article, we argue that Darwin was mistaken: the profound biological continuity between human and nonhuman animals masks an equally profound discontinuity between human and nonhuman minds. To wit, there is a significant discontinuity in the degree to which human and nonhuman animals are able to approximate the higher-order, systematic, relational capabilities of a physical symbol system (PSS) (Newell 1980). We show that this symbolic-relational discontinuity pervades nearly every domain of cognition and runs much deeper than even the spectacular scaffolding provided by language or culture alone can explain. We propose a representational-level specification as to where human and nonhuman animals' abilities to approximate a PSS are similar and where they differ. We conclude by suggesting that recent symbolic-connectionist models of cognition shed new light on the mechanisms that underlie the gap between human and nonhuman minds.},
author = {Penn, Derek C and Holyoak, Keith J and Povinelli, Daniel J},
doi = {10.1017/S0140525X08003543},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Penn, Holyoak, Povinelli - 2008 - Darwin's mistake explaining the discontinuity between human and nonhuman minds.pdf:pdf},
isbn = {0140-525X},
issn = {1469-1825},
journal = {The Behavioral and brain sciences},
keywords = {Animals,Brain,Brain: physiology,Cognition,Cognition: physiology,Evolution,Humans,Pan troglodytes,Pan troglodytes: physiology,Perception,Space Perception,Species Specificity,Symbolism,Wild,Wild: physiology},
number = {2},
pages = {109--30; discussion 130--178},
pmid = {18479531},
title = {{Darwin's mistake: explaining the discontinuity between human and nonhuman minds.}},
volume = {31},
year = {2008}
}
@incollection{Wilson2001,
author = {Wilson, William H and Halford, Graeme S and Gray, Brett and Phillips, Steven},
booktitle = {The analogical mind},
editor = {Gentner, Dedre and Holyoak, Keith J. and Kokinov, B. N.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson et al. - 2001 - The STAR-2 Model for Mapping Hierarchically Structure Analogs.pdf:pdf},
pages = {125--60},
title = {{The STAR-2 Model for Mapping Hierarchically Structure Analogs}},
year = {2001}
}
@article{Gentner2010,
author = {Gentner, Dedre},
doi = {10.1111/j.1551-6709.2010.01114.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gentner - 2010 - Bootstrapping the Mind Analogical Processes and Symbol Systems.pdf:pdf},
keywords = {analogical learning,cognitive development,language and cogni-,structure-mapping},
pages = {752--775},
title = {{Bootstrapping the Mind : Analogical Processes and Symbol Systems}},
volume = {34},
year = {2010}
}
@article{Hummel1997,
abstract = {This article describes an integrated theory of analogical access and mapping, instantiated in a computational model called LISA (Learning and Inference with Schemas and Analogies). LISA represents predicates and objects as distributed patterns of activation that are dynamically bound into propositional structures, thereby achieving both the flexibility of a connectionist system and the structure sensitivity of a symbolic system. The model treats access and mapping as types of guided pattern classification, differing only in that mapping is augmented by a capacity to learn new correspondences. The resulting model simulates a wide range of empirical findings concerning human analogical access and mapping. LISA also has a number of inherent limitations, including capacity limits, that arise in human reasoning and suggests a specific computational account of these limitations. Extensions of this approach also account for analogical inference and schema induction.},
archivePrefix = {arXiv},
arxivId = {1112.4045},
author = {Hummel, John E. and Holyoak, Keith J.},
doi = {10.1037/0033-295X.104.3.427},
eprint = {1112.4045},
isbn = {0033-295X$\backslash$r1939-1471},
issn = {0033295X},
journal = {Psychological Review},
number = {3},
pages = {427--466},
pmid = {1000104248},
title = {{Distributed representations of structure: A theory of analogical access and mapping.}},
volume = {104},
year = {1997}
}
@article{Holyoak1989,
author = {Holyoak, Keith J},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holyoak - 1989 - Analogical Mapping by Constraint Satisfaction.pdf:pdf},
title = {{Analogical Mapping by Constraint Satisfaction}},
volume = {5},
year = {1989}
}
@article{Anderson1972,
author = {Anderson, P W},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anderson - 1972 - More Is Different.pdf:pdf},
journal = {Science},
number = {4047},
pages = {393--396},
title = {{More Is Different}},
volume = {177},
year = {1972}
}
@misc{Rust2014,
abstract = {(from the chapter) Many of our everyday perceptual and cognitive tasks require our brains to transform information from implicit representations in which task-relevant information exists but in a format that is difficult to extract, into explicit representations in which this information is accessible. For example, determining the identities of objects that are currently in view across naturally occurring variation, such as changes in an object's position, requires our brains to reformat the pattern of light-based representations encoded by our photoreceptors into representations that explicitly reflect object identity. The brain faces similar challenges for other perceptual tasks, such as identifying words spoken by different voices, as well as more cognitive challenges, such as determining whether a chair belongs to the category of furniture. Insight into these challenges, and the brain's solutions, can be understood using geometrical, population-based coding approaches. Once formulated, these population-based descriptions can be linked to the single- and multi-neuron mechanisms that support a successful task solution as well as provide important insights into the computations that the brain uses to process information. (PsycINFO Database Record (c) 2015 APA, all rights reserved)},
author = {Rust, Nicole C},
booktitle = {The cognitive neurosciences (5th ed.).},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rust - 2014 - Population-based representations From implicit to explicit.pdf:pdf},
isbn = {978-0-262-02777-9 (Hardcover)},
keywords = {*Age Differences,*Brain,*Cognitive Processes,*Neurons,*Task Analysis},
pages = {337--348},
title = {{Population-based representations: From implicit to explicit.}},
year = {2014}
}
@article{Waltz1985,
abstract = {This is a description of research in developing a natural language processing system with modular knowledge sources but strongly interactive processing. The system offers insights into a variety of linguistic phenomena and allows easy testing of a variety of hypotheses. Language interpretation takes place on a activation network which is dynamically created from input, recent context, and long-term knowledge. Initially ambiguous and unstable, the network settles on a single interpretation, using a parallel, analog relaxation process. We also describe a parallel model for the representation of context and of the priming of concepts. Examples illustrating contextual influence on meaning interpretation and "semantic garden path" sentence processing, among other issues, are included. ?? 1985.},
author = {Waltz, David L. and Pollack, Jordan B.},
doi = {10.1016/S0364-0213(85)80009-4},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Waltz, Pollack - 1985 - Massively parallel parsing A strongly interactive model of natural language interpretation.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
number = {1},
pages = {51--74},
title = {{Massively parallel parsing: A strongly interactive model of natural language interpretation}},
volume = {9},
year = {1985}
}
@article{Ha,
archivePrefix = {arXiv},
arxivId = {arXiv:1704.03477v1},
author = {Ha, David and Eck, Douglas},
eprint = {arXiv:1704.03477v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ha, Eck - 2017 - A Neural Representation of Sketch Drawings.pdf:pdf},
journal = {arXiv},
pages = {1--20},
title = {{A Neural Representation of Sketch Drawings}},
year = {2017}
}
@article{Mcshane2017,
abstract = {A typical behavioral research paper features multiple studies of a common phenomenon that are analyzed solely in isolation. Because the studies are of a common phenomenon, this practice is inefficient and foregoes important benefits that be obtained only by analyzing them jointly in a single paper meta-analysis (SPM). To facilitate SPM, we introduce metaanalytic methodology that is user-friendly, widely applicable, and specially tailored to the SPM of the set of studies that appear in a typical behavioral research paper. Our SPM methodology provides important benefits for study summary, theory-testing, and replicability that we illustrate via three case studies that include papers recently published in the Journal of Consumer Research and the Journal of Marketing Research. We advocate that authors of typical behavioral research papers use it to supplement the single-study analyses that independently discuss the multiple studies in the body of their papers as well as the "qualitative meta-analysis" that verbally synthesizes the studies in the general discussion of their papers. When used as such, this requires only a minor modification of current practice. We provide an easy-to-use website that implements our SPM methodology.},
author = {Mcshane, Blakeley B. and B{\"{o}}ckenholt, Ulf},
doi = {10.1093/jcr/ucw085},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcshane, B{\"{o}}ckenholt - 2017 - Single paper meta-analysis Benefits for study summary, theory-testing, and replicability.pdf:pdf},
issn = {0093-5301},
journal = {Journal of Consumer Research},
keywords = {between-study variation,heterogeneity,hierarchical,meta-analysis,multilevel,random effects},
pages = {ucw085},
title = {{Single paper meta-analysis: Benefits for study summary, theory-testing, and replicability}},
volume = {43},
year = {2017}
}
@article{Vezhnevets2016a,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.01161v2},
author = {Vezhnevets, Alexander Sasha and Com, Korayk Google},
eprint = {arXiv:1703.01161v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vezhnevets, Com - 2016 - FeUdal Networks for Hierarchical Reinforcement Learning arXiv 1703 . 01161v2 cs . AI 6 Mar 2017.pdf:pdf},
title = {{FeUdal Networks for Hierarchical Reinforcement Learning}},
year = {2016}
}
@article{Strub2017,
abstract = {End-to-end design of dialogue systems has recently become a popular research topic thanks to powerful tools such as encoder-decoder architectures for sequence-to-sequence learning. Yet, most current approaches cast human-machine dialogue management as a supervised learning problem, aiming at predicting the next utterance of a participant given the full history of the dialogue. This vision is too simplistic to render the intrinsic planning problem inherent to dialogue as well as its grounded nature, making the context of a dialogue larger than the sole history. This is why only chit-chat and question answering tasks have been addressed so far using end-to-end architectures. In this paper, we introduce a Deep Reinforcement Learning method to optimize visually grounded task-oriented dialogues, based on the policy gradient algorithm. This approach is tested on a dataset of 120k dialogues collected through Mechanical Turk and provides encouraging results at solving both the problem of generating natural dialogues and the task of discovering a specific object in a complex picture.},
archivePrefix = {arXiv},
arxivId = {1703.05423},
author = {Strub, Florian and de Vries, Harm and Mary, Jeremie and Piot, Bilal and Courville, Aaron and Pietquin, Olivier},
eprint = {1703.05423},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Strub et al. - 2017 - End-to-end optimization of goal-driven and visually grounded dialogue systems.pdf:pdf},
journal = {arXiv preprint},
title = {{End-to-end optimization of goal-driven and visually grounded dialogue systems}},
year = {2017}
}
@article{DeVries2016,
abstract = {We introduce GuessWhat?!, a two-player guessing game as a testbed for research on the interplay of computer vision and dialogue systems. The goal of the game is to locate an unknown object in a rich image scene by asking a sequence of questions. Higher-level image understanding, like spatial reasoning and language grounding, is required to solve the proposed task. Our key contribution is the collection of a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images. We explain our design decisions in collecting the dataset and introduce the oracle and questioner tasks that are associated with the two players of the game. We prototyped deep learning models to establish initial baselines of the introduced tasks.},
archivePrefix = {arXiv},
arxivId = {1611.08481},
author = {de Vries, Harm and Strub, Florian and Chandar, Sarath and Pietquin, Olivier and Larochelle, Hugo and Courville, Aaron},
eprint = {1611.08481},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de Vries et al. - 2016 - GuessWhat! Visual object discovery through multi-modal dialogue.pdf:pdf},
journal = {arXiv preprint},
pages = {23},
title = {{GuessWhat?! Visual object discovery through multi-modal dialogue}},
year = {2016}
}
@incollection{Wilensky1991,
address = {Westport, CT, US},
author = {Wilensky, Uri},
booktitle = {Constructionism},
editor = {Harel, Idit and Papert, Seymour},
publisher = {Ablex Publishing},
title = {{Abstract Meditations on the Concrete and Concrete Implications for Mathematics Education}},
year = {1991}
}
@article{Schwarz1978,
author = {Schwarz, Gideon E.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwarz - 1978 - Estimating the Dimension of a Model.pdf:pdf},
journal = {Annals of Statistics},
number = {2},
title = {{Estimating the Dimension of a Model}},
volume = {6},
year = {1978}
}
@article{Simmons2011,
author = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
doi = {10.1177/0956797611417632},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simmons, Nelson, Simonsohn - 2011 - False-Positive Psychology Undisclosed Flexibility in Data Collection and Analysis Allows Presenting.pdf:pdf},
journal = {Psychological Science},
keywords = {11,17,23,about the world,disclosure,is to discover truths,methodology,motivated reasoning,our job as scientists,publication,received 3,revision accepted 5,we},
number = {11},
pages = {1359--1366},
title = {{False-Positive Psychology : Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant}},
volume = {22},
year = {2011}
}
@article{Gong1986,
author = {Gong, Gail},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gong - 1986 - Cross-Validation, the Jackknife, and the Bootstrap Excess Error Estimation in Forward Logistic Regression.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {383},
pages = {108--113},
title = {{Cross-Validation, the Jackknife, and the Bootstrap: Excess Error Estimation in Forward Logistic Regression}},
volume = {81},
year = {1986}
}
@book{Wasserman2006,
author = {Wasserman, Larry},
isbn = {0-387-25145-6},
publisher = {Springer Texts in Statistics},
title = {{All of Nonparametric Statistics}},
year = {2006}
}
@article{Mnih2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.01783v2},
author = {Mnih, Volodymyr and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David},
eprint = {arXiv:1602.01783v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
journal = {International Conference on Machine Learning},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
year = {2016}
}
@article{Gelman2006,
author = {Gelman, Andrew},
doi = {10.1198/004017005000000661},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman - 2006 - Multilevel (Hierarchical) Modeling What It Can and Cannot Do.pdf:pdf},
journal = {Technometrics},
keywords = {contextual effects,hierarchical model,multilevel regression},
number = {3},
pages = {432--435},
title = {{Multilevel (Hierarchical) Modeling: What It Can and Cannot Do}},
volume = {48},
year = {2006}
}
@article{Krogh1991,
author = {Krogh, Anders},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krogh - 1991 - A Simple Weight Decay Can Improve Generalization.pdf:pdf},
journal = {Advances in Neural Information Processing Systems(Proceedings of NIPS)},
pages = {950--957},
title = {{A Simple Weight Decay Can Improve Generalization}},
volume = {4},
year = {1991}
}
@article{Long2016,
author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Long et al. - 2016 - Unsupervised Domain Adaptation with Residual Transfer Networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Unsupervised Domain Adaptation with Residual Transfer Networks}},
year = {2016}
}
@article{Vezhnevets2016,
author = {Vezhnevets, Alexander Sasha and Mnih, Volodymyr and Agapiou, John and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Kavukcuoglu, Koray},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vezhnevets et al. - 2016 - Strategic Attentive Writer for Learning Macro-Actions.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Strategic Attentive Writer for Learning Macro-Actions}},
year = {2016}
}
@article{Day2015,
author = {Day, Samuel B and Motz, Benjamin A and Goldstone, Robert L and Bastian, Claudia C Von and Day, Samuel B},
doi = {10.3389/fpsyg.2015.01876},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Day et al. - 2015 - The Cognitive Costs of Context The Effects of Concreteness and Immersiveness in Instructional.pdf:pdf},
journal = {Frontiers in Psychology},
keywords = {analogical reasoning,analogical transfer,cognition,concreteness,context,learning,transfer},
number = {December},
pages = {1--13},
title = {{The Cognitive Costs of Context : The Effects of Concreteness and Immersiveness in Instructional}},
volume = {6},
year = {2015}
}
@article{Lobato2012,
author = {Lobato, Joanne},
doi = {10.1080/00461520.2012.693353},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lobato - 2012 - The Actor-Oriented Transfer Perspective and Its Contributions to Educational Research and Practice.pdf:pdf},
journal = {Educational Psychologist},
number = {3},
pages = {232--247},
title = {{The Actor-Oriented Transfer Perspective and Its Contributions to Educational Research and Practice}},
volume = {47},
year = {2012}
}
@incollection{Bisanz1992,
author = {Bisanz, Jeffrey and LeFevre, Jo-Anne},
booktitle = {The Nature and Origins of Mathematical Skills},
pages = {113--136},
title = {{Understanding Elementary Mathematics}},
year = {1992}
}
@article{Goldstone2005,
author = {Goldstone, Robert L and Son, Ji Y},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldstone, Son - 2005 - The Transfer of Scientific Principles Using Concrete and Idealized Simulations.pdf:pdf},
journal = {The Journal of the Learning Sciences},
number = {1},
pages = {69--110},
title = {{The Transfer of Scientific Principles Using Concrete and Idealized Simulations}},
volume = {14},
year = {2005}
}
@article{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
eprint = {1411.1792},
file = {:home/andrew/Documents/grad/Papers/5347-how-transferable-are-features-in-deep-neural-networks.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27 (Proceedings of NIPS)},
title = {{How transferable are features in deep neural networks?}},
volume = {27},
year = {2014}
}
@article{Tunney2001,
abstract = {Participants can transfer grammatical knowledge acquired implicitly in 1 vocabulary to new sequences instantiated in both the same and a novel vocabulary. Two principal theories have been advanced to account for these effects. One suggests that sequential dependencies form the basis for cross-domain transfer (e.g., Z. Dienes, G. T. M. Altmann, {\&} S. J. Gao, 1999). Another argues that a form of episodic memory known as abstract analogy is sufficient (e.g., L. R. Brooks {\&} J. R. Vokey, 1991). Three experiments reveal the contributions of the 2. In Experiment 1 sequential dependencies form the only basis for transfer. Experiment 2 demonstrates that this process is impaired by a change in the distributional properties of the language. Experiment 3 demonstrates that abstract analogy of repetition structure is relatively immune to such a change. These findings inform theories of artificial grammar learning and the transfer of grammatical knowledge.},
author = {Tunney, Richard J. and Altmann, Gerry T.M.},
doi = {10.1037/0278-7393.27.3.614},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tunney, Altmann - 2001 - Two modes of transfer in artificial grammar learning.pdf:pdf},
isbn = {0278-7393},
issn = {02787393},
journal = {Journal of Experimental Psychology: Learning Memory and Cognition},
keywords = {Adult,Female,Humans,Learning,Linguistics,Male,Models,Psychological,Serial Learning,Transfer (Psychology),Vocabulary},
number = {3},
pages = {614--639},
pmid = {11394670},
title = {{Two Modes of Transfer in Artificial Grammar Learning}},
volume = {27},
year = {2001}
}
@article{Dienes1999,
abstract = {This paper shows how a neural network can model the way people who have acquired knowledge of an artificial grammar in one perceptual domain (e.g., sequences of tones differing in pitch) can apply the knowledge to a quite different perceptual domain (e.g., sequences of letters). it is shown that a version of the Simple Recurrent Network (SRN) can transfer its knowledge of artificial grammars across domains without feedback. The performance of the model is sensitive to at least some of the same variables that affect subjects' performance-for example, the model is responsive to both the grammaticality of test sequences and their similarity to training sequences, to the cover task used during training, and to whether training is on bigrams or larger sequences.},
author = {Dienes, Zolt{\'{a}}n and Altmann, Gerry T. M. and Gao, Shi-Ji},
doi = {10.1207/s15516709cog2301},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dienes, Altmann, Gao - 1999 - Mapping across Domains Without Feedback A Neural Network Model of Transfer of Implicit Knowledge.pdf:pdf},
issn = {0364-0213},
journal = {Cognitive Science},
number = {1},
pages = {53--82},
title = {{Mapping across Domains Without Feedback: A Neural Network Model of Transfer of Implicit Knowledge}},
volume = {23},
year = {1999}
}
@article{Kuhlmann2004,
author = {Kuhlmann, Gregory and Stone, Peter and Mooney, Raymond and Shavlik, Jude},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuhlmann et al. - 2004 - Guiding a Reinforcement Learner with Natural Language Advice Initial Results in RoboCup Soccer.pdf:pdf},
number = {July},
pages = {30--35},
title = {{Guiding a Reinforcement Learner with Natural Language Advice : Initial Results in RoboCup Soccer}},
year = {2004}
}
@article{McClelland1999,
author = {McClelland, James L and Plaut, David C},
doi = {10.1016/S1364-6613(99)01320-0},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McClelland, Plaut - 1999 - Does generalization in infant learning implicate abstract algebra-like rules.pdf:pdf},
isbn = {1879-307X (Electronic)$\backslash$r1364-6613 (Linking)},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {5},
pages = {166--168},
pmid = {10322471},
title = {{Does generalization in infant learning implicate abstract algebra-like rules?}},
volume = {3},
year = {1999}
}
@article{Sristava2014,
author = {Sristava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sristava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Hansen2017,
author = {Hansen, Steven S. and Lampinen, Andrew and Suri, Gaurav and McClelland, James L.},
journal = {Behavioral and Brain Sciences},
title = {{Building on prior knowledge without building it in}},
volume = {40},
year = {2017}
}
@article{Shwartz-Ziv2017,
abstract = {Despite their great success, there is still no com-prehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their in-ner organization. Previous work [Tishby {\&} Za-slavsky (2015)] proposed to analyze DNNs in the Information Plane; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the In-formation Bottleneck (IB) tradeoff between com-pression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. We first show that the stochastic gradient descent (SGD) epochs have two distinct phases: fast empirical error minimization followed by slow representation compression, for each layer. We then argue that the DNN layers end up very close to the IB theo-retical bound, and present a new theoretical argu-ment for the computational benefit of the hidden layers.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1703.00810},
author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
eprint = {1703.00810},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shwartz-Ziv, Tishby - 1999 - Opening the Black Box of Deep Neural Networks via Information.pdf:pdf},
journal = {arXiv},
title = {{Opening the Black Box of Deep Neural Networks via Information}},
year = {2017}
}
@article{Fernando2017,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1701.08734},
author = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
eprint = {1701.08734},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fernando et al. - 2017 - PathNet Evolution Channels Gradient Descent in Super Neural Networks.pdf:pdf},
journal = {arXiv},
keywords = {basal ganglia,continual learning,evolution and,giant networks,learning,multitask,path evolution algorithm,transfer learning},
title = {{PathNet: Evolution Channels Gradient Descent in Super Neural Networks}},
year = {2017}
}
@article{Palmer2013,
abstract = {Guiding behavior requires the brain to make predictions about future sensory inputs. Here we show that efficient predictive computation starts at the earliest stages of the visual system. We estimate how much information groups of retinal ganglion cells carry about the future state of their visual inputs, and show that every cell we can observe participates in a group of cells for which this predictive information is close to the physical limit set by the statistical structure of the inputs themselves. Groups of cells in the retina also carry information about the future state of their own activity, and we show that this information can be compressed further and encoded by downstream predictor neurons, which then exhibit interesting feature selectivity. Efficient representation of predictive information is a candidate principle that can be applied at each stage of neural computation.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1307.0225},
author = {Palmer, Stephanie E. and Marre, Olivier and Berry, Michael J. and Bialek, William},
doi = {10.1073/pnas.0709640104},
eprint = {1307.0225},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Palmer et al. - 2013 - Predictive information in a sensory population.pdf:pdf},
isbn = {1091-6490 (Electronic)$\backslash$n0027-8424 (Linking)},
issn = {1091-6490},
journal = {arXiv preprint arXiv: {\ldots}},
number = {1},
pages = {1--11},
pmid = {18056803},
title = {{Predictive information in a sensory population}},
year = {2013}
}
@article{Alemi2016,
abstract = {We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method "Deep Variational Information Bottleneck", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1612.00410},
author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
eprint = {1612.00410},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alemi et al. - 2016 - Deep Variational Information Bottleneck.pdf:pdf},
pages = {13},
title = {{Deep Variational Information Bottleneck}},
year = {2016}
}
@article{Aitchison2015,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.04544v2},
author = {Aitchison, Laurence and Latham, Peter E},
eprint = {arXiv:1505.04544v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aitchison, Latham - 2015 - Synaptic sampling A connection between PSP variability and uncertainty explains neurophysiological observati.pdf:pdf},
journal = {arXiv},
pages = {1--11},
title = {{Synaptic sampling : A connection between PSP variability and uncertainty explains neurophysiological observations}},
year = {2015}
}
@article{Kirkpatrick2016,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1612.00796},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
eprint = {1612.00796},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kirkpatrick et al. - 2016 - Overcoming catastrophic forgetting in neural networks.pdf:pdf},
journal = {arXiv preprint},
title = {{Overcoming catastrophic forgetting in neural networks}},
year = {2016}
}
@book{Hiebert1997,
annote = {NULL},
author = {Hiebert, James and Carpenter, Thomas P. and Fennena, Elizabeth and Fuson, Karen C. and Wearne, Diana and Murray, Hanlie and Olivier, Alwyn and Human, Piet and Lindquist, Mary M.},
title = {{Making Sense: Teaching and learning mathematics with understanding}},
year = {1997}
}
@article{Goodman2013,
annote = {NULL},
author = {Goodman, Noah D},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodman - 2013 - Learning Stochastic Inverses Â¨.pdf:pdf},
pages = {1--9},
title = {{Learning Stochastic Inverses Â¨}},
year = {2013}
}
@article{Rezende2014,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.4082v3},
author = {Rezende, Danilo J and Mohamed, Shakir and Wierstra, Daan},
eprint = {arXiv:1401.4082v3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rezende, Mohamed, Wierstra - 2014 - Stochastic Backpropagation and Approximate Inference.pdf:pdf},
journal = {arXiv preprint},
title = {{Stochastic Backpropagation and Approximate Inference}},
year = {2014}
}
@incollection{Detterman1993,
annote = {"First, significant transfer is probably rare and accounts for very little human behavior. Studies that claim transfer often tell subjects to transfer, or use a `trick' to call the subjects attention to the similarity of the two problems. Such studies cannot be taken as evidence for transfer. We generally do what we have learned to do and no more. The lesson learned from studies of transfer is that, if you want people to learn something, teach it to them. Don't teach them something else and expect them to figure out what you really want them to do."

On studies showing successful transfer "The experimeter's manipulations have all the subtlety of the famer's baseball bat."},
author = {Detterman, Douglas K.},
booktitle = {Transfer on Trial: Intelligence, Cognition, and Instruction},
pages = {1--24},
title = {{The Case for the Prosecution: Transfer as an Epiphenomenon}},
year = {1993}
}
@article{Kramer1991,
annote = {NULL},
author = {Kramer, M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kramer - 1991 - Nonlinear principal component analysis using autoassociative neural networks.pdf:pdf},
journal = {AIChE journal},
keywords = {{\&}abstraction {\&}accounted {\&}application {\&}architecture},
number = {2},
pages = {233--243},
title = {{Nonlinear principal component analysis using autoassociative neural networks}},
volume = {37},
year = {1991}
}
@article{Mao1995,
abstract = {A number of networks and learning algorithms which provide$\backslash$nnew or alternative tools for feature extraction and data$\backslash$nprojection is proposed. The networks include a network$\backslash$n(SAMANN) for Sammon's nonlinear projection, a linear$\backslash$ndiscriminant analysis (LDA) network, a nonlinear$\backslash$ndiscriminant analysis (NDA) network, and a network for$\backslash$nnonlinear projection (NP-SOM) based on Kohonen's$\backslash$nself-organizing map. Five representative neural networks$\backslash$nfor feature extraction and data projection based on a$\backslash$nvisual judgement of two-dimensional projection maps and$\backslash$nquantitative criteria on data sets with various properties$\backslash$nare evaluated.},
annote = {NULL},
author = {Mao, Jianchang and Jain, Anil K.},
doi = {10.1109/72.363467},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao, Jain - 1995 - Artificial neural networks for feature extraction and multivariate data projection.pdf:pdf},
isbn = {1045-9227},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {296--317},
pmid = {18263314},
title = {{Artificial neural networks for feature extraction and multivariate data projection}},
volume = {6},
year = {1995}
}
@article{Scholz2005,
abstract = {MOTIVATION: Visualizing and analysing the potential non-linear structure of a dataset is becoming an important task in molecular biology. This is even more challenging when the data have missing values. RESULTS: Here, we propose an inverse model that performs non-linear principal component analysis (NLPCA) from incomplete datasets. Missing values are ignored while optimizing the model, but can be estimated afterwards. Results are shown for both artificial and experimental datasets. In contrast to linear methods, non-linear methods were able to give better missing value estimations for non-linear structured data.Application: We applied this technique to a time course of metabolite data from a cold stress experiment on the model plant Arabidopsis thaliana, and could approximate the mapping function from any time point to the metabolite responses. Thus, the inverse NLPCA provides greatly improved information for better understanding the complex response to cold stress. CONTACT: scholz@mpimp-golm.mpg.de.},
annote = {NULL},
author = {Scholz, Matthias and Kaplan, Fatma and Guy, Charles L. and Kopka, Joachim and Selbig, Joachim},
doi = {10.1093/bioinformatics/bti634},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scholz et al. - 2005 - Non-linear PCA A missing data approach.pdf:pdf},
isbn = {1367-4803},
issn = {13674803},
journal = {Bioinformatics},
number = {20},
pages = {3887--3895},
pmid = {16109748},
title = {{Non-linear PCA: A missing data approach}},
volume = {21},
year = {2005}
}
@phdthesis{Lackey2013,
annote = {NULL},
author = {Lackey, Christopher},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lackey - 2013 - Relationships Between Motivation, Self-Efficacy, Mindsets, Attributions, And Learning Strategies An Exploratory Study.pdf:pdf},
title = {{Relationships Between Motivation, Self-Efficacy, Mindsets, Attributions, And Learning Strategies: An Exploratory Study}},
year = {2013}
}
@techreport{Fuys1984,
author = {Fuys, David},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fuys - 1984 - English Translation of Selected Writings of Dina van Hiele-Geldof and Pierre M. van Hiele.pdf:pdf},
title = {{English Translation of Selected Writings of Dina van Hiele-Geldof and Pierre M. van Hiele.}},
year = {1984}
}
@article{Corpus2007,
author = {Corpus, Jennifer Henderlong and Lepper, Mark R},
doi = {10.1080/01443410601159852},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Corpus, Lepper - 2007 - The Effects of Person Versus Performance Praise on Children's Motivation Gender and age as moderating factors.pdf:pdf},
isbn = {0144341060115},
journal = {Educational Psychology},
number = {4},
pages = {487--508},
title = {{The Effects of Person Versus Performance Praise on Children's Motivation: Gender and age as moderating factors}},
volume = {27},
year = {2007}
}
@article{Mayberry1983,
author = {Mayberry, Joanne},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayberry - 1983 - The Van Hiele Levels of Geometric Thought in Undergraduate Preservice Teachers.pdf:pdf},
journal = {Journal for Research in Matematics Education},
number = {1},
pages = {58--69},
title = {{The Van Hiele Levels of Geometric Thought in Undergraduate Preservice Teachers}},
volume = {14},
year = {1983}
}
@article{Rattan2012,
author = {Rattan, Aneeta and Good, Catherine and Dweck, Carol S},
doi = {10.1016/j.jesp.2011.12.012},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rattan, Good, Dweck - 2012 - â€œIt's ok â€” Not everyone can be good at mathâ€ Instructors with an entity theory comfort (and demotivat.pdf:pdf},
issn = {0022-1031},
journal = {Journal of Experimental Social Psychology},
keywords = {Implicit theory,Intelligence,Pedagogical practice,Teaching},
number = {3},
pages = {731--737},
publisher = {Elsevier Inc.},
title = {{â€œIt's ok â€” Not everyone can be good at mathâ€: Instructors with an entity theory comfort (and demotivate) students}},
volume = {48},
year = {2012}
}
@article{Nathan2003,
author = {Nathan, Mitchell J and Petrosino, Anthony},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nathan, Petrosino - 2003 - Expert Blind Spot Among Pre-Service Teachers.pdf:pdf},
journal = {American Educational Research Journal},
number = {4},
pages = {905--928},
title = {{Expert Blind Spot Among Pre-Service Teachers}},
volume = {40},
year = {2003}
}
@misc{Jackson1984,
author = {Jackson, Douglas N.},
title = {{Personality Research Manual}},
year = {1984}
}
@techreport{Yeager2013,
author = {Yeager, David S and Paunesku, Dave and Walton, Gregory M and Dweck, Carol S},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeager et al. - 2013 - Excellence in Education The Importance of Academic Mindsets.pdf:pdf},
title = {{Excellence in Education: The Importance of Academic Mindsets}},
year = {2013}
}
@article{Haimovitz2016,
author = {Haimovitz, Kyla and Dweck, Carol S},
doi = {10.1177/0956797616639727},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haimovitz, Dweck - 2016 - What Predicts Children's Fixed and Growth Intelligence Mind-Sets Not Their Parents' Views of Intelligence.pdf:pdf},
journal = {Psychological Science},
keywords = {15,16,17,25,academic achievement,and policymakers agree that,childhood development,educational psychology,educators,motivation,open materials,par-,received 9,researchers,revision accepted 2},
number = {6},
pages = {859--869},
title = {{What Predicts Children's Fixed and Growth Intelligence Mind-Sets? Not Their Parents' Views of Intelligence but Their Parents' Views of Failure}},
volume = {27},
year = {2016}
}
@article{Haimovitz2011,
author = {Haimovitz, Kyla and Corpus, Jennifer H.},
doi = {10.1080/01443410.2011.585950},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haimovitz, Corpus - 2011 - Effects of person versus process praise on student motivation stability and change in emerging adulthood.pdf:pdf},
journal = {Educational Psychology},
number = {5},
pages = {595--609},
title = {{Effects of person versus process praise on student motivation: stability and change in emerging adulthood}},
volume = {31},
year = {2011}
}
@misc{NCES2016,
author = {NCES},
title = {{NCES SAT Statistics}},
year = {2016}
}
@article{Deci2001,
author = {Deci, Edward L and Koestner, Richard and Ryan, Richard M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deci, Koestner, Ryan - 2001 - Extrinsic Rewards and Intrinsic Motivation in Education Reconsidered Once Again.pdf:pdf},
journal = {Review of Educational Research},
number = {1},
pages = {1--27},
title = {{Extrinsic Rewards and Intrinsic Motivation in Education: Reconsidered Once Again}},
volume = {71},
year = {2001}
}
@incollection{Greeno1987,
author = {Greeno, J. and Riley, M.},
booktitle = {Metacognition, motivation, and understanding},
pages = {289--313},
title = {{Processes and development of understanding}},
year = {1987}
}
@article{Campbell1986,
author = {Campbell, Nancy K and Hackett, Gail},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Campbell, Hackett - 1986 - The Effects of Mathematics Task Performance on Math Self-Efficacy and Task Interest.pdf:pdf},
journal = {Journal of Vocational Behavior},
pages = {149--162},
title = {{The Effects of Mathematics Task Performance on Math Self-Efficacy and Task Interest}},
volume = {162},
year = {1986}
}
@article{Becker1990,
author = {Becker, Betsy J},
journal = {American Educational Research Journal},
pages = {65--87},
title = {{Item characteristics and gender differences on the SAT-M for mathematically able youths}},
volume = {27},
year = {1990}
}
@article{Blackwell2007,
author = {Blackwell, Lisa S and Trzesniewski, Kali H and Dweck, Carol S},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blackwell, Trzesniewski, Dweck - 2007 - Implicit Theories of Intelligence Predict Achievement Across an Adolescent Transition A Longitud.pdf:pdf},
journal = {Child Development},
number = {1},
pages = {246--263},
title = {{Implicit Theories of Intelligence Predict Achievement Across an Adolescent Transition: A Longitudinal Study and an Intervention}},
volume = {78},
year = {2007}
}
@article{Dweck2010,
author = {Dweck, Carol S},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dweck - 2010 - Mind-Sets and Equitable Education.pdf:pdf},
journal = {Principal Leadership},
number = {5},
pages = {26--29},
title = {{Mind-Sets and Equitable Education}},
volume = {10},
year = {2010}
}
@article{Pajares1999,
author = {Pajares, Frank and Graham, Laura},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pajares, Graham - 1999 - Self-Efficacy , Motivation Constructs , and Mathematics Performance of Entering Middle School Students.pdf:pdf},
pages = {124--139},
title = {{Self-Efficacy , Motivation Constructs , and Mathematics Performance of Entering Middle School Students}},
volume = {139},
year = {1999}
}
@incollection{Davis2012,
abstract = {In this chapter, three interrelated conceptsâ€”student engagement, motiva- tion, and resilienceâ€”are examined through the lens of â€œmindsets.â€ Mindsets are assumptions that we possess about ourselves and others that guide our behavior. The mindset that educators hold about the factors that contribute to student engagement, motivation, and resilience determines their expectations, teaching practices, and relationships with students. We identify the key components of these three concepts, highlighting those that overlap. We distinguish between extrinsic and intrinsic motivation and the ways in which the latter is more closely attuned with student engagement and resilience than the former. We encourage the ongoing discussion of mindsets at staff meetings so that teachers become increas- ingly aware of the mindset of engaged, motivated learners and consider how to nurture this mindset in the classroom. We offer many strategies to facilitate the enrichment of this mindset in all students},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Davis, Marcia H and Mcpartland, James M},
booktitle = {Handbook of Research on Student Engagement},
doi = {10.1007/978-1-4614-2018-7},
eprint = {arXiv:1011.1669v3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Mcpartland - 2012 - The Power of Mindsets Nurturing Engagement, Motivation, and Resilience in Students.pdf:pdf},
isbn = {978-1-4614-2017-0},
issn = {03610365},
pages = {515--539},
pmid = {6370941},
title = {{The Power of Mindsets: Nurturing Engagement, Motivation, and Resilience in Students}},
year = {2012}
}
@article{Atwood2010,
abstract = {The belief that a trait can be cultivated with effort, known as an incremental theory or growth mindset, promotes behavior that leads to higher levels of achievement, such as the enthusiastic embrace of challenges and resilience to obstacles. Roughly 40{\%} of the general student population in the United States, however, conceptualizes intelligence as an innate and immutable trait, a belief that tends to inhibit motivation and learning. To better inculcate an incremental theory of intelligence, educators and psychologists should identity traits that a majority of students believe are malleable, and investigate the dynamics that facilitate optimism about their developmental potential. In service to this end, the present study illuminates a bifurcation of both belief and behavior related to student engagement in the domains of school and sport. A survey of 251 middle school students confirmed two hypotheses: individuals are significantly more likely (a) to have a growth mindset of athletic ability compared to intelligence, and (b) to exhibit mastery-oriented responses in athletic versus academic environments. The organizational infrastructure of athletic programs, which institutionalizes practice, emphasizes effort, and values the coach as a developmental expert, is thought to powerfully cultivate the idea of athletic ability as a malleable traitand offers clues about how to design educational interventions that increase the number of students who believe intelligence is something they can improve with effort.},
author = {Atwood, Jason R},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Atwood - 2010 - Mindset, Motivation and Metaphor in School and Sport Bifurcated beliefs and behaviour in two different achievement domai.pdf:pdf},
journal = {American Education Research Association},
pages = {1--30},
title = {{Mindset, Motivation and Metaphor in School and Sport: Bifurcated beliefs and behaviour in two different achievement domains}},
year = {2010}
}
@article{Boaler2013,
abstract = {Recent scientific evidence demonstrates both the incredible potential of the brain to grow and change and the powerful impact of growth mindset messages upon students' attainment. Schooling practices, however, particularly in England, are based upon notions of fixed ability thinking which limits students' attainment and increases inequality. This article reviews evidence for brain plasticity, the importance of mindset and the ways that mindset messages may be communicated through classroom and grouping practices.},
author = {Boaler, Jo},
doi = {10.2304/forum.2013.55.1.143},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boaler - 2013 - Ability and Mathematics The Mindset Revolution that Is Reshaping Education.pdf:pdf},
issn = {0963-8253},
journal = {FORUM: for promoting 3-19 comprehensive education},
keywords = {Ability Grouping,Brain,Brain Hemisphere Functions,Child Development,Cognitive Ability,Elementary Education,England,Foreign Countries,Mathematics Achievement,Mathematics Instruction,Teaching Methods},
number = {1},
pages = {143--152},
title = {{Ability and Mathematics: The Mindset Revolution that Is Reshaping Education}},
volume = {55},
year = {2013}
}
@article{Masters2013,
author = {Masters, Geoff N},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Masters - 2013 - Towards a growth mindset in assessment.pdf:pdf},
journal = {Australian Council for Educational Research Occasional Essays},
title = {{Towards a growth mindset in assessment}},
year = {2013}
}
@article{Dweck2008,
abstract = {There is a growing body of evidence that students mindsets play a key role in their math and science achievement. Students who believe that intelligence or math and sci- ence ability is simply a fixed trait (a fixed mindset) are at a significant disadvantage com- pared to students who believe that their abilities can be developed (a growth mindset). Moreover, research is showing that these mindsets can play an important role in the relative underachievement of women and minorities in math and science. Below, I will present research showing that a) mindsets can predict math/science achievement over time; b) mindsets can contribute to math/science achievement discrepancies for women and minorities; c) interventions that change mindsets can boost achievement and reduce achievement discrepancies; and d) educators play a key role in shaping students mindsets.},
author = {Dweck, Carol S},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dweck - 2008 - Mindsets and Math Science Achievement.pdf:pdf},
journal = {The Opportunity Equation: Transforming Mathematics and Science Education for Citizenship and the Global Economy},
pages = {1--17},
title = {{Mindsets and Math / Science Achievement}},
year = {2008}
}
@article{Singh2017,
author = {Singh, Kusum and Granville, Monique and Dika, Sandra},
doi = {10.1080/00220670209596607},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh, Granville, Dika - 2017 - Mathematics and Science Achievement Effects of Motivation, Interest, and Academic Engagement.pdf:pdf},
journal = {The Journal of Education Research},
keywords = {1988,academic engagement effects,are a crit-,grades 5 through 8,he middle school years,interest,longitudinal study,mathematics and science achievement,motivation,national education},
number = {February},
title = {{Mathematics and Science Achievement: Effects of Motivation, Interest, and Academic Engagement}},
volume = {0671},
year = {2017}
}
@article{Schiefele1995,
author = {Schiefele, Ulrich and Csikszentmihalyi, Mihaly},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schiefele, Csikszentmihalyi - 1995 - Motivation and Ability as Factors in Mathematics Experience and Achievement.pdf:pdf},
journal = {Journal for Research in Matematics Education},
number = {2},
pages = {163--181},
title = {{Motivation and Ability as Factors in Mathematics Experience and Achievement}},
volume = {26},
year = {1995}
}
@book{Dweck2008a,
author = {Dweck, Carol S},
title = {{Mindset: The new psychology of success}},
year = {2008}
}
@article{Hannula2006,
author = {Hannula, Markku S.},
doi = {10.1007/s10649-005-9019-8},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hannula - 2006 - MOTIVATION IN MATHEMATICS GOALS REFLECTED IN EMOTIONS.pdf:pdf},
journal = {Educational studies in mathematics},
keywords = {affect,emotion,goal,mathematics learning,motivation,needs},
number = {2},
pages = {165--178},
title = {{MOTIVATION IN MATHEMATICS : GOALS REFLECTED IN EMOTIONS}},
volume = {63},
year = {2006}
}
@article{Ng2002,
author = {Ng, Andrew Y and Jordan, Michael I},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng, Jordan - 2002 - On Discriminative vs. Generative classifiers A comparison of logistic regression and naive Bayes.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {841--848},
title = {{On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes}},
year = {2002}
}
@article{Belenky2014,
abstract = {Research in both cognitive and educational psychology has explored the effect of different types of external knowledge representations (e.g., manipulatives, graphical/pictorial representations, texts) on a variety of important outcome measures. We place this large and multifaceted research literature into an organizing framework, classifying three categories of external knowledge representations along a dimension of groundedness: (1) idealized, (2) grounded and including only relevant features, and (3) grounded and including irrelevant features. This organizing framework allows us to focus on the implications of these characteristics of external knowledge representations on three important educational outcomes: learning and immediate performance using the target knowledge, the degree to which that knowledge can transfer flexibly, and the interest engendered by the learning materials. We illustrate the frame- work by mapping a wide body of research from educational and cognitive psychology onto its dimensions. This framework can aid educators by clearly statingwhat the research literature says about these characteristics of external knowledge representations and how they activate and support the construction of internal knowledge representations. In particular, it will speak to how to best structure instruction using external knowledge representations with different characteris- tics, depending on the learning objective. Researcherswill benefit from the analysis of the current state of knowledge and by the description of what open questions still remain.},
author = {Belenky, Daniel M. and Schalk, Lennart},
doi = {10.1007/s10648-014-9251-9},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Belenky, Schalk - 2014 - The Effects of Idealized and Grounded Materials on Learning, Transfer, and Interest An Organizing Framework for.pdf:pdf},
isbn = {1040-726X$\backslash$n1573-336X},
issn = {1040726X},
journal = {Educational Psychology Review},
keywords = {Grounded,Idealized,Interest,Learning,Learning materials,Transfer},
number = {1},
pages = {27--50},
title = {{The Effects of Idealized and Grounded Materials on Learning, Transfer, and Interest: An Organizing Framework for Categorizing External Knowledge Representations}},
volume = {26},
year = {2014}
}
@article{Ainsworth2006,
abstract = {Multiple (external) representations can provide unique benefits when people are learning complex new ideas. Unfortunately, many studies have shown this promise is not always achieved. The DeFT (Design, Functions, Tasks) framework for learning with multiple representations integrates research on learning, the cognitive science of representation and constructivist theories of education. It proposes that the effectiveness of multiple representations can best be understood by considering three fundamental aspects of learning: the design parameters that are unique to learning with multiple representations; the functions that multiple representations serve in supporting learning and the cognitive tasks that must be undertaken by a learner interacting with multiple representations. The utility of this framework is proposed to be in identifying a broad range of factors that influence learning, reconciling inconsistent experimental findings, revealing under-explored areas of multi-representational research and pointing forward to potential design heuristics for learning with multiple representations. ?? 2006 Elsevier Ltd. All rights reserved.},
author = {Ainsworth, Shaaron},
doi = {10.1016/j.learninstruc.2006.03.001},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ainsworth - 2006 - DeFT A conceptual framework for considering learning with multiple representations.pdf:pdf},
isbn = {0959-4752 DO  - http://dx.doi.org/10.1016/j.learninstruc.2006.03.001},
issn = {09594752},
journal = {Learning and Instruction},
keywords = {Diagrams,Multimedia,Multiple representations,Pictures},
number = {3},
pages = {183--198},
title = {{DeFT: A conceptual framework for considering learning with multiple representations}},
volume = {16},
year = {2006}
}
@article{Schwartz2005,
author = {Schwartz, Daniel L and Bransford, John D and Sears, David},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwartz, Bransford, Sears - 2005 - Efficiency and innvoation in transfer.pdf:pdf},
pages = {1--73},
title = {{Efficiency and innvoation in transfer}},
year = {2005}
}
@article{Rau2016,
abstract = {Visual representations play a critical role in enhancing science, technology, engi- neering, and mathematics (STEM) learning. Educational psychology research shows that adding visual representations to text can enhance students' learning of content knowl- edge, compared to text-only. But should students learn with a single type of visual representation or with multiple different types of visual representations? This article addresses this question from the perspective of the representation dilemma, namely that students often learn content they do not yet understand from representations they do not yet understand. To benefit from visual representations, students therefore need representational competencies, that is, knowledge about how visual representations depict information about the content. This article reviews literature on representational competencies involved in students' learning of content knowledge. Building on this review, this article analyzes how the number of visual representations affects the role these representational competencies play during students' learning of content knowl- edge. To this end, the article compares two common scenarios: text plus a single type of visual representations (T+SV) and text plus multiple types of visual representations (T+MV). The comparison yields seven hypotheses that describe under which condi- tions T+MV scenarios are more effective than T+SV scenarios. Finally, the article reviews empirical evidence for each hypothesis and discusses open questions about the representation dilemma.},
author = {Rau, Martina A.},
doi = {10.1007/s10648-016-9365-3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rau - 2016 - Conditions for the Effectiveness of Multiple Visual Representations in Enhancing STEM Learning.pdf:pdf},
issn = {1573336X},
journal = {Educational Psychology Review},
keywords = {Conceptual and perceptual knowledge,Inductive learning processes,Multiple external representations,Sense-making processes,Visualizations},
pages = {1--45},
publisher = {Educational Psychology Review},
title = {{Conditions for the Effectiveness of Multiple Visual Representations in Enhancing STEM Learning}},
year = {2016}
}
@article{Nokes2005,
abstract = {Contemporary theories of learning postulate one or at most a small number of different learning mechanisms. However, people are capable of mastering a given task through qualitatively different learning paths such as learning by instruction and learning by doing. We hypothesize that the knowledge acquired through such alternative paths differs with respect to the level of abstraction and the balance between declarative and procedural knowledge. In a laboratory experiment we investigated what was learned about patterned letter sequences via either direct instruction in the relevant patterns or practice in solving letter-sequence extrapolation problems. Results showed that both types of learning led to mastery of the target task as measured by accuracy performance. However, behavioral differences emerged in how participants applied their knowledge. Participants given instruction showed more variability in the types of strategies they used to articulate their knowledge as well as longer solution times for generating the action implications of that knowledge as compared to the participants given practice. Results are discussed regarding the implications for transfer, generalization, and procedural application. Learning theories that claim generality should be tested against cross-scenario phenomena, not just parametric variations of a single learning scenario.},
author = {Nokes, Timothy J and Ohlsson, Stellan},
doi = {10.1207/s15516709cog0000_32},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nokes, Ohlsson - 2005 - Comparing multiple paths to mastery What is learned.pdf:pdf},
isbn = {0364-0213},
issn = {0364-0213},
journal = {Cognitive Science},
keywords = {human experimentation,instruction,problem solving,psychology,representation,skill acquisition and learning},
pages = {769--796},
pmid = {21702793},
title = {{Comparing multiple paths to mastery: What is learned?}},
volume = {29},
year = {2005}
}
@article{Koedinger2012,
abstract = {Despite the accumulation of substantial cognitive science research relevant to education, there remains confusion and controversy in the application of research to educational practice. In support of a more systematic approach, we describe the Knowledge-Learning-Instruction (KLI) framework. KLI promotes the emergence of instructional principles of high potential for generality, while explicitly identifying constraints of and opportunities for detailed analysis of the knowledge students may acquire in courses. Drawing on research across domains of science, math, and language learning, we illustrate the analyses of knowledge, learning, and instructional events that the KLI framework affords. We present a set of three coordinated taxonomies of knowledge, learning, and instruction. For example, we identify three broad classes of learning events (LEs): (a) memory and fluency processes, (b) induction and refinement processes, and (c) understanding and sense-making processes, and we show how these can lead to different knowledge changes and constraints on optimal instructional choices.},
author = {Koedinger, Kenneth R. and Corbett, Albert T. and Perfetti, Charles},
doi = {10.1111/j.1551-6709.2012.01245.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koedinger, Corbett, Perfetti - 2012 - The Knowledge-Learning-Instruction Framework Bridging the Science-Practice Chasm to Enhance Robust.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Cognitive modeling,Cognitive task analysis,Education,Experimentation,Instructional principles,Knowledge representation,Learning principles},
number = {5},
pages = {757--798},
pmid = {22486653},
title = {{The Knowledge-Learning-Instruction Framework: Bridging the Science-Practice Chasm to Enhance Robust Student Learning}},
volume = {36},
year = {2012}
}
@article{Greeno1997,
abstract = {Anderson, Reder, and Simon (1996) contested four propositions that they incorrectly called "claims of situated learning." This response argues that the important differences between situative and cognitive perspectives are not addressed by discussion of these imputed claims. Instead, there are significant differences in the framing assumptions of the two perspectives. I clarify these differences by inferring questions to which Anderson et al.'s discussion provided answers, by identifying presuppositions of those questions made by Anderson et al., and by stating the different presuppositions and questions that I believe are consistent with the situative perspective. The evidence given by Anderson et al. is compatible with the framing assumptions of situativity; therefore, deciding between the perspectives will involve broader considerations than those presented in their article. These considerations include expectations about which framework offers the better prospect for developing a unified scientific account of activity considered from both social and individual points of view, and which framework supports research that will inform discussions of educational practice more productively. The cognitive perspective takes the theory of individual cognition as its basis and builds toward a broader theory by incrementally developing analyses of additional components that are considered as contexts. The situative perspective takes the theory of social and ecological interaction as its basis and builds toward a more comprehensive theory by developing increasingly detailed analyses of information structures in the contents of people's interactions. While I believe that the situative framework is more promising, the best strategy for the field is for both perspectives to be developed energetically.},
author = {Greeno, J. G.},
doi = {10.3102/0013189X026001005},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Greeno - 1997 - On Claims That Answer the Wrong Questions.pdf:pdf},
isbn = {0013189X},
issn = {0013-189X},
journal = {Educational Researcher},
number = {1},
pages = {5--17},
title = {{On Claims That Answer the Wrong Questions}},
volume = {26},
year = {1997}
}
@article{Oh2017,
annote = {Seems like a lot of the advantage is built in by their "analogy regularization" which is more of building in the solution to an entirely new cost function than a regularization per se.},
author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oh et al. - 2017 - COMMUNICATING HIERARCHICAL NEURAL CONTROLLERS FOR LEARNING ZERO - SHOT TASK GENERALIZATION.pdf:pdf},
journal = {Submitted to ICLR 2017},
title = {{COMMUNICATING HIERARCHICAL NEURAL CONTROLLERS FOR LEARNING ZERO - SHOT TASK GENERALIZATION}},
year = {2017}
}
@article{Day2011,
annote = {NULL},
author = {Day, Samuel B and Goldstone, Robert L},
doi = {10.1037/a0022333},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Day, Goldstone - 2011 - Analogical Transfer From a Simulated Physical System.pdf:pdf},
journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
number = {3},
pages = {551--567},
title = {{Analogical Transfer From a Simulated Physical System}},
volume = {37},
year = {2011}
}
@article{Day2007,
author = {Day, Samuel B and Gentner, Dedre},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Day, Gentner - 2007 - Nonintentional analogical inference in text comprehension.pdf:pdf},
title = {{Nonintentional analogical inference in text comprehension}},
year = {2007}
}
@article{Day2012,
author = {Day, Samuel B and Goldstone, Robert L},
doi = {10.1080/00461520.2012.696438},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Day, Goldstone - 2012 - The Import of Knowledge Export Connecting Findings and Theories of Transfer of Learning.pdf:pdf},
journal = {Educational Psychologist},
number = {3},
pages = {153--176},
title = {{The Import of Knowledge Export : Connecting Findings and Theories of Transfer of Learning}},
volume = {47},
year = {2012}
}
@article{Bransford1999,
author = {Bransford, John D and Schwartz, Daniel L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bransford, Schwartz - 1999 - Rethinking Transfer A Simple Proposal With Multiple Implications.pdf:pdf},
journal = {Review of Research in Education},
number = {1},
pages = {61--100},
title = {{Rethinking Transfer : A Simple Proposal With Multiple Implications}},
volume = {24},
year = {1999}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6â€“8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9â€“11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}
@article{Richland2012a,
abstract = {Many students graduate from Kâ€“12 mathematics programs without flexible, conceptual mathematics knowledge. This article reviews psychological and educational research to propose that refining Kâ€“12 classroom instruction such that students draw connections through relational comparisons may enhance their long-term ability to transfer and engage with mathematics as a meaningful system. We begin by examining the mathematical knowledge of students in one community college, reviewing results that show even after completing a Kâ€“12 required mathematics sequence, these students were unlikely to flexibly reason about mathematics. Rather than drawing relationships between presented problems or inferences about the representations, students preferred to attempt previously memorized (often incorrect) procedures (Givvin, Stigler, {\&} Thompson, 2011; Stigler, Givvin, {\&} Thompson, 2010). We next describe the relations between the cognition of flexible, comparative reasoning and experimentally derived strategies for supporting students' ability to make these connections. A cross-cultural study found that U.S. teachers currently use these strategies much less frequently than their international counterparts (Hiebert et al., 2003; Richland, Zur, {\&} Holyoak, 2007), suggesting that these practices may be correlated with high student performance. Finally, we articulate a research agenda for improving and studying pedagogical practices for fostering students' relational thinking about mathematics.},
author = {Richland, Lindsey E. and Stigler, James W. and Holyoak, Keith J.},
doi = {10.1080/00461520.2012.667065},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Richland, Stigler, Holyoak - 2012 - Teaching the Conceptual Structure of Mathematics(2).pdf:pdf},
isbn = {0046-1520$\backslash$r1532-6985},
issn = {0046-1520},
journal = {Educational Psychologist},
number = {3},
pages = {189--203},
title = {{Teaching the Conceptual Structure of Mathematics}},
volume = {47},
year = {2012}
}
@article{Andreas,
archivePrefix = {arXiv},
arxivId = {arXiv:1601.01705v4},
author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
eprint = {arXiv:1601.01705v4},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andreas et al. - 2016 - Learning to Compose Neural Networks for Question Answering.pdf:pdf},
journal = {arXiv preprint},
title = {{Learning to Compose Neural Networks for Question Answering}},
year = {2016}
}
@article{Mikolov2015,
abstract = {The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment.},
archivePrefix = {arXiv},
arxivId = {1511.08130},
author = {Mikolov, Tomas and Joulin, Armand and Baroni, Marco},
eprint = {1511.08130},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Joulin, Baroni - 2015 - A Roadmap towards Machine Intelligence.pdf:pdf},
journal = {ArXiv},
pages = {1--39},
title = {{A Roadmap towards Machine Intelligence}},
year = {2015}
}
@article{Wang2016,
abstract = {We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing how to quickly learn a semantic parsing model from scratch, and that modeling pragmatics further accelerates learning for successful players.},
archivePrefix = {arXiv},
arxivId = {1606.02447},
author = {Wang, Sida I. and Liang, Percy and Manning, Christopher D.},
doi = {10.18653/v1/P16-1224},
eprint = {1606.02447},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Liang, Manning - 2016 - Learning Language Games through Interaction.pdf:pdf},
journal = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)},
pages = {2368--2378},
title = {{Learning Language Games through Interaction}},
year = {2016}
}
@article{Yamins2016a,
author = {Yamins, Daniel L K and Dicarlo, James J},
doi = {10.1038/nn.4244},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yamins, Dicarlo - 2016 - Using goal-driven deep learning models to understand sensory cortex.pdf:pdf},
journal = {Nature Neuroscience},
number = {3},
pages = {356--365},
title = {{Using goal-driven deep learning models to understand sensory cortex}},
volume = {19},
year = {2016}
}
@incollection{McClelland2016b,
author = {Mcclelland, J L and Sadeghi, Zahra and Saxe, A M},
booktitle = {Neurocomputational Models of Cognitive Development and Processing},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcclelland, Sadeghi, Saxe - 2016 - A Critique of Pure Hierarchy Uncovering Cross-Cutting Structure in a Natural Dataset.pdf:pdf},
pages = {51--68},
title = {{A Critique of Pure Hierarchy : Uncovering Cross-Cutting Structure in a Natural Dataset}},
year = {2016}
}
@article{Mirsky1960,
author = {Mirsky, L.},
doi = {10.1093/qmath/11.1.50},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mirsky - 1960 - Symmetric gauge functions and unitarily invariant norms.pdf:pdf},
issn = {0033-5606},
journal = {The Quarterly Journal of Mathematics},
number = {1},
pages = {50--59},
title = {{Symmetric gauge functions and unitarily invariant norms}},
volume = {11},
year = {1960}
}
@article{Falkenhainer1989,
author = {Falkenhainer, Brian and Forbus, Kenneth D and Gentner, Dedre},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Falkenhainer, Forbus, Gentner - 1989 - The Structure-Mapping Engine Algorithm and Examples.pdf:pdf},
journal = {Artificial Intelligence},
number = {1},
pages = {1--63},
title = {{The Structure-Mapping Engine : Algorithm and Examples}},
volume = {41},
year = {1989}
}
@article{Thompson2000,
author = {Thompson, Roger K.R. and Oden, David L.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thompson, Oden - 2000 - Categorical Perception and Conceptual Judgments by Nonhuman Primates The Paleological Monkey and the Analogical.pdf:pdf},
journal = {Cognitive Science},
number = {3},
pages = {363--396},
title = {{Categorical Perception and Conceptual Judgments by Nonhuman Primates : The Paleological Monkey and the Analogical Ape}},
volume = {24},
year = {2000}
}
@article{Novick1988,
author = {Novick, Laura R},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Novick - 1988 - Analogical Transfer , Problem Similarity , and Expertise.pdf:pdf},
number = {3},
pages = {510--520},
title = {{Analogical Transfer , Problem Similarity , and Expertise}},
volume = {14},
year = {1988}
}
@article{Golub1987,
abstract = {The Eckart-Young-Mirsky theorem solves the problem of approximating a matrix by one of lower rank. However, the approximation generally differs from the original in all its elements. In this paper it is shown how to obtain a best approximation of lower rank in which a specified set of columns of the matrix remains fixed. The paper concludes with some applications of the generalization. {\textcopyright} 1987.},
author = {Golub, G. H. and Hoffman, Alan and Stewart, G. W.},
doi = {10.1016/0024-3795(87)90114-5},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Golub, Hoffman, Stewart - 1987 - A generalization of the Eckart-Young-Mirsky matrix approximation theorem.pdf:pdf},
isbn = {0024-3795},
issn = {00243795},
journal = {Linear Algebra and Its Applications},
number = {C},
pages = {317--327},
title = {{A generalization of the Eckart-Young-Mirsky matrix approximation theorem}},
volume = {88-89},
year = {1987}
}
@article{Thompson1997,
author = {Thompson, Roger K.R. and Oden, David L. and Boysen, Sarah T.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thompson, Oden, Boysen - 1997 - Language-Naive Chimpanzees ( Pan troglodytes ) Judge Relations Between Relations in a Conceptual Matchin.pdf:pdf},
journal = {Journal of Experimental Psychology: Animal Behavior Processes},
number = {1},
pages = {31--43},
title = {{Language-Naive Chimpanzees ( Pan troglodytes ) Judge Relations Between Relations in a Conceptual Matching-to-Sample Task}},
volume = {23},
year = {1997}
}
@article{Johnson2016a,
abstract = {We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for Englishâ†’French and surpasses state-of-the-art results for Englishâ†’German. Similarly, a single multilingual model surpasses state-of-the-art results for Frenchâ†’English and Germanâ†’English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.},
archivePrefix = {arXiv},
arxivId = {1611.04558},
author = {Johnson, Melvin and Schuster, Mike and Le, Quoc V and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'{e}}gas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1611.04558},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson et al. - 2016 - Google's Multilingual Neural Machine Translation System Enabling Zero-Shot Translation.pdf:pdf},
journal = {arXiv},
pages = {1--16},
title = {{Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation}},
year = {2016}
}
@article{Jaderberg2016,
abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the- art on Atari, averaging 880{\%} expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10Ã— and averaging 87{\%} expert human performance on Labyrinth.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.03044v2},
author = {Jaderberg, Max and Mnih, Volodymyr and Czarneckim, Marian Wojciech and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
eprint = {arXiv:1509.03044v2},
journal = {arXiv},
pages = {1--11},
title = {{Reinforcement Learning with Unsupervised Auxiliary Tasks}},
year = {2016}
}
@incollection{Gentner2003,
author = {Gentner, Dedre},
booktitle = {Language in mind: Advances in the study of language and thought.},
pages = {195--235},
title = {{Why We're So Smart}},
year = {2003}
}
@article{Dong2015,
abstract = {In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available.},
author = {Dong, Daxiang and Wu, Hua and He, Wei and Yu, Dianhai and Wang, Haifeng},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
pages = {1723--1732},
title = {{Multi-Task Learning for Multiple Language Translation}},
volume = {1},
year = {2015}
}
@article{Luong2016,
abstract = {Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three settings to multi-task sequence to sequence learning: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on parsing and image caption generation improves translation accuracy and vice versa. We also present novel findings on the benefit of the different unsupervised learning objectives: we found that the skip-thought objective is beneficial to translation while the sequence autoencoder objective is not.},
archivePrefix = {arXiv},
arxivId = {1511.06114},
author = {Luong, Minh-Thang and Le, Quoc V. and Sutskever, Ilya and Vinyals, Oriol and Kaiser, Lukasz},
eprint = {1511.06114},
isbn = {9789876400817},
journal = {ICLR},
pages = {1--9},
title = {{Multi-task Sequence to Sequence Learning}},
year = {2016}
}
@article{Rusu2015,
abstract = {Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.},
archivePrefix = {arXiv},
arxivId = {1511.06295},
author = {Rusu, Andrei A and {Gomez Colmenarejo}, Sergio and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
doi = {10.1038/nature14236},
eprint = {1511.06295},
isbn = {978-1-4799-0356-6},
issn = {0028-0836},
journal = {arXiv},
pages = {1--12},
pmid = {25719670},
title = {{Policy Distillation}},
year = {2015}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennington, Socher, Manning - 2014 - GloVe Global Vectors for Word Representation.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
pages = {1532--1543},
pmid = {1710995},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}
@article{Kollias2013,
abstract = {We present a PDP model of binary choice verbal analogy problems (A:B as C:[D1|D2], where D1 and D2 represent choice alternatives). We train a recurrent neural network in item-relation-item triples and use this network to test performance on analogy questions. Without training on analogy problems per se, the model explains the developmental shift from associative to relational responding as an emergent consequence of learning upon the environment's statistics. Such learning allows gradual, item-specific acquisition of relational knowledge to overcome the influence of unbalanced association frequency, accounting for association effects of analogical reasoning seen in cognitive development. The network also captures the overall degradation in performance after anterior temporal damage by deleting a fraction of learned connections, while capturing the return of associative dominance after frontal damage by treating frontal structures as necessary for maintaining activation of A and B while seeking a relation between C and D. While our theory is still far from being complete it provides a unified explanation of findings that need to be considered together in any integrated account of analogical reasoning.},
author = {Kollias, Pavlos and McClelland, James L.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kollias, McClelland - 2013 - Context, cortex, and associations A connectionist developmental approach to verbal analogies.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Analogical reasoning,Cognitive control,Cognitive development,Connectionist models,FTLD,Word association},
number = {NOV},
pages = {1--14},
pmid = {24312068},
title = {{Context, cortex, and associations: A connectionist developmental approach to verbal analogies}},
volume = {4},
year = {2013}
}
@article{Parisotto2015,
abstract = {The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed "Actor-Mimic", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.},
archivePrefix = {arXiv},
arxivId = {1511.06342},
author = {Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
eprint = {1511.06342},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parisotto, Ba, Salakhutdinov - 2015 - Actor-Mimic Deep Multitask and Transfer Reinforcement Learning.pdf:pdf},
journal = {arXiv preprint},
pages = {1--15},
title = {{Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning}},
year = {2015}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144v2},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1609.08144v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
journal = {arXiv},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Learning},
pages = {1--23},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
year = {2016}
}
@misc{Hinton1986,
abstract = {Concepts can be represented by distributed patterns of activity in networks of neuron-like units. One advantage of this kind of representation is that it leads to automatic generalization. When the weighjts in the network are changed to incorporate new knowledgwe about one concept, the changes affect the knowledge associated with other concepts that are represented by similar activity patterns. There have been numerous demonstrations of sensible generalization which have depended on the experimenter choosing appropriately similar patterns for diferent concepts. This paper shows how the network can be made to choose the patterns itself when shown a set of propositions that use the concepts. It chooses patterns which make explicit the underlying features that are only implicit in the propositions it is shown.},
author = {Hinton, Geoffrey},
booktitle = {Proceedings of the Eighth Annual Conference of the Cognitive Science Society},
doi = {10.1109/69.917563},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton - 1986 - Learning distributed representations of concepts.pdf:pdf},
isbn = {0-262-68053-X},
issn = {10414347},
pages = {1--12},
pmid = {21943171},
title = {{Learning distributed representations of concepts}},
year = {1986}
}
@article{Jacobs1991,
abstract = {We describe a multi-network, or modular, connectionist architecture that captures that fact that many tasks have structure at a level of granularity intermediate to that assumed by local and global function approximation schemes. The main innovation of the architecture is that it combines associative and competitive learning in order to learn task decompositions. A task decomposition is discovered by forcing the networks comprising the architecture to compete to learn the training patterns. As a result of the competition, different networks learn different training patterns and, thus, learn to partition the input space. The performance of the architecture on a "what" and "where" vision task and on a multi-payload robotics task are presented.},
author = {Jacobs, Ra and Jordan, Mi},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jacobs, Jordan - 1991 - A competitive modular connectionist architecture.pdf:pdf},
isbn = {1-55860-184-8},
journal = {Advances in neural information processing systems},
pages = {1--7},
title = {{A competitive modular connectionist architecture}},
year = {1991}
}
@article{Lake2016a,
abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving perfor- mance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recog- nition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
archivePrefix = {arXiv},
arxivId = {1604.00289},
author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
doi = {1511.09249v1},
eprint = {1604.00289},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake et al. - 2016 - Building Machines that learn and think like people.pdf:pdf},
issn = {14691825},
journal = {arXiv:1604.00289v1[cs.AI]},
number = {2012},
pages = {1--55},
pmid = {1000303116},
title = {{Building Machines that learn and think like people}},
year = {2016}
}
@article{Cvpr2017,
archivePrefix = {arXiv},
arxivId = {1612.02297},
author = {Cvpr, Anonymous and Id, Paper},
eprint = {1612.02297},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cvpr, Id - 2017 - Spatially Adaptive Computation Time for Residual Networks.pdf:pdf},
pages = {1--9},
title = {{Spatially Adaptive Computation Time for Residual Networks}},
year = {2017}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layersâ€”8Ã— deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {Arxiv.Org},
keywords = {deep learning,denoising auto-encoder,image denoising},
number = {3},
pages = {171--180},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
volume = {7},
year = {2015}
}
@article{GoogleResearch2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.04467v2},
author = {GoogleResearch},
eprint = {arXiv:1603.04467v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/GoogleResearch - 2015 - TensorFlow Large-scale machine learning on heterogeneous systems.pdf:pdf},
title = {{TensorFlow: Large-scale machine learning on heterogeneous systems}},
year = {2015}
}
@article{Maddison2016,
abstract = {The reparameterization trick enables the optimization of large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack continuous reparameterizations due to the discontinuous nature of discrete states. In this work we introduce concrete random variables -- continuous relaxations of discrete random variables. The concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-likelihood of latent stochastic nodes) on the corresponding discrete graph. We demonstrate their effectiveness on density estimation and structured prediction tasks using neural networks.},
archivePrefix = {arXiv},
arxivId = {1611.00712},
author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
eprint = {1611.00712},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maddison, Mnih, Teh - 2016 - The Concrete Distribution A Continuous Relaxation of Discrete Random Variables.pdf:pdf},
pages = {1--17},
title = {{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}},
year = {2016}
}
@article{Lake2015,
abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithmsâ€”for action, imagination, and explanation.We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.We also present several â€œvisual Turing testsâ€ probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
archivePrefix = {arXiv},
arxivId = {10.1126/science.aab3050},
author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
doi = {10.1126/science.aab3050},
eprint = {science.aab3050},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake, Salakhutdinov, Tenenbaum - 2015 - Human-level concept learning through probabilistic program induction.pdf:pdf},
isbn = {0036-8075},
issn = {0036-8075},
journal = {Science},
number = {6266},
pages = {1332--1338},
pmid = {26659050},
primaryClass = {10.1126},
title = {{Human-level concept learning through probabilistic program induction}},
volume = {350},
year = {2015}
}
@article{Rezende2016,
abstract = {Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples---having seen new examples just once---providing an important class of general-purpose models for one-shot machine learning.},
archivePrefix = {arXiv},
arxivId = {1603.05106},
author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Danihelka, Ivo and Gregor, Karol and Wierstra, Daan},
eprint = {1603.05106},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rezende et al. - 2016 - One-Shot Generalization in Deep Generative Models.pdf:pdf},
journal = {Arxiv},
title = {{One-Shot Generalization in Deep Generative Models}},
volume = {48},
year = {2016}
}
@article{Graves2016,
abstract = {Modern computers separate computation and memory. Computation is performed by a processor, which can use an addressable memory to bring operands in and out of play. This confers two important benefits: the use of extensible storage to write new information and the ability to treat the contents of memory as variables. Variables are critical to algorithm generality: to perform the same procedure on one datum or another, an algorithm merely has to change the address it reads from. In contrast to computers, the computational and memory resources of artificial neural networks are mixed together in the network weights and neuron activity. This is a major liability: as the memory demands of a task increase, these networks cannot allocate new storage dynam-ically, nor easily learn algorithms that act independently of the values realized by the task variables. Although recent breakthroughs demonstrate that neural networks are remarkably adept at sensory processing 1 , sequence learning 2,3 and reinforcement learning 4 , cognitive scientists and neuroscientists have argued that neural networks are limited in their ability to represent variables and data structures 5â€“9 , and to store data over long timescales without interference 10,11 . We aim to combine the advantages of neu-ral and computational processing by providing a neural network with readâ€“write access to external memory. The access is narrowly focused, minimizing interference among memoranda and enabling long-term storage 12,13 . The whole system is differentiable, and can therefore be trained end-to-end with gradient descent, allowing the network to learn how to operate and organize the memory in a goal-directed manner.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.5401v2},
author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'{n}}ska, Agnieszka and {G{\'{o}}mez Colmenarejo}, Sergio and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and {Moritz Hermann}, Karl and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
doi = {10.1038/nature20101},
eprint = {arXiv:1410.5401v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves et al. - 2016 - Hybrid computing using a neural network with dynamic external memory.pdf:pdf},
issn = {0028-0836},
journal = {Nature Publishing Group},
number = {7626},
pages = {471--476},
publisher = {Nature Publishing Group},
title = {{Hybrid computing using a neural network with dynamic external memory}},
volume = {538},
year = {2016}
}
@article{Siegler1998,
author = {Siegler, R and Stern, E},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Siegler, Stern - 1998 - Conscious and unconscious strategy discovery A microgenetic analysis.pdf:pdf},
journal = {Journal of Experimental Psychology: General},
number = {4},
pages = {377--397},
title = {{Conscious and unconscious strategy discovery: A microgenetic analysis}},
volume = {127},
year = {1998}
}
@article{Gregor2015,
abstract = {This paper introduces the Deep Recurrent Atten-tive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distin-guished from real data with the naked eye.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.04623v1},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Wierstra, Daan},
eprint = {arXiv:1502.04623v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gregor et al. - 2015 - DRAW A Recurrent Neural Network For Image Generation.pdf:pdf},
journal = {arXiv preprint},
pages = {1--16},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
year = {2015}
}
@article{Kumaran2016,
author = {Kumaran, Dharshan and Hassabis, Demis and McClelland, James L},
doi = {10.1016/j.tics.2016.05.004},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumaran, Hassabis, McClelland - 2016 - What Learning Systems do Intelligent Agents Need Complementary Learning Systems Theory Updated.pdf:pdf},
issn = {1364-6613},
journal = {Trends in Cognitive Sciences},
number = {7},
pages = {512--534},
publisher = {Elsevier Ltd},
title = {{What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated}},
volume = {20},
year = {2016}
}
@incollection{Cleeremans2002,
author = {Cleeremans, Axel and Jim{\'{e}}nez, Luis},
booktitle = {Implicit Learning and Consciousness: An Empirical, Philosophical and Computational Consensus in the Making (Frontiers of Cognitive Science)},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cleeremans, Jim{\'{e}}nez - 2002 - Implicit learning and consciousness A graded, dynamic perspective.pdf:pdf},
title = {{Implicit learning and consciousness: A graded, dynamic perspective}},
year = {2002}
}
@article{Reber1967,
author = {Reber, Arthur s.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reber - 1967 - Implicit Learning of Artificial Grammars.pdf:pdf},
journal = {Journal of Verbal Learning and Verbal Behavior},
number = {6},
pages = {855--863},
title = {{Implicit Learning of Artificial Grammars}},
volume = {6},
year = {1967}
}
@article{Davidson2012,
abstract = {We tested the hypothesis that, when children learn to correctly count sets, they make a semantic induction about the meanings of their number words. We tested the logical understanding of number words in 84 children that were classified as "cardinal-principle knowers" by the criteria set forth by Wynn (1992). Results show that these children often do not know (1) which of two numbers in their count list denotes a greater quantity, and (2) that the difference between successive numbers in their count list is 1. Among counters, these abilities are predicted by the highest number to which they can count and their ability to estimate set sizes. Also, children's knowledge of the principles appears to be initially item-specific rather than general to all number words, and is most robust for very small numbers (e.g., 5) compared to larger numbers (e.g., 25), even among children who can count much higher (e.g., above 30). In light of these findings, we conclude that there is little evidence to support the hypothesis that becoming a cardinal-principle knower involves a semantic induction over all items in a child's count list. ?? 2011 Elsevier B.V.},
author = {Davidson, Kathryn and Eng, Kortney and Barner, David},
doi = {10.1016/j.cognition.2011.12.013},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davidson, Eng, Barner - 2012 - Does learning to count involve a semantic induction.pdf:pdf},
isbn = {0010-0277},
issn = {00100277},
journal = {Cognition},
keywords = {Cardinal principle,Counting,Induction,Number knowledge acquisition},
number = {1},
pages = {162--173},
pmid = {22245033},
publisher = {Elsevier B.V.},
title = {{Does learning to count involve a semantic induction?}},
volume = {123},
year = {2012}
}
@article{Anderson1996,
abstract = {In the Adaptive Character of Thought (ACT-R) theory, complex cognition arises from an interaction of procedural and declarative knowledge. Procedural knowledge is represented in units called production rules, and declarative knowledge is represented in units called chunks. The individual units are created by simple encodings of objects in the environment (chunks) or simple encodings of transformations in the environment (production rules). A great many such knowledge units underlie human cognition. From this large database, the appropriate units are selected for a particular context by activation processes that are tuned to the statistical structure of the environment. According to the ACT-R theory, the power of human cognition depends on the amount of knowledge encoded and the effective deployment of the encoded knowledge.},
author = {Anderson, John R.},
doi = {10.1037//0003-066X.51.4.355},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anderson - 1996 - A Simple Theory of Complex Cognition.pdf:pdf},
isbn = {1935-990X},
issn = {0003066X},
journal = {American Psychologist},
number = {4},
pages = {355--365},
pmid = {155},
title = {{A Simple Theory of Complex Cognition}},
volume = {51},
year = {1996}
}
@article{Mason2012,
abstract = {Amazon's Mechanical Turk is an online labor market where requesters post jobs and workers choose which jobs to do for pay. The central purpose of this article is to demonstrate how to use this Web site for conducting behavioral research and to lower the barrier to entry for researchers who could benefit from this platform. We describe general techniques that apply to a variety of types of research and experiments across disciplines. We begin by discussing some of the advantages of doing experiments on Mechanical Turk, such as easy access to a large, stable, and diverse subject pool, the low cost of doing experiments, and faster iteration between developing theory and executing experiments. While other methods of conducting behavioral research may be comparable to or even better than Mechanical Turk on one or more of the axes outlined above, we will show that when taken as a whole Mechanical Turk can be a useful tool for many researchers. We will discuss how the behavior of workers compares with that of experts and laboratory subjects. Then we will illustrate the mechanics of putting a task on Mechanical Turk, including recruiting subjects, executing the task, and reviewing the work that was submitted. We also provide solutions to common problems that a researcher might face when executing their research on this platform, including techniques for conducting synchronous experiments, methods for ensuring high-quality work, how to keep data private, and how to maintain code security.},
archivePrefix = {arXiv},
arxivId = {http://ssrn.com/abstract=1691163},
author = {Mason, Winter and Suri, Siddharth},
doi = {10.3758/s13428-011-0124-6},
eprint = {/ssrn.com/abstract=1691163},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mason, Suri - 2012 - Conducting behavioral research on Amazon's Mechanical Turk.pdf:pdf},
isbn = {1554-351X},
issn = {1554-3528},
journal = {Behavior Research Methods},
keywords = {Behavioral Research,Data Collection,Humans,Research Design},
number = {1},
pages = {1--23},
pmid = {21717266},
primaryClass = {http:},
title = {{Conducting behavioral research on Amazon's Mechanical Turk}},
volume = {44},
year = {2012}
}
@article{Buhrmester2011,
author = {Buhrmester, M. and Kwang, T. and Gosling, S. D.},
doi = {10.1177/1745691610393980},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buhrmester, Kwang, Gosling - 2011 - Amazon's Mechanical Turk A New Source of Inexpensive, Yet High-Quality, Data.pdf:pdf},
isbn = {1745-6916},
issn = {1745-6916},
journal = {Perspectives on Psychological Science},
keywords = {amazon,amazon mechanical turk,com,data collection,getting work done by,here,internet,is a novel,mturk,online,open online marketplace for,others,research methods,s mechanical turk,web,www},
number = {1},
pages = {3--5},
pmid = {26162106},
title = {{Amazon's Mechanical Turk: A New Source of Inexpensive, Yet High-Quality, Data?}},
volume = {6},
year = {2011}
}
@article{OReilly2014,
abstract = {This paper reviews the fate of the central ideas behind the complementary learning systems (CLS) framework as originally articulated in McClelland, McNaughton, and O'Reilly (1995). This framework explains why the brain requires two differentially specialized learning and memory systems, and it nicely specifies their central properties (i.e., the hippocampus as a sparse, pattern-separated system for rapidly learning episodic memories, and the neocortex as a distributed, overlapping system for gradually integrating across episodes to extract latent semantic structure). We review the application of the CLS framework to a range of important topics, including the following: the basic neural processes of hippocampal memory encoding and recall, conjunctive encoding, human recognition memory, consolidation of initial hippocampal learning in cortex, dynamic modulation of encoding versus recall, and the synergistic interactions between hippocampus and neocortex. Overall, the CLS framework remains a vital theoretical force in the field, with the empirical data over the past 15 years generally confirming its key principles.},
author = {O'Reilly, Randall C. and Bhattacharyya, Rajan and Howard, Michael D. and Ketz, Nicholas},
doi = {10.1111/j.1551-6709.2011.01214.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Reilly et al. - 2014 - Complementary learning systems.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Consolidation,Hippocampus,Learning,Memory,Neocortex,Neural network models},
number = {6},
pages = {1229--1248},
pmid = {22141588},
title = {{Complementary learning systems}},
volume = {38},
year = {2014}
}
@article{Chinn2000,
abstract = {A systematic review may encompass both odds ratios and mean differences in continuous outcomes. A separate meta-analysis of each type of outcome results in loss of information and may be misleading. It is shown that a ln(odds ratio) can be converted to effect size by dividing by 1.81. The validity of effect size, the estimate of interest divided by the residual standard deviation, depends on comparable variation across studies. If researchers routinely report residual standard deviation, any subsequent review can combine both odds ratios and effect sizes in a single meta-analysis when this is justified.},
author = {Chinn, Susan},
doi = {10.1002/1097-0258(20001130)19:22<3127::AID-SIM784>3.0.CO;2-M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chinn - 2000 - A simple method for converting an odds ratio to effect size for use in meta-analysis.pdf:pdf},
isbn = {0277-6715 (Print)$\backslash$r0277-6715 (Linking)},
issn = {0277-6715},
journal = {Statistics in Medicine},
number = {22},
pages = {3127--3131},
pmid = {11113947},
title = {{A simple method for converting an odds ratio to effect size for use in meta-analysis}},
volume = {19},
year = {2000}
}
@article{Kass1995,
author = {Kass, Robert E. and Raftery, Adrian E.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kass, Raftery - 1995 - Bayes Factors.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {430},
pages = {773--795},
title = {{Bayes Factors}},
volume = {90},
year = {1995}
}
@incollection{Schwartz2015,
author = {Schwartz, Daniel L. and Goldstone, Robert L.},
booktitle = {Handbook of educational psychology},
chapter = {5},
edition = {3rd},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwartz, Goldstone - 2015 - Learning as Coordination.pdf:pdf},
title = {{Learning as Coordination}},
year = {2015}
}
@article{Gick1983,
abstract = {An analysis of the process of analogical thinking predicts that analogies will be noticed on the basis of semantic retrieval cues and that the induction of a general schema from concrete analogs will facilitate analogical transfer. These predictions were tested in experiments in which subjects first read one or more stories illustrating problems and their solutions and then attempted to solve a disparate but analogous transfer problem. The studies in Part I attempted to foster the abstraction of a problem schema from a single story analog by means of summarization instructions, a verbal statement of the underlying principle, or a diagrammatic representation of it. None of these devices achieved a notable degree of sucess. In contrast, the experiments in Part II demonstrated that if two prior analogs were given, subjects often derived a problem schema as an incidental product of describing the similarities of the analogs. The quality of the induced schema was highly predictive of subsequent transfer performance. Furthermore, the verbal statements and diagrams that had failed to facilitate transfer from one analog proved highly beneficial when paired with two. The function of examples in learning was discussed in light of the present study. {\textcopyright} 1983.},
author = {Gick, Mary L. and Holyoak, Keith J.},
doi = {10.1016/0010-0285(83)90002-6},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gick, Holyoak - 1983 - Schema induction and analogical transfer.pdf:pdf},
isbn = {0010-0285},
issn = {00100285},
journal = {Cognitive Psychology},
number = {1},
pages = {1--38},
title = {{Schema induction and analogical transfer}},
volume = {15},
year = {1983}
}
@article{Maurer2007,
author = {Maurer, Daphne and Mondloch, Catherine J and Lewis, Terri L},
doi = {10.1111/j.1467-7687.2007.00562.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maurer, Mondloch, Lewis - 2007 - Sleeper effects.pdf:pdf},
pages = {40--47},
title = {{Sleeper effects}},
volume = {1},
year = {2007}
}
@article{Hinton2011,
abstract = {The artificial neural networks that are used to recognise shapes typcially use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered representations of the pose of the feature, like SIFT, that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instatiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adpating the features to the domain},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Hinton, Geoffrey E. and Krizhevsky, Alex and Wang, Sida D.},
doi = {10.1007/978-3-642-21735-7_6},
eprint = {9605103},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Krizhevsky, Wang - 2011 - Transforming auto-encoders.pdf:pdf},
isbn = {9783642217340},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Invariance,auto-encoder,shape representation},
number = {PART 1},
pages = {44--51},
pmid = {1000183096},
primaryClass = {cs},
title = {{Transforming auto-encoders}},
volume = {6791 LNCS},
year = {2011}
}
@article{Gick1980,
author = {Gick, Mary L and Holyoak, Keith J.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gick, Holyoak - 1980 - Analogical Problem Solving.pdf:pdf},
journal = {Cognitive P},
pages = {306--355},
title = {{Analogical Problem Solving}},
volume = {12},
year = {1980}
}
@article{Rogers2008,
author = {Rogers, Timothy T and McClelland, James L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rogers, McClelland - 2008 - Precis of Semantic Cognition A Parallel Distributed Processing Approach.pdf:pdf},
journal = {Behavioral and Brain Sciences},
keywords = {categorization,causal knowledge,concepts,connectionism,development,innateness,learning,memory,semantics},
pages = {689--749},
title = {{Precis of Semantic Cognition : A Parallel Distributed Processing Approach}},
volume = {31},
year = {2008}
}
@article{Sanborn2010,
author = {Sanborn, Adam N. and Griffiths, Thomas L. and Navarro, Daniel J.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanborn, Griffiths, Navarro - 2010 - Rational Approximations to Category Learning.pdf:pdf},
journal = {Psychological Review},
number = {4},
pages = {1144--1167},
title = {{Rational Approximations to Category Learning}},
volume = {117},
year = {2010}
}
@article{Sanborn2008,
abstract = {Many formal models of cognition implicitly use subjective probability distribu- tions to capture the assumptions of human learners. Most applications of these models determine these distributions indirectly. We propose a method for directly determining the assumptions of human learners by sampling from subjective prob- ability distributions. Using a correspondence between a model of human choice and Markov chain Monte Carlo (MCMC), we describe a method for sampling from the distributions over objects that people associate with different categories. In our task, subjects choose whether to accept or reject a proposed change to an object. The task is constructed so that these decisions follow an MCMC accep- tance rule, defining a Markov chain for which the stationary distribution is the category distribution. We test this procedure for both artificial categories acquired in the laboratory, and natural categories acquired from experience.},
author = {Sanborn, Adam N and Griffiths, Thomas L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanborn, Griffiths - 2008 - Markov Chain Monte Carlo with People.pdf:pdf},
journal = {Advances in neural information processing systems},
number = {47045},
pages = {1--8},
pmid = {5036717154477609753},
title = {{Markov Chain Monte Carlo with People}},
volume = {20},
year = {2008}
}
@article{Sanborn2006,
annote = {Order effects from Bayesian cognitive modeling as results of inference algorithms with finite capacity, etc., see also Sanborn's other shit},
author = {Sanborn, Adam N and Griffiths, Thomas L and Navarro, Daniel J.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanborn, Griffiths, Navarro - 2006 - A More Rational Model of Categorization.pdf:pdf},
journal = {Proceedings of the 28th Annual Conference of the Cognitive Science Society},
title = {{A More Rational Model of Categorization}},
year = {2006}
}
@article{Markant2014a,
abstract = {People can test hypotheses through either selection or reception. In a selection task, the learner actively chooses observations to test his or her beliefs, whereas in reception tasks data are passively encountered. People routinely use both forms of testing in everyday life, but the critical psychological differences between selection and reception learning remain poorly understood. One hypothesis is that selection learning improves learning performance by enhancing generic cognitive processes related to motivation, attention, and engagement. Alternatively, we suggest that differences between these 2 learning modes derives from a hypothesis-dependent sampling bias that is introduced when a person collects data to test his or her own individual hypothesis. Drawing on influential models of sequential hypothesis-testing behavior, we show that such a bias (a) can lead to the collection of data that facilitates learning compared with reception learning and (b) can be more effective than observing the selections of another person. We then report a novel experiment based on a popular category learning paradigm that compares reception and selection learning. We additionally compare selection learners to a set of â€œyokedâ€ participants who viewed the exact same sequence of observations under reception conditions. The results revealed systematic differences in performance that depended on the learner's role in collecting information and the abstract structure of the problem.},
author = {Markant, Douglas B. and Gureckis, Todd M.},
doi = {10.1037/a0032108},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Markant, Gureckis - 2014 - Is it better to select or to receive Learning via active and passive hypothesis testing(2).pdf:pdf},
isbn = {1939-2222 (Electronic)$\backslash$n0022-1015 (Linking)},
issn = {1939-2222},
journal = {Journal of Experimental Psychology: General},
keywords = {bayesian modeling,category learning,dependent sampling bias,hypothesis testing,hypothesis-,self-directed learning},
number = {1},
pages = {94--122},
pmid = {23527948},
title = {{Is it better to select or to receive? Learning via active and passive hypothesis testing}},
volume = {143},
year = {2014}
}
@article{Johnson2016,
author = {Johnson, Mark H},
doi = {10.1080/17470218.2011.590596},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson - 2016 - Face processing as a brain adaptation at multiple timescales EPS Mid-Career Award 2010 Face processing as a brain adapt.pdf:pdf},
journal = {The Quarterly Journal of Experimental Psychology},
number = {September},
title = {{Face processing as a brain adaptation at multiple timescales EPS Mid-Career Award 2010 Face processing as a brain adaptation at multiple timescales}},
volume = {0218},
year = {2016}
}
@article{Rich2015,
author = {Rich, Alexander S and Gureckis, Todd M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rich, Gureckis - 2015 - The Attentional Learning Trap and How to Avoid It.pdf:pdf},
journal = {Proceedings of the 37th Annual Conference of the Cognitive Science Society.},
keywords = {approach-avoid behavior,attention and the hot,biases,categorization,decision-making,effect as a prob-,learning traps,on the hot stove,past work has focused,selective atten-,stove effect,tion},
title = {{The Attentional Learning Trap and How to Avoid It}},
year = {2015}
}
@article{Markant2015,
author = {Markant, Douglas B and Settles, Burr and Gureckis, M},
doi = {10.1111/cogs.12220},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Markant, Settles, Gureckis - 2015 - Self-Directed Learning Favors Local , Rather Than Global , Uncertainty.pdf:pdf},
journal = {Cognitive Science},
keywords = {active learning,information sampling,machine learning,self-directed learning},
number = {1},
pages = {1--21},
title = {{Self-Directed Learning Favors Local , Rather Than Global , Uncertainty}},
volume = {40},
year = {2015}
}
@article{Bates2015,
abstract = {The analysis of experimental data with mixed-effects models requires decisions about the specifica- tion of the appropriate random-effects structure. Recently, Barr et al. (2013) recommended fitting â€˜maximal' models with all possible random effect components included. Estimation of maximal models, however, may not converge. We show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too com- plex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on Bayesian hierarchical modeling with uninformative or weakly informative priors. Importantly, even under convergence, overparameterization may lead to uninterpretable models. We provide diagnostic tools for detecting overparameterization and guiding model simplification. Finally, we clarify that the simulations on which Barr et al. base their recommendations are atyp- ical for real data. A detailed example is provided of how subject-related attentional fluctuation across trials may further qualify statistical inferences about fixed effects, and of how such nonlinear effects can be accommodated within the mixed-effects modeling framework.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.04967v1},
author = {Bates, Douglas M and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald},
doi = {arXiv:1506.04967},
eprint = {arXiv:1506.04967v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bates et al. - 2015 - Parsimonious mixed models.pdf:pdf},
journal = {arXiv preprint arXiv:1506.04967},
keywords = {crossed random effects,generalized additive mixed models,linear mixed models,model selection,model simplicity},
pages = {1--27},
title = {{Parsimonious mixed models}},
year = {2015}
}
@article{Westfall2016,
author = {Westfall, Jacob and Yarkoni, Tal},
doi = {10.1371/journal.pone.0152719},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Westfall, Yarkoni - 2016 - Statistically Controlling for Confounding Constructs Is Harder than You Think.pdf:pdf},
pages = {1--22},
title = {{Statistically Controlling for Confounding Constructs Is Harder than You Think}},
year = {2016}
}
@article{Ganesh2014,
abstract = {Background: Cataracts are a major cause of childhood blindness globally. Although surgically treatable, it is unclear whether children would benefit from such interventions beyond the first few years of life, which are believed to constitute 'critical' periods for visual development. Aims To study visual acuity outcomes after late treatment of early-onset cataracts and also to determine whether there are longitudinal changes in postoperative acuity. Methods: We identified 53 children with dense cataracts with an onset within the first half-year after birth through a survey of over 20 000 rural children in India. All had accompanying nystagmus and were older than 8 years of age at the time of treatment. They underwent bilateral cataract surgery and intraocular lens implantation. We then assessed their best-corrected visual acuity 6 weeks and 6 months after surgery. Results: 48 children from the pool of 53 showed improvement in their visual acuity after surgery. Our longitudinal assessments demonstrated further improvements in visual acuity for the majority of these children proceeding from the 6-week to 6-month assessment. Interestingly, older children in our subject pool did not differ significantly from the younger ones in the extent of improvement they exhibit. Conclusions: and relevance Our results demonstrate that not only can significant vision be acquired until late in childhood, but that neural processes underlying even basic aspects of vision like resolution acuity remain malleable until at least adolescence. These data argue for the provision of cataract treatment to all children, irrespective of their age.},
author = {Ganesh, S and Arora, P and Sethi, S and Gandhi, T K and Kalia, A and Chatterjee, G and Sinha, P},
doi = {10.1136/bjophthalmol-2013-304475},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganesh et al. - 2014 - Results of late surgical intervention in children with early-onset bilateral cataracts.pdf:pdf},
issn = {0007-1161},
journal = {British Journal of Ophthalmology},
keywords = {age,article,best corrected visual acuity,cataract extraction,cataract/su [Surgery],child,disease association,early onset bilateral cataract/su [Surgery],female,follow up,human,lens implantation,longitudinal study,major clinical study,male,nystagmus,onset age,optical low vision aid,outcome assessment,prospective study,therapy delay,time to treatment,visual acuity,visual impairment},
number = {10},
pages = {1424--1428},
pmid = {2014737030},
title = {{Results of late surgical intervention in children with early-onset bilateral cataracts}},
volume = {98},
year = {2014}
}
@article{Judd2016,
author = {Judd, Charles M and Westfall, Jacob and Kenny, David A.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Judd, Westfall, Kenny - 2016 - Experiments with More Than One Factor Designs, Analytic Models, and Statistical Power.pdf:pdf},
pages = {1--59},
title = {{Experiments with More Than One Factor: Designs, Analytic Models, and Statistical Power}},
year = {2016}
}
@article{Vinyals2016,
abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6{\%} to 93.2{\%} and from 88.0{\%} to 93.8{\%} on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
archivePrefix = {arXiv},
arxivId = {1606.04080},
author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1606.04080},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vinyals et al. - 2016 - Matching Networks for One Shot Learning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Matching Networks for One Shot Learning}},
year = {2016}
}
@article{Knudsen1984,
abstract = {We studied the ability of barn owls to recover accurate sound localization after being raised with one ear occluded. Most of the owls had ear plugs inserted before they reached adult size, and therefore they never experienced normal adult localization cues until their ear plugs were removed. Upon removal of their ear plugs, these owls exhibited large systematic sound localization errors. The rate at which they recovered accurate localization decreased with the age of the bird at the time of plug removal, and recovery essentially ceased when owls reached 38 to 42 weeks of age. We interpret this age as the end of a critical period for the consolidation of associations between auditory cues and locations in space. Owls that had experienced adult localization cues for a short period of time before ear plugging recovered normal accuracy rapidly, even if they remained plugged well past the end of the critical period. This suggests that a brief exposure to normal adult cues early in the critical period is sufficient to enable the recovery of localization accuracy much later in life.},
author = {Knudsen, F and Knudsen, P.F and Esterly, S.D.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Knudsen, Knudsen, Esterly - 1984 - A critical period for the recovery of sound localization accuracy following monaural occlusion in the.pdf:pdf},
isbn = {0270-6474 (Print)},
issn = {0270-6474; 0270-6474},
journal = {Journal of Neuroscience},
number = {4},
pages = {1012--1020},
pmid = {6716128},
title = {{A critical period for the recovery of sound localization accuracy following monaural occlusion in the barn owl}},
volume = {4},
year = {1984}
}
@article{Brainard1998,
abstract = {Previous studies have identified sensitive periods for the developing barn owl during which visual experience has a powerful influence on the calibration of sound localization behavior. Here we investigated neural correlates of these sensitive periods by assessing developmental changes in the capacity of visual experience to alter the map of auditory space in the optic tectum of the barn owl. We used two manipulations. (1) We equipped owls with prismatic spectacles that optically displaced the visual field by 23 degrees to the left or right, and (2) we restored normal vision to prism-reared owls that had been raised wearing prisms. In agreement with previous behavioral experiments, we found that the capacity of abnormal visual experience to shift the tectal auditory space map was restricted to an early sensitive period. However, this period extended until later in life (approximately 200 d) than described previously in behavioral studies (approximately 70 d). Furthermore, unlike the previous behavioral studies that found that the capacity to recover normal sound localization after restoration of normal vision was lost at approximately 200 d of age, we found that the capacity to recover a normal auditory space map was never lost. Finally, we were able to reconcile the behaviorally and neurophysiologically defined sensitive periods by taking into account differences in the richness of the environment in the two sets of experiments. We repeated the behavioral experiments and found that when owls were housed in a rich environment, the capacity to adjust sound localization away from normal extended to later in life, whereas the capacity to recover to normal was never lost. Conversely, when owls were housed in an impoverished environment, the capacity to recover a normal auditory space map was restricted to a period ending at approximately 200 d of age. The results demonstrate that the timing and even the existence of sensitive periods for plasticity of a neural circuit and associated behavior can depend on multiple factors, including (1) the nature of the adjustment demanded of the system and (2) the richness of the sensory and social environment in which the plasticity is studied.},
author = {Brainard, Michael S. and Knudsen, Eric I.},
doi = {papers://47831562-1F78-4B52-B52E-78BF7F97A700/Paper/p11},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brainard, Knudsen - 1998 - Sensitive periods for visual calibration of the auditory space map in the barn owl optic tectum.pdf:pdf},
isbn = {0270-6474},
issn = {0270-6474},
journal = {The Journal of Neuroscience},
keywords = {auditory map,auditory plastic-,barn owl,critical period,ity,knudsen and knud-,of the auditory stimulus,optic tectum,sensitive period,sound localization,space map,superior colliculus,through the prisms,tyto alba},
number = {10},
pages = {3929--3942},
pmid = {9570820},
title = {{Sensitive periods for visual calibration of the auditory space map in the barn owl optic tectum.}},
volume = {18},
year = {1998}
}
@article{Afraz2014,
abstract = {Invariant visual object recognition and the underlying neural representations are fundamental to higher-level human cognition. To understand these neural underpinnings, we combine human and monkey psychophysics, large-scale neurophysiology, neural perturbation methods, and computational modeling to construct falsifiable, predictive models that aim to fully account for the neural encoding and decoding processes that underlie visual object recognition. A predictive encoding model must minimally describe the transformation of the retinal image to population patterns of neural activity along the entire cortical ventral stream of visual processing and must accurately predict the responses to any retinal image. A predictive decoding model must minimally describe the transformation from those population patterns of neural activity to observed object recognition behavior (i.e., subject reports), and, given that population pattern of activity, it must accurately predict behavior for any object recognition task. To date, we have focused on core object recognition-a remarkable behavior that is accomplished with image viewing durations of {\textless}200 msec. Our work thus far reveals that the neural encoding process is reasonably well explained by a largely feed-forward, highly complex, multistaged nonlinear neural network-the current best neuronal simulation models predict approximately one-half of the relevant neuronal response variance across the highest levels of the ventral stream (areas V4 and IT). Remarkably, however, the decoding process from IT to behavior for all object recognition tasks tested thus far is very accurately predicted by simple direct linear conversion of the inferior temporal neural population state to behavior choice. We have recently examined the behavioral consequences of direct suppression of IT neural activity using pharmacological and optogenetic methods and find them to be well-explained by the same linear decoding model.},
author = {Afraz, Arash and Yamins, Daniel L K and DiCarlo, James J},
doi = {10.1101/sqb.2014.79.024729},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Afraz, Yamins, DiCarlo - 2014 - Neural Mechanisms Underlying Visual Object Recognition.pdf:pdf},
issn = {1943-4456 (Electronic)},
journal = {Cold Spring Harbor Symposia on Quantitative Biology},
pages = {99--107},
pmid = {26092883},
title = {{Neural Mechanisms Underlying Visual Object Recognition.}},
volume = {79},
year = {2014}
}
@article{West1987,
abstract = {All organisms inherit parents' genes, but many also inherit parents, peers, and the places they inhabit as well. We suggest the term ontogenetic niche to signify the ecological and social legacies that accompany genes. A formal name is needed to give the idea of the inherited environment equal status with its conceptual cognates; nature and nurture. We argue here that increased recognition of the inherited environment facilitates unification efforts within the developmental sciences by emphasizing the affinity, rather than opposability, of ontogenetic processes.},
author = {West, M J and King, a P},
doi = {10.1002/dev.420200508},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/West, King - 1987 - Settling nature and nurture into an ontogenetic niche.pdf:pdf},
isbn = {0012-1630 (Print)$\backslash$n0012-1630 (Linking)},
issn = {0012-1630},
journal = {Developmental psychobiology},
number = {April},
pages = {549--562},
pmid = {3678619},
title = {{Settling nature and nurture into an ontogenetic niche.}},
volume = {20},
year = {1987}
}
@article{Morishita2008,
abstract = {Neural circuits are shaped by experience in early postnatal life. The permanent loss of visual acuity (amblyopia) and anatomical remodeling within primary visual cortex following monocular deprivation is a classic example of critical period development from mouse to man. Recent work in rodents reveals a residual subthreshold potentiation of open eye response throughout life. Resetting excitatory-inhibitory balance or removing molecular 'brakes' on structural plasticity may unmask the potential for recovery of function in adulthood. Novel pharmacological or environmental interventions now hold great therapeutic promise based on a deeper understanding of critical period mechanisms. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Morishita, Hirofumi and Hensch, Takao K.},
doi = {10.1016/j.conb.2008.05.009},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morishita, Hensch - 2008 - Critical period revisited impact on vision.pdf:pdf},
isbn = {0959-4388 (Print)$\backslash$n0959-4388 (Linking)},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
number = {1},
pages = {101--107},
pmid = {18534841},
title = {{Critical period revisited: impact on vision}},
volume = {18},
year = {2008}
}
@article{Lewis2002,
abstract = {The question of whether age-of-acquisition (AoA), frequency, and repetition priming effects occur at a common stage or at different stages of processing is addressed. Two single-stage accounts (i.e., cumulative frequency and a neural-network simulation) are considered in regard to their predictions concerning the interactions between AoA and frequency with aging and priming effects. A repetition-priming face-classification task was conducted on both older and younger participants to test these predictions. Consistent with the predictions of the neural-network simulation, AoA had an effect on reaction times that could not be explained by cumulative frequency alone. Also, as predicted by the simulation, the size of the priming effect was determined by the cumulative frequency of the item. It is discussed how this evidence is supportive of the notion that AoA , frequency, and priming all have effects at a common and single stage during face processing.},
author = {Lewis, Michael B and Chadwick, Andrea J and Ellis, Hadyn D},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lewis, Chadwick, Ellis - 2002 - Exploring a neural-network account of age-of-acquisition effects using repetition priming of faces.pdf:pdf},
isbn = {0090-502X (Print)$\backslash$r0090-502X (Linking)},
issn = {0090-502X},
journal = {Memory {\&} Cognition},
keywords = {80 and over,Adolescent,Adult,Age Factors,Aged,Aging,Face,Female,Humans,Learning,Male,Middle Aged,Nerve Net,Nerve Net: physiology,Periodicity,Photic Stimulation,Reaction Time,Recognition (Psychology)},
number = {8},
pages = {1228--1237},
pmid = {12661854},
title = {{Exploring a neural-network account of age-of-acquisition effects using repetition priming of faces.}},
volume = {30},
year = {2002}
}
@article{Yamins2016,
abstract = {Propelled by advances in biologically inspired computer vision and artificial intelligence, the past five years have seen significant progress in using deep neural networks to model response patterns of neurons in visual cortex. In this paper, we briefly review this progress and then discuss eight key 'open questions' that we believe will drive research in computational models of sensory systems over the next five years, both in visual cortex and beyond. Any scientific development of long-term value opens up as many new questions as it answers. This is certainly the case with recent progress in building deep neural network models of visual cortex. In this piece, our goal is to briefly describe these recent advances, and to outline what we consider to be the most interesting open problems in cortical modeling, both in vision and beyond. We focus is on questions that will require both cutting-edge algorith-mic developments as well as next-generation neurosci-ence and cognitive science experiments.},
author = {Yamins, Daniel LK and DiCarlo, James J},
doi = {10.1016/j.conb.2016.02.001},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yamins, DiCarlo - 2016 - Eight open questions in the computational modeling of higher sensory cortex Brief review of recent progress.pdf:pdf},
isbn = {1873-6882 (Electronic) 0959-4388 (Linking)},
issn = {1873-6882},
journal = {Current Opinion in Neurobiology},
pages = {114--120},
pmid = {26921828},
title = {{Eight open questions in the computational modeling of higher sensory cortex}},
volume = {37},
year = {2016}
}
@article{Landy2007,
abstract = {Although a general sense of the magnitude, quantity, or numerosity of objects is common in both untrained people and animals, the abilities to deal exactly with large quantities and to reason precisely in complex but well-specified situations--to behave formally, that is--are skills unique to people trained in symbolic notations. These symbolic notations typically employ complex, hierarchically embedded structures, which all extant analyses assume are constructed by concatenative, rule-based processes. The primary goal of this article is to establish, using behavioral measures on naturalistic tasks, that some of the same cognitive resources involved in representing spatial relations and proximities are also involved in representing symbolic notations--in short, that formal notations are a kind of diagram. We examined self-generated productions in the domains of handwritten arithmetic expressions and typewritten statements in a formal logic. In both tasks, we found substantial evidence for spatial representational schemes even in these highly symbolic domains.},
author = {Landy, David and Goldtone, Robert L},
doi = {10.3758/BF03192935},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Landy, Goldtone - 2007 - Formal notations are diagrams evidence from a production task.pdf:pdf},
isbn = {0090-502X},
issn = {0090-502X},
journal = {Memory {\&} cognition},
number = {8},
pages = {2033--2040},
pmid = {18265618},
title = {{Formal notations are diagrams: evidence from a production task.}},
volume = {35},
year = {2007}
}
@article{Keuroghlian2007,
abstract = {Enormous progress has been made in our understanding of adaptive plasticity in the central auditory system. Experiments on a range of species demonstrate that, in adults, the animal must attend to (i.e., respond to) a stimulus in order for plasticity to be induced, and the plasticity that is induced is specific for the acoustic feature to which the animal has attended. The requirement that an adult animal must attend to a stimulus in order for adaptive plasticity to occur suggests an essential role of neuromodulatory systems in gating plasticity in adults. Indeed, neuromodulators, particularly acetylcholine (ACh), that are associated with the processes of attention, have been shown to enable adaptive plasticity in adults. In juvenile animals, attention may facilitate plasticity, but it is not always required: during sensitive periods, mere exposure of an animal to an atypical auditory environment can result in large functional changes in certain auditory circuits. Thus, in both the developing and mature auditory systems substantial experience-dependent plasticity can occur, but the conditions under which it occurs are far more stringent in adults. We review experimental results that demonstrate experience-dependent plasticity in the central auditory representations of sound frequency, level and temporal sequence, as well as in the representations of binaural localization cues in both developing and adult animals. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Keuroghlian, Alex S. and Knudsen, Eric I.},
doi = {10.1016/j.pneurobio.2007.03.005},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Keuroghlian, Knudsen - 2007 - Adaptive auditory plasticity in developing and adult animals.pdf:pdf},
isbn = {0301-0082 (Print)$\backslash$r0301-0082 (Linking)},
issn = {03010082},
journal = {Progress in Neurobiology},
keywords = {Adult,Attention,Auditory system,Experience,Juvenile,Neuromodulators,Plasticity,Sensitive period},
number = {3},
pages = {109--121},
pmid = {17493738},
title = {{Adaptive auditory plasticity in developing and adult animals}},
volume = {82},
year = {2007}
}
@article{Fausey2016,
abstract = {Human development takes place in a social context. Two pervasive sources of social information are faces and hands. Here, we provide the first report of the visual frequency of faces and hands in the everyday scenes available to infants. These scenes were collected by having infants wear head cameras during unconstrained everyday activities. Our corpus of 143hours of infant-perspective scenes, collected from 34 infants aged 1month to 2years, was sampled for analysis at 1/5Hz. The major finding from this corpus is that the faces and hands of social partners are not equally available throughout the first two years of life. Instead, there is an earlier period of dense face input and a later period of dense hand input. At all ages, hands in these scenes were primarily in contact with objects and the spatio-temporal co-occurrence of hands and faces was greater than expected by chance. The orderliness of the shift from faces to hands suggests a principled transition in the contents of visual experiences and is discussed in terms of the role of developmental gates on the timing and statistics of visual experiences.},
author = {Fausey, Caitlin M and Jayaraman, Swapnaa and Smith, Linda B},
doi = {10.1016/j.cognition.2016.03.005},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fausey, Jayaraman, Smith - 2016 - From faces to hands Changing visual input in the first two years.pdf:pdf},
issn = {1873-7838},
journal = {Cognition},
keywords = {Egocentric vision,Faces,Hands,Head camera,Infancy,Scene statistics},
pages = {101--107},
pmid = {27043744},
publisher = {Elsevier B.V.},
title = {{From faces to hands: Changing visual input in the first two years.}},
volume = {152},
year = {2016}
}
@article{Landy2010,
abstract = {How does the physical structure of an arithmetic expression affect the computational processes engaged in by reasoners? In handwritten arithmetic expressions containing both multiplications and additions, terms that are multiplied are often placed physically closer together than terms that are added. Three experiments evaluate the role such physical factors play in how reasoners construct solutions to simple compound arithmetic expressions (such as "2 + 3 Ã— 4"). Two kinds of influence are found: First, reasoners incorporate the physical size of the expression into numerical responses, tending to give larger responses to more widely spaced problems. Second, reasoners use spatial information as a cue to hierarchical expression structure: More narrowly spaced subproblems within an expression tend to be solved first and tend to be multiplied. Although spatial relationships besides order are entirely formally irrelevant to expression semantics, reasoners systematically use these relationships to support their success with various formal properties.},
author = {Landy, David and Goldstone, Robert L},
doi = {10.1080/17470211003787619},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Landy, Goldstone - 2010 - Proximity and precedence in arithmetic.pdf:pdf},
isbn = {1747-0226 (Electronic)$\backslash$n1747-0218 (Linking)},
issn = {1747-0218},
journal = {The Quarterly Journal of Experimental Psychology},
keywords = {challenges facing the cognitive,embodied cognition,how do people use,interpretation,is symbolic,mathematical cognition,one of the central,study of mathematical reasoning,symbol systems,symbolic reasoning},
number = {10},
pages = {1953--1968},
pmid = {20509096},
title = {{Proximity and precedence in arithmetic.}},
volume = {63},
year = {2010}
}
@article{Jayaraman2015,
abstract = {Mature face perception has its origins in the face experiences of infants. However, little is known about the basic statistics of faces in early visual environments. We used head- cameras to capture and analyze over 72,000 infant-perspective scenes from 22 infants aged 1-11 months as they engaged in daily activities. The frequency of faces in these scenes declined markedly with age: for the youngest infants, faces were present 15 minutes in every waking hour but only 5 minutes for the oldest infants. In general, the available faces were well characterized by three properties: (1) they belonged to relatively few individuals; (2) they were close and visually large; and (3) they presented views showing both eyes. These three properties most strongly characterized the face corpora of our youngest infants and constitute environmental constraints on the early development of the visual system.},
author = {Jayaraman, Swapnaa and Fausey, Caitlin M. and Smith, Linda B.},
doi = {10.1371/journal.pone.0123780},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jayaraman, Fausey, Smith - 2015 - The faces in infant-perspective scenes change over the first year of life.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {5},
pages = {13--15},
title = {{The faces in infant-perspective scenes change over the first year of life}},
volume = {10},
year = {2015}
}
@article{Gottlieb1991,
author = {Gottlieb, Gilbert and Horton, Margaret and Rodriguiz, Ramona and Iii, Wayne Kelly},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gottlieb et al. - 1991 - Experiential Canalization of Behavioral Development Results.pdf:pdf},
number = {1},
pages = {35--39},
title = {{Experiential Canalization of Behavioral Development : Results}},
volume = {27},
year = {1991}
}
@article{Fong2013,
abstract = {In this dissertation we develop a new formal graphical framework for causal reasoning. Starting with a review of monoidal categories and their associated graphical languages, we then revisit probability theory from a categorical perspective and introduce Bayesian networks, an existing structure for describing causal relationships. Motivated by these, we propose a new algebraic structure, which we term a causal theory. These take the form of a symmetric monoidal category, with the objects representing variables and morphisms ways of deducing information about one variable from another. A major advantage of reasoning with these structures is that the resulting graphical representations of morphisms match well with intuitions for flows of information between these variables. These categories can then be modelled in other categories, providing concrete interpretations for the variables and morphisms. In particular, we shall see that models in the category of measurable spaces and stochastic maps provide a slight generalisation of Bayesian networks, and naturally form a category themselves. We conclude with a discussion of this category, classifying the morphisms and discussing some basic universal constructions. ERRATA: (i) Pages 41-42: Objects of a causal theory are words, not collections, in {\$}V{\$}, and we include swaps as generating morphisms, subject to the identities defining a symmetric monoidal category. (ii) Page 46: A causal model is a strong symmetric monoidal functor.},
archivePrefix = {arXiv},
arxivId = {1301.6201},
author = {Fong, Brendan},
eprint = {1301.6201},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong - 2013 - Causal Theories A Categorical Perspective on Bayesian Networks.pdf:pdf},
pages = {72},
title = {{Causal Theories: A Categorical Perspective on Bayesian Networks}},
year = {2013}
}
@article{Singh2004,
abstract = {Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a specific problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous en- tities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing arti- ficial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy.},
author = {Singh, S. and Barto, A.G. and Chentanez, N.},
doi = {10.1109/TAMD.2010.2051031},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh, Barto, Chentanez - 2004 - Intrinsically motivated reinforcement learning.pdf:pdf},
isbn = {0262195348, 9780262195348},
issn = {1943-0604},
journal = {18th Annual Conference on Neural Information Processing Systems (NIPS)},
number = {2},
pages = {1281--1288},
title = {{Intrinsically motivated reinforcement learning}},
volume = {17},
year = {2004}
}
@article{Bottou1998,
abstract = {An abstract is not available.},
author = {Bottou, L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bottou - 1998 - Online learning and stochastic approximations.pdf:pdf},
isbn = {978-0521117913},
journal = {On-line learning in neural networks},
pages = {1--34},
title = {{Online learning and stochastic approximations}},
year = {1998}
}
@article{Doumas2008,
abstract = {Relational thinking plays a central role in human cognition. However, it is not known how children and adults acquire relational concepts and come to represent them in a form that is useful for the purposes of relational thinking (i.e., as structures that can be dynamically bound to arguments). The authors present a theory of how a psychologically and neurally plausible cognitive architecture can discover relational concepts from examples and represent them as explicit structures (predicates) that can take arguments (i.e., predicate them). The theory is instantiated as a computer program called DORA (Discovery Of Relations by Analogy). DORA is used to simulate the discovery of novel properties and relations, as well as a body of empirical phenomena from the domain of relational learning and the development of relational representations in children and adults.},
author = {Doumas, Leonidas A. A. and Hummel, John E and Sandhofer, Catherine M},
doi = {10.1037/0033-295X.115.1.1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doumas, Hummel, Sandhofer - 2008 - A theory of the discovery and predication of relational concepts.pdf:pdf},
isbn = {0033-295X$\backslash$r1939-1471},
issn = {0033-295X},
journal = {Psychological review},
keywords = {cognitive develop-,learning relations,learning structured representations,relation discovery},
number = {1},
pages = {1--43},
pmid = {18211183},
title = {{A theory of the discovery and predication of relational concepts.}},
volume = {115},
year = {2008}
}
@article{Ostrovsky2006,
abstract = {Animal studies suggest that early visual deprivation can cause permanent functional blindness. However, few human data on this issue exist. Given enough time for recovery, can a person gain visual skills after several years of congenital blindness? In India, we recently had an unusual opportunity to work with an individual whose case history sheds light on this question. S.R.D. was born blind, and remained so until age 12. She then underwent surgery for the removal of dense congenital cataracts. We evaluated her performance on an extensive battery of visual tasks 20 years after surgery. We found that although S.R.D.'s acuity is compromised, she is proficient on mid- and high-level visual tasks. These results suggest that the human brain retains an impressive capacity for visual learning well into late childhood. They have implications for current conceptions of cortical plasticity and provide an argument for treating congenital blindness even in older children.},
author = {Ostrovsky, Yuri and Andalman, Aaron and Sinha, Pawan},
doi = {10.1111/j.1467-9280.2006.01827.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ostrovsky, Andalman, Sinha - 2006 - Vision following extended congenital blindness.pdf:pdf},
isbn = {0956-7976},
issn = {09567976},
journal = {Psychological Science},
number = {12},
pages = {1009--1014},
pmid = {17201779},
title = {{Vision following extended congenital blindness}},
volume = {17},
year = {2006}
}
@article{Held2011,
abstract = {Would a blind subject, on regaining sight, be able to immediately visually recognize an object previously known only by touch? We addressed this question, first formulated by Molyneux three centuries ago, by working with treatable, congenitally blind individuals. We tested their ability to visually match an object to a haptically sensed sample after sight restoration. We found a lack of immediate transfer, but such cross-modal mappings developed rapidly.},
author = {Held, Richard and Ostrovsky, Yuri and de Gelder, Beatrice and DeGelder, Beatrice and Gandhi, Tapan and Ganesh, Suma and Mathur, Umang and Sinha, Pawan},
doi = {10.1038/nn.2795},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Held et al. - 2011 - The newly sighted fail to match seen with felt.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$r1097-6256 (Linking)},
issn = {1546-1726},
journal = {Nature neuroscience},
keywords = {Adolescent,Blindness,Blindness: congenital,Blindness: physiopathology,Blindness: psychology,Child,Choice Behavior,Choice Behavior: physiology,Female,Humans,Male,Photic Stimulation,Photic Stimulation: methods,Sensory Deprivation,Sensory Deprivation: physiology,Touch,Touch: physiology,Vision, Ocular,Vision, Ocular: physiology,Visual Perception,Visual Perception: physiology},
number = {5},
pages = {551--3},
pmid = {21478887},
title = {{The newly sighted fail to match seen with felt.}},
volume = {14},
year = {2011}
}
@article{Lake2016,
abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving perfor- mance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recog- nition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
archivePrefix = {arXiv},
arxivId = {1604.00289},
author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
eprint = {1604.00289},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake et al. - 2017 - Building Machines that learn and think like people.pdf:pdf},
journal = {Behavioral and Brain Sciences},
pages = {1--55},
pmid = {1000303116},
title = {{Building Machines that learn and think like people}},
year = {2017}
}
@article{Andrychowicz2016,
abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
archivePrefix = {arXiv},
arxivId = {1606.04474},
author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and de Freitas, Nando},
eprint = {1606.04474},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient descent.pdf:pdf},
pages = {1--16},
title = {{Learning to learn by gradient descent by gradient descent}},
year = {2016}
}
@article{Ostrovsky2009,
abstract = {How the visual system comes to bind diverse image regions into whole objects is not well understood. We recently had a unique opportunity to investigate this issue when we met three congenitally blind individuals in India. After providing them treatment, we studied the early stages of their visual skills. We found that prominent figural cues of grouping, such as good continuation and junction structure, were largely ineffective for image parsing. By contrast, motion cues were of profound significance in that they enabled intraobject integration and facilitated the development of object representations that permitted recognition in static images. Following 10 to 18 months of visual experience, the individuals' performance improved, and they were able to use the previously ineffective static figural cues to correctly parse many static scenes. These results suggest that motion information plays a fundamental role in organizing early visual experience and that parsing skills can be acquired even late in life.},
author = {Ostrovsky, Yuri and Meyers, Ethan and Ganesh, Suma and Mathur, Umang and Sinha, Pawan},
doi = {10.1111/j.1467-9280.2009.02471.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ostrovsky et al. - 2009 - Visual parsing after recovery from blindness.pdf:pdf},
isbn = {0956-7976},
issn = {09567976},
journal = {Psychological Science},
number = {12},
pages = {1484--1491},
pmid = {19891751},
title = {{Visual parsing after recovery from blindness}},
volume = {20},
year = {2009}
}
@article{Mante2013,
abstract = {Prefrontal cortex is thought to have a fundamental role in flexible, context-dependent behaviour, but the exact nature of the computations underlying this role remains largely unknown. In particular, individual prefrontal neurons often generate remarkably complex responses that defy deep understanding of their contribution to behaviour. Here we study prefrontal cortex activity in macaque monkeys trained to flexibly select and integrate noisy sensory inputs towards a choice. We find that the observed complexity and functional roles of single neurons are readily understood in the framework of a dynamical process unfolding at the level of the population. The population dynamics can be reproduced by a trained recurrent neural network, which suggests a previously unknown mechanism for selection and integration of task-relevant inputs. This mechanism indicates that selection and integration are two aspects of a single dynamical process unfolding within the same prefrontal circuits, and potentially provides a novel, general framework for understanding context-dependent computations.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Mante, Valerio and Sussillo, David and Shenoy, Krishna V and Newsome, William T},
doi = {10.1038/nature12742},
eprint = {15334406},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mante et al. - 2013 - Context-dependent computation by recurrent dynamics in prefrontal cortex.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$n0028-0836 (Linking)},
issn = {1476-4687},
journal = {Nature},
keywords = {Animals,Choice Behavior,Choice Behavior: physiology,Discrimination Learning,Macaca mulatta,Macaca mulatta: physiology,Male,Models, Neurological,Nerve Net,Nerve Net: cytology,Nerve Net: physiology,Neurons,Neurons: physiology,Prefrontal Cortex,Prefrontal Cortex: cytology,Prefrontal Cortex: physiology},
number = {7474},
pages = {78--84},
pmid = {24201281},
publisher = {Nature Publishing Group},
title = {{Context-dependent computation by recurrent dynamics in prefrontal cortex.}},
volume = {503},
year = {2013}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--13},
title = {{Adam: A Method for Stochastic Optimization}},
year = {2014}
}
@article{Deng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia Deng Jia and Dong, Wei Dong Wei and Socher, R. and Li, Li-Jia Li Li-Jia and Li, Kai Li Kai and Fei-Fei, Li Fei-Fei Li},
doi = {10.1109/CVPR.2009.5206848},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng et al. - 2009 - ImageNet A large-scale hierarchical image database.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
pages = {2--9},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
year = {2009}
}
@article{Braithwaite2012,
author = {Braithwaite, David W and Goldstone, Robert L},
journal = {Proceedings of the 34th Annual Conference of the Cognitive Science Society},
keywords = {analogy,comparison,instruction,mathematics,schemas,transfer},
number = {c},
pages = {138--143},
title = {{Inducing mathematical concepts from specific examples: The role of schema-level variation}},
year = {2012}
}
@article{PooleB.2016,
archivePrefix = {arXiv},
arxivId = {1606.05340},
author = {{et al. Poole, B.}},
eprint = {1606.05340},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/et al. Poole, B. - 2016 - Exponential expressivity in deep neural networks through transient chaos.pdf:pdf},
journal = {arXiv preprint arXiv:1606.05340v2},
title = {{Exponential expressivity in deep neural networks through transient chaos}},
year = {2016}
}
@article{Gross-Tsur1996,
abstract = {One hundred and forty-three 11-year-old children with development dyscalculia, from a cohort of 3029 students, were studied to determine demographic features and prevalence of this primary cognitive disorder. They were evaluated for gender, IQ, linguistic and perceptual skills, symptoms of attention-deficit hyperactivity disorder (ADHD), socio-economic status and associated learned disabilities. The IQs of the 140 children (75 girls and 65 boys) retained in the study group (three were excluded because of low IQs) ranged from 80 to 129 (mean 98.2, SD 9.9). 26 per cent of the children had symptoms of ADHD, and 17 per cent had dyslexia. Their socio-economic status was significantly lower than that of the rest of the cohort, and 42 per cent had first-degree relatives with learning disabilities. The prevalence of dyscalculia in the original cohort was 6.5 per cent, similar to that of dyslexia and ADHD. However, unlike these other learning disabilities, dyscalculia affected the two sexes in about the same proportions.},
author = {Gross-Tsur, V and Manor, O and Shalev, R S},
doi = {10.1007/s007870070009},
isbn = {1469-8749},
issn = {0012-1622},
journal = {Developmental medicine and child neurology},
keywords = {developmental dyscalculia,Ã° prevalence Ã° prognosis},
number = {1},
pages = {25--33},
pmid = {8606013},
title = {{Developmental dyscalculia: prevalence and demographic features.}},
volume = {38},
year = {1996}
}
@article{Blair2008,
abstract = {ABSTRACT â€” This article examines the role of working memo- ry, attention shifting, and inhibitory control executive cogni- tive functions in the development of mathematics knowledge and ability in children. It suggests that an examination of the executive cognitive demand of mathematical thinking can complement procedural and conceptual knowledge-based approaches to understanding the ways in which children become profi cient in mathematics. Task analysis indicates that executive cognitive functions likely operate in concert with procedural and conceptual knowledge and in some instances might act as a unique infl uence on mathematics problem-solving ability. It is concluded that consideration of the executive cognitive demand of mathematics can contribute to research on best practices in mathematics education.},
author = {Blair, Clancy and Knipe, Hilary and Gamson, David},
doi = {10.1111/j.1751-228X.2008.00036.x},
isbn = {1751-228X},
issn = {17512271},
journal = {Mind, Brain, and Education},
number = {2},
pages = {80--89},
title = {{Is there a role for executive functions in the development of mathematics ability?}},
volume = {2},
year = {2008}
}
@article{Matthews2015,
author = {Matthews, P G and Lewis, M R and Hubbard, E M},
doi = {10.1177/0956797615617799},
issn = {0956-7976},
journal = {Psychological Science},
keywords = {15,19,21,and a,educational psychology,individual differences,know it,mathematical ability,number comprehension,perception,received 6,revision accepted 10,that a man may,what is a number},
title = {{Individual Differences in Nonsymbolic Ratio Processing Predict Symbolic Math Performance}},
year = {2015}
}
@article{Zhang1997,
abstract = {This article proposes a theoretical framework for external representation based problem solving. The Tic-Tat-Toe and its isomorphs are used to illustrate the procedures of the framework as a methodology and test the predictions of the framework as a functional model. Experimental results show that the behavior in the Tic-Tat-Toe is determined by the directly available information in external and internal representations in terms of perceptual and cognitive biases, regardless of whether the biases are consistent with, inconsistent with, or irrelevant to the task. It is shown that external representations are not merely inputs and stimuli to the internal mind and that they have much more important functions than mere memory aids. A representational determinism is suggested-the form of a representation determines what information can be perceived, what processes can be activated, and what structures can be discovered from the specific representation.},
author = {Zhang, Jiajie},
doi = {10.1207/s15516709cog2102_3},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
number = {2},
pages = {179--217},
pmid = {9712194170},
title = {{The Nature Problem of External in Solving Representations}},
volume = {21},
year = {1997}
}
@article{Lupyan2016,
abstract = {For over 300 years, the humble triangle has served as the paradigmatic example of the problem of abstraction. How can we have the idea of a general triangle even though every experience with triangles is with specific ones? Classical cognitive science seemed to provide an answer in symbolic representation. With its easily enumerated necessary and sufficient conditions, the triangle would appear to be a ideal candidate for being represented in a symbolic form. I show that it is not. Across a variety of tasksâ€”drawing, speeded recognition, unspeeded visual judgments, and inferenceâ€”representations of triangles appear to be graded and contextdependent. I show that using the category name â€œtriangleâ€ activates a more prototypical representation than using an arguably coextensive cue, â€œthree-sided polygon.â€ For example, when asked to draw â€œtrianglesâ€ people draw more typical triangles than when asked to draw â€œthree-sided polygons.â€ Altogether, the results support the view that (even formal) concepts have a graded and flexible structure which takes on a more prototypical and stable form when activated by category labels.},
author = {Lupyan, Gary},
doi = {10.1080/17470218.2015.1130730},
issn = {1747-0218},
journal = {The Quarterly Journal of Experimental Psychology},
number = {January},
pages = {1--69},
title = {{The paradox of the universal triangle: concepts, language, and prototypes}},
year = {2016}
}
@article{Cosmides1989,
abstract = {In order to successfully engage in social exchange-cooperation between two or more individuals for mutual benefit-humans must be able to solve a number of complex computational problems, and do so with special efficiency. Following Marr (1982), Cosmides (1985) and Cosmides and Tooby (1989) used evolutionary principles to develop a computational theory of these adaptive problems. Specific hypotheses concerning the structure of the algorithms that govern how humans reason about social exchange were derived from this computational theory. This article presents a series of experiments designed to test these hypotheses, using the Wason selection task, a test of logical reasoning. Part I reports experiments testing social exchange theory against the availability theories of reasoning; Part II reports experiments testing it against Cheng and Holyoak's (1985) permission schema theory. The experimental design included eight critical tests designed to choose between social exchange theory and these other two families of theories; the results of all eight tests support social exchange theory. The hypothesis that the human mind includes cognitive processes specialized for reasoning about social exchange predicts the content effects. ?? 1989.},
author = {Cosmides, Leda},
doi = {10.1016/0010-0277(89)90023-1},
isbn = {0010-0277},
issn = {00100277},
journal = {Cognition},
number = {3},
pages = {187--276},
pmid = {2743748},
title = {{The logic of social exchange: Has natural selection shaped how humans reason? Studies with the Wason selection task}},
volume = {31},
year = {1989}
}
@article{Sweller1983,
abstract = {Previous studies have found that the strategies used by expert and novice problem solvers differ. Novices tend to use means-ends analysis, which involves working backward from the goal, whereas experts prefer to work forward from the givens of a problem. Experiment 1 was designed to study the course of development of expertise using a subset of kinematics problems. After solving many problems, subjects demonstrated the switch from a means-ends to a forward-chaining strategy. This was associated with the conventional concomitants of expertise such as a decrease in the number of moves required for solution. In addition, the speed at which expertise developed varied for different categories of problems. Subjects appeared to categorize problems according to the order in which equations would be required, with these categories being discovered at nonuniform rates. This was assumed to be due to the differential rate of acquisition of schemas associated with different categories of problems. Experiments 2 and 3, again using kinematics problems, tested the hypothesis that the means-ends strategies used by novices retarded the acquisition of appropriate'schemas. It was suggested that under a means-ends strategy, moves are controlled by the problem goal, which reduces the information obtained by problem solvers concerning problem structure. The use of nonspecific rather than specific goals was found to enhance the acquisition of expertise as measured by the use of a forward-oriented strategy, the number of moves required for solution, and the number of equations written without substitutions. Experiments 4 and 5, using geometry problems, duplicated the enhanced rate of strategy alteration found with reduced goal specificity. The results of Experiments 6 and 7, again using geometry problems, indicated that reduced goal specificity also enhanced the rate at which problem solvers induced appropriate problem categories. It was concluded that in circumstances in which the primary reason for presenting problems is to assist problem solvers in acquiring knowledge concerning problem structure, the use of conventional problems solved by means-ends analysis may not be maximally efficient.},
author = {Sweller, John and Mawer, Robert F and Ward, Mark R},
doi = {10.1037/0096-3445.112.4.639},
issn = {0096-3445},
journal = {Journal of Experimental Psychology: General},
number = {4},
pages = {639--661},
pmid = {1120},
title = {{Development of expertise in mathematical problem solving.}},
volume = {112},
year = {1983}
}
@article{Wagenmakers2016,
abstract = {The practical advantages of Bayesian inference are demonstrated through two concrete examples. In the first example, we wish to learn about a crimi-nal's IQ: a problem of parameter estimation. In the second example, we wish to quantify support in favor of a null hypothesis, and track this support as the data accumulate: a problem of hypothesis testing. The Bayesian frame-work unifies both problems within a coherent predictive framework, where parameters and models that predict the data successfully will receive a boost in plausibility, whereas parameters and models that predict poorly suffer a decline. Our examples demonstrate how Bayesian analyses can be more in-formative, more elegant, and more flexible than the orthodox methodology that remains dominant within the field of psychology. On a sunny morning in Florida, while the birds were singing and the crickets chirping, Bob decided to throw his wife from the bedroom balcony, killing her instantly. The case is},
author = {Wagenmakers, Eric-jan and Morey, Richard D and Lee, Michael D},
doi = {10.1177/0963721416643289},
issn = {0963-7214},
keywords = {hypothesis testing,parameter estimation,prediction,updating},
pages = {1--9},
title = {{Bayesian Benefits for the Pragmatic Researcher}},
year = {2016}
}
@article{Jiang2015,
abstract = {Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training. The two methods share a similar concep-tual learning paradigm, but differ in specific learning schemes. In CL, the curriculum is predetermined by pri-or knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of pri-or knowledge while ignoring feedback about the learn-er. In SPL, the curriculum is dynamically determined to adjust to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a u-nified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress dur-ing training. In comparison to human education, SPCL is analogous to " instructor-student-collaborative " learn-ing mode, as opposed to " instructor-driven " in CL or " student-driven " in SPL. Empirically, we show that the advantage of SPCL on two tasks. Curriculum learning (Bengio et al. 2009) and self-paced learning (Kumar, Packer, and Koller 2010) have been at-tracting increasing attention in the field of machine learning and artificial intelligence. Both the learning paradigms are inspired by the learning principle underlying the cognitive process of humans and animals, which generally start with learning easier aspects of a task, and then gradually take more complex examples into consideration. The intuition can be explained in analogous to human education in which a pupil is supposed to understand elementary algebra be-fore he or she can learn more advanced algebra topics. This learning paradigm has been empirically demonstrated to be instrumental in avoiding bad local minima and in achieving a better generalization result (Khan, Zhu, and Mutlu 2011; Basu and Christensen 2013; Tang et al. 2012). A curriculum determines a sequence of training samples which essentially corresponds to a list of samples ranked in ascending order of learning difficulty. A major disparity Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. between curriculum learning (CL) and self-paced learning (SPL) lies in the derivation of the curriculum. In CL, the cur-riculum is assumed to be given by an oracle beforehand, and remains fixed thereafter. In SPL, the curriculum is dynami-cally generated by the learner itself, according to what the learner has already learned. The advantage of CL includes the flexibility to incorpo-rate prior knowledge from various sources. Its drawback stems from the fact that the curriculum design is determined independently of the subsequent learning, which may result in inconsistency between the fixed curriculum and the dy-namically learned models. From the optimization perspec-tive, since the learning proceeds iteratively, there is no guar-antee that the predetermined curriculum can even lead to a converged solution. SPL, on the other hand, formulates the learning problem as a concise biconvex problem, where the curriculum design is embedded and jointly learned with model parameters. Therefore, the learned model is consis-tent. However, SPL is limited in incorporating prior knowl-edge into learning, rendering it prone to overfitting. Ignoring prior knowledge is less reasonable when reliable prior infor-mation is available. Since both methods have their advan-tages, it is difficult to judge which one is better in practice. In this paper, we discover the missing link between CL and SPL. We formally propose a unified framework called Self-paced Curriculum Leaning (SPCL). SPCL represents a general learning paradigm that combines the merits from both the CL and SPL. On one hand, it inherits and further generalizes the theory of SPL. On the other hand, SPCL ad-dresses the drawback of SPL by introducing a flexible way to incorporate prior knowledge. This paper also discusses concrete implementations within the proposed framework, which can be useful for solving various problems. This paper offers a compelling insight on the relation-ship between the existing CL and SPL methods. Their re-lation can be intuitively explained in the context of human education, in which SPCL represents an " instructor-student collaborative " learning paradigm, as opposed to " instructor-driven " in CL or " student-driven " in SPL. In SPCL, instruc-tors provide prior knowledge on a weak learning sequence of samples, while leaving students the freedom to decide the actual curriculum according to their learning pace. Since an optimal curriculum for the instructor may not necessarily be optimal for all students, we hypothesize that given reason-able prior knowledge, the curriculum devised by instructors and students together can be expected to be better than the curriculum designed by either part alone. Empirically, we substantiate this hypothesis by demonstrating that the pro-posed method outperforms both CL and SPL on two tasks. The rest of the paper is organized as follows. We first briefly introduce the background knowledge on CL and SPL. Then we propose the model and the algorithm of SPCL. After that, we discuss concrete implementations of SPCL. The experimental results and conclusions are presented in the last two sections.},
author = {Jiang, Lu and Meng, Deyu and Zhao, Qian and Shan, Shiguang and Hauptmann, Alexander G},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2015 - Self-paced Curriculum Learning.pdf:pdf},
isbn = {9781577357025},
issn = {9781577357025},
journal = {Aaai},
pages = {1--30},
title = {{Self-paced Curriculum Learning}},
year = {2015}
}
@article{Stanley,
abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixedtopology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
author = {Stanley, Kenneth O and Miikkulainen, Risto},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stanley, Miikkulainen - Unknown - Evolving Neural Networks through Augmenting Topologies.pdf:pdf},
title = {{Evolving Neural Networks through Augmenting Topologies}}
}
@article{Baxter,
abstract = {Probably the most important problem in machine learning is the preliminary biasing of a learner's hypothesis space so that it is small enough to ensure good generalisation from reasonable training sets, yet large enough that it contains a good solution to the problem being learnt. In this paper a mechanism for automatically learning or biasing the learner's hypothesis space is introduced. It works by rst learning an appropriate internal representation for a learning environment and then using that representation to bias the learner's hypothesis space for the learning of future tasks drawn from the same environment. An internal representation must be learnt by sampling from many similar tasks, not just a single task as occurs in ordinary machine learning. It is proved that the number of examples m per task required to ensure good generalisation from a representation learner obeys m = O(a+b=n) where n is the number of tasks being learnt and a and b are constants. If the tasks are learnt independently (i.e. without a common representation) then m = O(a+b). It is argued that for learning environments such as eech and character recognition b a and hence representation learning in these environments can potentially yield a drastic reduction in the number of examples required per task. It is also proved that if n = O(b) (with m = O(a+b=n)) then the representation learnt Appeared in Proceedings of the 8th International ACM Workshop on Computational Learning Theory (long talk). will be good for learning novel tasks from the same environment, and that the number of examples required to generalise well on a novel task will be reduced to O(a) (as opposed to O(a + b) if no representation is used). It is shown that gradient descent can be used to train neural network representations and the results of an experiment are reported in which a neural network representation was learnt for an environment consisting of translationally invariant Boolean functions. The experiment provides strong qualitative support for the theoretical results.},
author = {Baxter, Jonathan},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baxter - Unknown - Learning Internal Representations.pdf:pdf},
title = {{Learning Internal Representations}}
}
@article{Pascanu2014,
abstract = {We study the complexity of functions computable by deep feedforward neural networks with piece-wise linear activations in terms of the number of regions of linearity that they have. Deep networks are able to sequentially map portions of each layer's input space to the same output. In this way, deep models compute functions with a compositional structure that is able to re-use pieces of computation exponentially often in terms of their depth. This note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece-wise linear activation functions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1869v1},
author = {Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {arXiv:1402.1869v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pascanu, Cho, Bengio - 2014 - On the Number of Linear Regions of Deep Neural Networks Â´.pdf:pdf},
issn = {10495258},
journal = {Nips},
keywords = {deep learning,input space partition,maxout,neural network,rectifier},
pages = {1--12},
title = {{On the Number of Linear Regions of Deep Neural Networks Â´}},
year = {2014}
}
@article{PooleB.2016,
archivePrefix = {arXiv},
arxivId = {1606.05340},
author = {et al. {Poole, B.}},
eprint = {1606.05340},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/et al. Poole, B. - 2016 - Exponential expressivity in deep neural networks through transient chaos.pdf:pdf},
journal = {arXiv preprint arXiv:1606.05340v2},
title = {{Exponential expressivity in deep neural networks through transient chaos}},
year = {2016}
}
@article{Hardt2015,
abstract = {We show that any model trained by a stochastic gradient method with few iterations has vanishing generalization error. We prove this by showing the method is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. Our results apply to both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new explanations for why multiple epochs of stochastic gradient descent generalize well in practice. In the nonconvex case, we provide a new interpretation of common practices in neural networks, and provide a formal rationale for stability-promoting mechanisms in training large, deep models. Conceptually, our findings underscore the importance of reducing training time beyond its obvious benefit.},
archivePrefix = {arXiv},
arxivId = {1509.01240},
author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
eprint = {1509.01240},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hardt, Recht, Singer - 2015 - Train faster, generalize better Stability of stochastic gradient descent.pdf:pdf},
journal = {srXiv:1509.01240},
pages = {1--24},
title = {{Train faster, generalize better: Stability of stochastic gradient descent}},
year = {2015}
}
@article{Bartlett1996,
author = {Bartlett, Pl and Long, Pm and Williamson, Rc},
doi = {10.1006/jcss.1996.0033},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bartlett, Long, Williamson - 1996 - Fat-shattering and the learnability of real-valued functions.pdf:pdf},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
keywords = {1996,434 452,rnal of computer and,system sciences 52},
number = {0033},
pages = {434--452},
title = {{Fat-shattering and the learnability of real-valued functions}},
volume = {52},
year = {1996}
}
@article{Dubinsky1991,
abstract = {Dubinsky describes a generalization of Piaget's notion of reflective$\backslash$nabstraction to the context of advanced mathematical thinking. The$\backslash$nresult is a description of learning in terms of encapsulating processes$\backslash$nand interiorizing actions. These abstractions are organized in a$\backslash$nschema for a particular concept, and Dubinsky offers "genetic decomposition"$\backslash$nas a manner for describing schema. These decompositions may be useful$\backslash$nfor planning and reflecting on instructional sequences. See also$\backslash$nSfard and Linchevski (1994) and Asial et. al. (1996).},
author = {Dubinsky, Ed},
doi = {10.1007/0-306-47203-1_7},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dubinsky - 1991 - Reflective abstraction in advanced mathematical thinking.pdf:pdf},
isbn = {0792314565},
issn = {1098-6065},
journal = {Advanced mathematical thinking},
pages = {95--121},
title = {{Reflective abstraction in advanced mathematical thinking}},
year = {1991}
}
@article{Bouchard2015,
abstract = {Stochastic Gradient Descent (SGD) is one of the most widely used techniques for online optimization in machine learning. In this work, we accelerate SGD by adaptively learning how to sample the most useful training examples at each time step. First, we show that SGD can be used to learn the best possible sampling distribution of an importance sampling estimator. Second, we show that the sampling distribution of a SGD algorithm can be estimated online by incrementally minimizing the variance of the gradient. The resulting algorithm - called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to optimize, as well as a set of parameters to sample learning examples. We show that AW-SGD yields faster convergence in three different applications: (i) image classification with deep features, where the sampling of images depends on their labels, (ii) matrix factorization, where rows and columns are not sampled uniformly, and (iii) reinforcement learning, where the optimized and explore policies are estimated at the same time.},
archivePrefix = {arXiv},
arxivId = {1506.09016},
author = {Bouchard, Guillaume and Trouillon, Th{\'{e}}o and Perez, Julien and Gaidon, Adrien},
eprint = {1506.09016},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bouchard et al. - 2015 - Accelerating Stochastic Gradient Descent via Online Learning to Sample.pdf:pdf},
journal = {Arxiv},
number = {iii},
pages = {5},
title = {{Accelerating Stochastic Gradient Descent via Online Learning to Sample}},
year = {2015}
}
@article{Gu2014,
abstract = {â€”In the last few years, deep learning has lead to very good performance on a variety of problems, such as object recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Due to the lack of training data and computing power in early days, it is hard to train a large high-capacity convolutional neural network without overfitting. Recently, with the rapid growth of data size and the increasing power of graphics processor unit, many researchers have improved the convolutional neural networks and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. Besides, we also introduce some applications of convolutional neural networks in computer vision.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Krogh, Anders and Vedelsby, Jesper and Ukil, a. and Bernasconi, J. and Fukumizu, K and Poland, Jan and Zell, Andreas and Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and Papamakarios, George and Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram and Ba, Jimmy and Caruana, Rich and Bianchini, Monica and Scarselli, Franco and M., Bianchini and F., Scarselli and Bianchini, Monica and Scarselli, Franco and Larochelle, Hugo and Larochelle, Hugo and Bengio, Yoshua and Bengio, Yoshua and Lourador, Jerome and Lourador, Jerome and Lamblin, Pascal and Lamblin, Pascal and Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod and Welbl, Johannes},
doi = {Doi 10.1109/Tsmcc.2012.2220963},
eprint = {1503.02531},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu et al. - 2014 - Exploring Strategies for Training Deep Neural Networks.pdf:pdf},
isbn = {9783319117522},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Deep learning,Index Termsâ€”Convolutional Neural Network,active learning,fisher,information matrix,multilayer perceptron,pruning},
number = {1},
pages = {1--16},
pmid = {18249735},
title = {{Exploring Strategies for Training Deep Neural Networks}},
volume = {11},
year = {2014}
}
@article{Wagenmakers2016,
abstract = {The practical advantages of Bayesian inference are demonstrated through two concrete examples. In the first example, we wish to learn about a crimi-nal's IQ: a problem of parameter estimation. In the second example, we wish to quantify support in favor of a null hypothesis, and track this support as the data accumulate: a problem of hypothesis testing. The Bayesian frame-work unifies both problems within a coherent predictive framework, where parameters and models that predict the data successfully will receive a boost in plausibility, whereas parameters and models that predict poorly suffer a decline. Our examples demonstrate how Bayesian analyses can be more in-formative, more elegant, and more flexible than the orthodox methodology that remains dominant within the field of psychology. On a sunny morning in Florida, while the birds were singing and the crickets chirping, Bob decided to throw his wife from the bedroom balcony, killing her instantly. The case is},
author = {Wagenmakers, Eric-jan and Morey, Richard D and Lee, Michael D},
doi = {10.1177/0963721416643289},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wagenmakers, Morey, Lee - 2016 - Bayesian Benefits for the Pragmatic Researcher.pdf:pdf},
issn = {0963-7214},
keywords = {hypothesis testing,parameter estimation,prediction,updating},
pages = {1--9},
title = {{Bayesian Benefits for the Pragmatic Researcher}},
year = {2016}
}
@article{Zaremba2014,
abstract = {Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks' performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99{\%} accuracy.},
archivePrefix = {arXiv},
arxivId = {1410.4615},
author = {Zaremba, Wojciech and Sutskever, Ilya},
doi = {10.1016/S0893-6080(96)00073-1},
eprint = {1410.4615},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaremba, Sutskever - 2014 - Learning to Execute.pdf:pdf},
isbn = {1410.4615},
issn = {08936080},
journal = {International Conference on Learning Representations},
pages = {1--25},
title = {{Learning to Execute}},
year = {2014}
}
@article{Dauphin2014,
abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
archivePrefix = {arXiv},
arxivId = {1406.2572},
author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
eprint = {1406.2572},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.pdf:pdf},
isbn = {1406.2572},
issn = {10495258},
journal = {arXiv},
pages = {1--14},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
year = {2014}
}
@article{Saxe2013,
abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
archivePrefix = {arXiv},
arxivId = {1312.6120},
author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
eprint = {1312.6120},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saxe, McClelland, Ganguli - 2013 - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.pdf:pdf},
isbn = {1312.6120},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Exact solutions to the nonlinear dynamics of learning in deep linear neural networks}},
year = {2013}
}
@article{Rohde1997,
abstract = {Contra Elman (e.g., 1991), starting iwth simplified inputs is not necessary for training recurrent neural networks to learn pseudo-natural languages.},
author = {Rohde, Dlt and Plaut, Dc},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rohde, Plaut - 1997 - Simple recurrent networks and natural language How important is starting small.pdf:pdf},
journal = {Proceedings of the 19th annual conference of the Cognitive Science Society},
pages = {656--661},
title = {{Simple recurrent networks and natural language: How important is starting small}},
year = {1997}
}
@article{Bengio2012,
abstract = {We propose a theory that relates difficulty of learning in deep architectures to culture and language. It is articulated around the following hypotheses: (1) learning in an individual human brain is hampered by the presence of effective local minima; (2) this optimization difficulty is particularly important when it comes to learning higher-level abstractions, i.e., concepts that cover a vast and highly-nonlinear span of sensory configurations; (3) such high-level abstrac- tions are best represented in brains by the composition of many levels of representation, i.e., by deep architectures; (4) a human brain can learn such high-level abstractions if guided by the signals produced by other humans, which act as hints or indirect supervision for these high-level abstractions; and (5), language and the recombination and optimization of mental concepts provide an efficient evolutionary recombination operator, and this gives rise to rapid search in the space of communicable ideas that help humans build up better high-level internal representations of their world. These hypotheses put together imply that human culture and the evolution of ideas have been crucial to counter an optimization difficulty: this optimization difficulty would otherwise make it very difficult for human brains to capture high-level knowl- edge of the world. The theory is grounded in experimental observations of the difficulties of training deep artificial neural networks. Plausible consequences of this theory for the efficiency of cultural evolution are sketched.},
archivePrefix = {arXiv},
arxivId = {arXiv:1203.2990v2},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-55337-0_3},
eprint = {arXiv:1203.2990v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2012 - Evolving culture vs local minima.pdf:pdf},
isbn = {978-3-642-55336-3, 978-3-642-55337-0},
issn = {1860949X},
journal = {arXiv preprint arXiv:1203.2990},
pages = {1--28},
title = {{Evolving culture vs local minima}},
volume = {2006},
year = {2012}
}
@article{Bengio2013,
abstract = {Unsupervised learning of representations has been found useful in many applications and benefits from several advantages, e.g., where there are many un- labeled examples and few labeled ones (semi-supervised learning), or where the unlabeled or labeled examples are from a distribution different but related to the one of interest (self-taught learning, multi-task learning, and domain adaptation). Some of these algorithms have successfully been used to learn a hierarchy of features, i.e., to build a deep architecture, either as initialization for a supervised predictor, or as a generative model. Deep learning algorithms can yield representations that are more abstract and better disentangle the hidden factors of variation underlying the unknown generating distribution, i.e., to capture invariances and discover non-local structure in that distribution. This chapter reviews the main motivations and ideas behind deep learning algorithms and their representation-learning components, as well as recent results in this area, and proposes a vision of challenges and hopes on the road ahead, focusing on the questions of invariance and disentangling.},
archivePrefix = {arXiv},
arxivId = {arXiv:1305.0445v1},
author = {Bengio, Yoshua and Courville, Aaron},
doi = {10.1007/978-3-642-36657-4_1},
eprint = {arXiv:1305.0445v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Courville - 2013 - Deep Learning of Representations.pdf:pdf},
isbn = {9783642366567},
issn = {18684394},
journal = {Intelligent Systems Reference Library},
pages = {1--28},
title = {{Deep Learning of Representations}},
volume = {49},
year = {2013}
}
@article{Lin1997,
author = {Lin, F-R. and Shaw, M J},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Shaw - 1997 - Active training of backpropagation neural networks using the learning by experimentation methodology.pdf:pdf},
issn = {02545330},
journal = {Annals of Operations Research},
keywords = {active learning,learning by experimentation methodology,neural networks},
pages = {129--145},
title = {{Active training of backpropagation neural networks using the learning by experimentation methodology}},
volume = {75},
year = {1997}
}
@article{Fukumizu2000,
abstract = {This paper proposes new methods for generating input locations actively in gathering training data, aiming at solving problems unique to multilayer perceptrons. One of the problems is that optimum input locations, which are calculated deterministically, sometimes distribute densely around the same point and cause local minima in backpropagation training. Two probabilistic active learning methods, which utilize the statistical variance of locations, are proposed to solve this problem. One is parametric active learning and the other is multipoint-search active learning. Another serious problem in applying active learning to multilayer perceptrons is that a Fisher information matrix can be singular, while many methods, including the proposed ones, assume its regularity. A technique of pruning redundant hidden units is proposed to keep the Fisher information matrix regular. Combined with this technique, active learning can be applied stably to multilayer perceptrons. The effectiveness of the proposed methods is demonstrated through computer simulations on simple artificial problems and a real-world problem of color conversion.},
author = {Fukumizu, Kenji},
doi = {10.1109/72.822506},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fukumizu - 2000 - Statistical active learning in multilayer perceptrons.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {active learning,fisher,information matrix,multilayer perceptron,pruning},
number = {1},
pages = {17--26},
pmid = {18249735},
title = {{Statistical active learning in multilayer perceptrons}},
volume = {11},
year = {2000}
}
@article{Krogh1995,
abstract = {Learning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme. 1 INTRODUCTION It is well known that a combination of many different predictors can improve predictions. In the neural networks community "ensembles" of neural networks h...},
author = {Krogh, Anders and Vedelsby, Jesper},
doi = {10.1.1.37.8876},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krogh, Vedelsby - 1995 - Neural Network Ensembles, Cross Validation, and Active Learning.pdf:pdf},
isbn = {0262201046},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 7},
pages = {231--238},
title = {{Neural Network Ensembles, Cross Validation, and Active Learning}},
year = {1995}
}
@article{Vijayakumar1998,
author = {Vijayakumar, Sethu and Sugiyama, Masashi and Ogawa, Hidemitsu},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vijayakumar, Sugiyama, Ogawa - 1998 - Training Data Selection for Optimal Generalization with Noise Variance Reduction in Neural Network.pdf:pdf},
title = {{Training Data Selection for Optimal Generalization with Noise Variance Reduction in Neural Networks Functional analytic framework for NN learning}},
year = {1998}
}
@article{Gulcehre2013,
abstract = {We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hy- pothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experi- ments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first level of the two-tiered MLP is pre-trained with intermediate level targets being the presence of sprites at each location, while the second level takes the output of the first level as input and predicts the final task target binary event. The two-tiered MLP architecture, with a few tens of thou- sand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, de- cision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not per- formed is due to the composition of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective lo- cal minima.},
archivePrefix = {arXiv},
arxivId = {1301.4083},
author = {G{\"{u}}l{\c{c}}ehre, {\c{C}}aÄŸlar and Bengio, Yoshua},
eprint = {1301.4083},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/G{\"{u}}l{\c{c}}ehre, Bengio - 2013 - Knowledge Matters Importance of Prior Information for Optimization.pdf:pdf},
journal = {arXiv preprint arXiv:1301.4083},
pages = {1--12},
title = {{Knowledge Matters : Importance of Prior Information for Optimization}},
year = {2013}
}
@article{Foundalis2006,
abstract = {Phaeaco is a cognitive architecture for visual pattern recognition that starts at the ground level of receiving pixels as input, and works its way through creating abstract representations of geometric figures formed by those pixels. Phaeaco can tell how similar such figures are by using a psychologically plausible metric to compute a difference value among representations, and use that value to group figures together, if possible. Groups of figures are represented by statistical attributes (average, standard deviation, and other statistics), and serve as the basis for a formed and thereafter learned concept (e.g., triangle), stored in long-term memory. Phaeaco focuses on the Bongard problems, a set of puzzles in visual categorization, and applies its cognitive principles in its efforts to solve them, faring nearly as well as humans in the puzzles it manages to solve.},
author = {Foundalis, He},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Foundalis - 2006 - Phaeaco A Cognitive Architecture Inspired By Bongard's Problems.pdf:pdf},
journal = {Citeulike.Org},
number = {May},
pages = {1--461},
title = {{Phaeaco: A Cognitive Architecture Inspired By Bongard's Problems}},
year = {2006}
}
@article{Lupyan2013,
abstract = {It is shown that educated adults routinely make errors in placing stimuli into familiar, well-defined categories such as triangle and odd number. Scalene triangles are often rejected as instances of triangles and 798 is categorized by some as an odd number. These patterns are observed both in timed and untimed tasks, hold for people who can fully express the necessary and sufficient conditions for category membership, and for individuals with varying levels of education. A sizeable minority of people believe that 400 is more even than 798 and that an equilateral triangle is the most "trianglest" of triangles. Such beliefs predict how people instantiate other categories with necessary and sufficient conditions, e.g., grandmother. I argue that the distributed and graded nature of mental representations means that human algorithms, unlike conventional computer algorithms, only approximate rule-based classification and never fully abstract from the specifics of the input. This input-sensitivity is critical to obtaining the kind of cognitive flexibility at which humans excel, but comes at the cost of generally poor abilities to perform context-free computations. If human algorithms cannot be trusted to produce unfuzzy representations of odd numbers, triangles, and grandmothers, the idea that they can be trusted to do the heavy lifting of moment-to-moment cognition that is inherent in the metaphor of mind as digital computer still common in cognitive science, needs to be seriously reconsidered. ?? 2013 Elsevier B.V.},
author = {Lupyan, Gary},
doi = {10.1016/j.cognition.2013.08.015},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lupyan - 2013 - The difficulties of executing simple algorithms Why brains make mistakes computers don't.pdf:pdf},
issn = {00100277},
journal = {Cognition},
keywords = {Categorization,Concepts,Distributed representations,Human algorithms,Inference,Prototypes},
number = {3},
pages = {615--636},
pmid = {24156803},
publisher = {Elsevier B.V.},
title = {{The difficulties of executing simple algorithms: Why brains make mistakes computers don't}},
volume = {129},
year = {2013}
}
@article{Lupyan2016,
abstract = {For over 300 years, the humble triangle has served as the paradigmatic example of the problem of abstraction. How can we have the idea of a general triangle even though every experience with triangles is with specific ones? Classical cognitive science seemed to provide an answer in symbolic representation. With its easily enumerated necessary and sufficient conditions, the triangle would appear to be a ideal candidate for being represented in a symbolic form. I show that it is not. Across a variety of tasksâ€”drawing, speeded recognition, unspeeded visual judgments, and inferenceâ€”representations of triangles appear to be graded and contextdependent. I show that using the category name â€œtriangleâ€ activates a more prototypical representation than using an arguably coextensive cue, â€œthree-sided polygon.â€ For example, when asked to draw â€œtrianglesâ€ people draw more typical triangles than when asked to draw â€œthree-sided polygons.â€ Altogether, the results support the view that (even formal) concepts have a graded and flexible structure which takes on a more prototypical and stable form when activated by category labels.},
author = {Lupyan, Gary},
doi = {10.1080/17470218.2015.1130730},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lupyan - 2016 - The paradox of the universal triangle concepts, language, and prototypes.pdf:pdf},
issn = {1747-0218},
journal = {The Quarterly Journal of Experimental Psychology},
number = {January},
pages = {1--69},
title = {{The paradox of the universal triangle: concepts, language, and prototypes}},
year = {2016}
}
@article{McClelland2003,
abstract = {How do we know what properties something has, and which of its properties should be generalized to other objects? How is the knowledge underlying these abilities acquired, and how is it affected by brain disorders? Our approach to these issues is based on the idea that cognitive processes arise from the interactions of neurons through synaptic connections. The knowledge in such interactive and distributed processing systems is stored in the strengths of the connections and is acquired gradually through experience. Degradation of semantic knowledge occurs through degradation of the patterns of neural activity that probe the knowledge stored in the connections. Simulation models based on these ideas capture semantic cognitive processes and their development and disintegration, encompassing domain-specific patterns of generalization in young children, and the restructuring of conceptual knowledge as a function of experience.},
author = {McClelland, J.L. and Rogers, T.T.},
doi = {10.1038/nrn1076},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McClelland, Rogers - 2003 - The parallel distributed processing approach to semantic cognition.pdf:pdf},
isbn = {1471003X},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
keywords = {Animals,Cerebral Cortex,Cerebral Cortex: anatomy {\&} histology,Cerebral Cortex: physiology,Cognition,Cognition: physiology,Humans,Learning,Learning Disorders,Learning Disorders: physiopathology,Learning: physiology,Models, Neurological,Nerve Net,Nerve Net: anatomy {\&} histology,Nerve Net: physiology,Neural Pathways,Neural Pathways: anatomy {\&} histology,Neural Pathways: physiology,Semantics},
number = {4},
pages = {310--322},
pmid = {12671647},
title = {{The parallel distributed processing approach to semantic cognition}},
volume = {4},
year = {2003}
}
@misc{Shepard1961,
abstract = {The present study explores some of the factors that determine how difficult a classification will be to learn or remember. The experiment to be reported was designed primarily to answer two questions: How does the difficulty of learning vary from one type of classification to another? Is something specific learned abut the structure of a classification that will transfer positively to the subsequent learning of a new classification of that same type? The experimental procedure conformed to the usual paired-associate paradigm except that only two responses were used. That is, the eight stimuli were presented, one at a time, in a continuing random sequence and an association between each stimulus and one of two alternative classificatory responses was built up by the method of anticipation. Six female freshmen at Fairleigh Dickinson University served for 15 hours each in this first experiment. These subjects ( S s) were selected to be as uniform as possible with respect to their college entrance examination scores. A combined empirical and theoretical investigation of the difficulties of different kinds of classifications was undertaken using both learning and memory tasks. Sets of stimuli of a variety of kinds were used but, in each set, there were eight stimuli each of which took on one of two possible values on each of three different dimensions. Results showed that of the 70 possible classifications of the eight stimuli into two equal groups, there are only six basic types. The different classifications belonging to any one of these types have the same structure; they differ only with respect to which of the three dimensions is assigned to which of the three roles in the classification, and with respect to which of the two classificatory responses is assigned to which group of four stimuli.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Shepard, Roger N and Hovland, Carl I and Jenkins, Herbert M},
booktitle = {Psychological Monographs: General and Applied},
doi = {10.1037/h0093825},
eprint = {arXiv:1011.1669v3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shepard, Hovland, Jenkins - 1961 - Learning and memorization of classifications.pdf:pdf},
isbn = {9788578110796},
issn = {0096-9753},
number = {13},
pages = {1--42},
pmid = {418},
title = {{Learning and memorization of classifications.}},
volume = {75},
year = {1961}
}
@article{Alahakoon2000,
abstract = {The growing self-organizing map (GSOM) has been presented as an extended version of the self-organizing map (SOM), which has significant advantages for knowledge discovery applications. In this paper, the GSOM algorithm is presented in detail and the effect of a spread factor, which can be used to measure and control the spread of the GSOM, is investigated. The spread factor is independent of the dimensionality of the data and as such can be used as a controlling measure for generating maps with different dimensionality, which can then be compared and analyzed with better accuracy. The spread factor is also presented as a method of achieving hierarchical clustering of a data set with the GSOM. Such hierarchical clustering allows the data analyst to identify significant and interesting clusters at a higher level of the hierarchy, and as such continue with finer clustering of only the interesting clusters. Therefore, only a small map is created in the beginning with a low spread factor, which can be generated for even a very large data set. Further analysis is conducted on selected sections of the data and as such of smaller volume. Therefore, this method facilitates the analysis of even very large data sets.},
author = {Alahakoon, D and Halgamuge, S K and Srinivasan, B},
doi = {10.1109/72.846732},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alahakoon, Halgamuge, Srinivasan - 2000 - Dynamic Self-Organizing Maps with Controlled Growth for Knoledge Discovery.pdf:pdf},
issn = {1045-9227},
journal = {IEEE Trans. on Neural Networks},
number = {3},
pages = {601--14},
pmid = {18249788},
title = {{Dynamic Self-Organizing Maps with Controlled Growth for Knoledge Discovery}},
volume = {11},
year = {2000}
}
@misc{Kruschke1992,
abstract = {ALCOVE (attention learning covering map) is a connectionist model of category learning that incorporates an exemplar-based representation (Medin {\&} Schaffer, 1978; Nosofsky, 1986) with error-driven learning (Gluck {\&} Bower, 1988; Rumelhart, Hinton, {\&} Williams, 1986). Alcove selectively attends to relevant stimulus dimensions, is sensitive to correlated dimensions, can account for a form of base-rate neglect, does not suffer catastrophic forgetting, and can exhibit 3-stage (U-shaped) learning of high-frequency exceptions to rules, whereas such effects are not easily accounted for by models using other combinations of representation and learning method.},
author = {Kruschke, J K},
booktitle = {Psychological review},
doi = {10.1037/0033-295X.99.1.22},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kruschke - 1992 - ALCOVE an exemplar-based connectionist model of category learning.pdf:pdf},
isbn = {0033-295X$\backslash$n1939-1471},
issn = {0033-295X},
keywords = {Attention,Discrimination Learning,Generalization (Psychology),Humans,Mental Recall,Models,Neural Networks (Computer),Pattern Recognition,Psychological,Reinforcement (Psychology),Visual},
number = {1},
pages = {22--44},
pmid = {1546117},
title = {{ALCOVE: an exemplar-based connectionist model of category learning.}},
volume = {99},
year = {1992}
}
@incollection{VanHulle2012,
abstract = {A topographic map is a two-dimensional, nonlinear approximation of a potentially high-dimensional data manifold, which makes it an appealing instrument for visualizing and exploring high-dimensional data. The self-organizing map (SOM) is the most widely used algorithm, and it has led to thousands of applications in very diverse areas. In this chapter we introduce the SOM algorithm, discuss its properties and applications, and also discuss some of its extensions and new types of topographic map formation, such as those that can be used for processing categorical data, time series, and tree-structured data.},
author = {{Van Hulle}, Marc M.},
booktitle = {Handbook of Natural Computing},
doi = {10.1007/978-3-540-92910-9_19},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Hulle - 2012 - 19 - Self-organizing Maps.pdf:pdf},
isbn = {978-3-540-92909-3},
pages = {585--622},
title = {{19 - Self-organizing Maps}},
year = {2012}
}
@article{Mood2010,
abstract = {Logistic regression estimates do not behave like linear regression estimates in one important respect: They are affected by omitted variables, even when these variables are unrelated to the independent variables in the model. This fact has important implications that have gone largely unnoticed by sociologists. Importantly, we cannot straightforwardly interpret log-odds ratios or odds ratios as effect measures, because they also reflect the degree of unobserved heterogeneity in the model. In addition, we cannot compare log-odds ratios or odds ratios for similar models across groups, samples, or time points, or across models with different independent variables in a sample. This article discusses these problems and possible ways of overcoming them. [ABSTRACT FROM PUBLISHER] Copyright of European Sociological Review is the property of Oxford University Press / UK and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
author = {Mood, Carina},
doi = {10.1093/esr/jcp006},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mood - 2010 - Logistic regression Why we cannot do what We think we can do, and what we can do about it.pdf:pdf},
isbn = {02667215},
issn = {02667215},
journal = {European Sociological Review},
number = {1},
pages = {67--82},
pmid = {48014827},
title = {{Logistic regression: Why we cannot do what We think we can do, and what we can do about it}},
volume = {26},
year = {2010}
}
@article{Richland2012,
abstract = {Many students graduate from Kâ€“12 mathematics programs without flexible, conceptual mathematics knowledge. This article reviews psychological and educational research to propose that refining Kâ€“12 classroom instruction such that students draw connections through relational comparisons may enhance their long-term ability to transfer and engage with mathematics as a meaningful system. We begin by examining the mathematical knowledge of students in one community college, reviewing results that show even after completing a Kâ€“12 required mathematics sequence, these students were unlikely to flexibly reason about mathematics. Rather than drawing relationships between presented problems or inferences about the representations, students preferred to attempt previously memorized (often incorrect) procedures (Givvin, Stigler, {\&} Thompson, 2011; Stigler, Givvin, {\&} Thompson, 2010). We next describe the relations between the cognition of flexible, comparative reasoning and experimentally derived strategies for supporting students' ability to make these connections. A cross-cultural study found that U.S. teachers currently use these strategies much less frequently than their international counterparts (Hiebert et al., 2003; Richland, Zur, {\&} Holyoak, 2007), suggesting that these practices may be correlated with high student performance. Finally, we articulate a research agenda for improving and studying pedagogical practices for fostering students' relational thinking about mathematics.},
author = {Richland, Lindsey E. and Stigler, James W. and Holyoak, Keith J.},
doi = {10.1080/00461520.2012.667065},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Richland, Stigler, Holyoak - 2012 - Teaching the Conceptual Structure of Mathematics.pdf:pdf},
isbn = {0046-1520$\backslash$r1532-6985},
issn = {0046-1520},
journal = {Educational Psychologist},
number = {3},
pages = {189--203},
title = {{Teaching the Conceptual Structure of Mathematics}},
volume = {47},
year = {2012}
}
@article{Kochen1983,
annote = {1) People suck at uncued analogical transfer
2) Multiple examples help, and if transfer is to be far diverse examples are important.
3) Adding a statement about a general principle had little effect after one example, but after two it worked much better.},
author = {Kochen, Manfred and Krantz, David and Hunt, Earl and Carroll, Tim and Frankovich, Teresa and Arbor, Ann},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kochen et al. - 1983 - Schema Induction and Analogical.pdf:pdf},
title = {{Schema Induction and Analogical}},
volume = {38},
year = {1983}
}
@article{Birman2015,
author = {Birman, Daniel and Gardner, Justin L},
doi = {10.1038/nn.4204},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Birman, Gardner - 2015 - Parietal and prefrontal categorical differences.pdf:pdf},
isbn = {1546-1726 (Electronic)1097-6256 (Linking)},
issn = {1097-6256},
journal = {Nature Neuroscience},
number = {1},
pages = {5--7},
pmid = {26713741},
publisher = {Nature Publishing Group},
title = {{Parietal and prefrontal: categorical differences?}},
volume = {19},
year = {2015}
}
@article{Gobet2001,
abstract = {Pioneering work in the 1940s and 1950s suggested that the concept of â€˜chunking' might be important in many processes of perception, learning and cognition in humans and animals. We summarize here the major sources of evidence for chunking mechanisms, and consider how such mechanisms have been implemented in computational models of the learning process. We distinguish two forms of chunking: the first deliberate, under strategic control, and goal-oriented; the second automatic, continuous, and linked to perceptual processes. Recent work with discrimination-network computational models of long- and short-term memory (EPAM/CHREST) has produced a diverse range of applications of perceptual chunking. We focus on recent successes in verbal learning, expert memory, language acquisition and learning multiple representations, to illustrate the implementation and use of chunking mechanisms within contemporary models of human learning.},
author = {Gobet, Fernand. and Lane, Peter C R. and Croker, Steve. and Cheng, Peter C-H. and Jones, Gary. and Oliver, Iain. and Pine, Julian M.},
doi = {10.1016/S1364-6613(00)01662-4},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gobet et al. - 2001 - Chunking Mechanisms in Human Learning.pdf:pdf},
isbn = {1364-6613},
issn = {1879-307X},
journal = {Trends in Cognitive Sciences},
number = {6},
pages = {236--243},
pmid = {11390294},
title = {{Chunking Mechanisms in Human Learning}},
volume = {5},
year = {2001}
}
@article{Mnih2014,
abstract = {Nice paper that shifts an attentional window to recognize digits.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.6247v1},
author = {Mnih, Volodymyr and Hess, N. and Graves, Alex and Kavukcuoglu, Koray},
doi = {ng},
eprint = {arXiv:1406.6247v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2014 - Recurrent Models of Visual Attention.pdf:pdf},
journal = {Nips},
keywords = {deep learning,digit recognition,neural network},
pages = {arXiv:1406.6247},
title = {{Recurrent Models of Visual Attention}},
year = {2014}
}
@article{Reed2015,
abstract = {We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.},
archivePrefix = {arXiv},
arxivId = {1511.06279},
author = {Reed, Scott and de Freitas, Nando},
eprint = {1511.06279},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reed, de Freitas - 2015 - Neural Programmer-Interpreters.pdf:pdf},
journal = {arXiv preprint},
pages = {1--12},
title = {{Neural Programmer-Interpreters}},
year = {2015}
}
@article{Ba2015,
abstract = {We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.7755v2},
author = {Ba, Jimmy Lei and Mnih, Volodymyr and Kavukcuoglu, Koray},
eprint = {arXiv:1412.7755v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ba, Mnih, Kavukcuoglu - 2015 - Multiple Object Recognition With Visual Attention.pdf:pdf},
journal = {Iclr},
pages = {1--10},
title = {{Multiple Object Recognition With Visual Attention}},
year = {2015}
}
@article{Tesauro1995,
author = {Tesauro, G},
doi = {http://doi.acm.org/10.1145/203330.203343},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tesauro - 1995 - Temporal difference learning and TD-Gammon.pdf:pdf},
issn = {0001-0782},
journal = {Commun. ACM},
number = {3},
pages = {58--68},
title = {{Temporal difference learning and TD-Gammon}},
volume = {38},
year = {1995}
}
@article{Dienes1997,
abstract = {Explored the extent to which unconscious knowledge could be applied flexibly in 2 experiments. In Exp 1, using 48 Ss, transfer was investigated in a case where the mapping between the domains was transparent so any transfer decrement must arise because the knowledge was perceptually-bound. The domains were colors and names of colors. In Exp 2, using 40 Ss, the relationship between consciousness and transfer was tested. In the test phase for each experiment, Ss were informed that there was a complex set of rules that governed the order of the items in each string, and were told to classify the strings. Exp 2 used letters and Ss were asked for a confidence judgement on a scale from 50â€“100. Exp 1 demonstrated that knowledge of artificial grammars was actually relatively inflexible in its application, in that there was limited transfer between 2 domains with obvious mappings. Results for Exp 2 indicate that the knowledge had an unconscious component in both the same and different domains. It is concluded that grammar learning is a useful paradigm for exploring the links between consciousness and flexibility. The training and test items are appended.},
author = {Dienes, Zolt{\'{a}}n and Altmann, Gerry T. M.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dienes, Altmann - 1997 - Transfer of implicit knowledge across domains How implicit and how abstract.pdf:pdf},
journal = {How implicit is implicit learning?},
pages = {107--123},
title = {{Transfer of implicit knowledge across domains: How implicit and how abstract?}},
year = {1997}
}
@article{Mikolov2013,
abstract = {Continuous space language models have re- cently demonstrated outstanding results across a variety of tasks. In this paper, we ex- amine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, â€œKing - Man + Womanâ€ results in a vector very close to â€œQueen.â€ We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regu- larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re- markably, this method outperforms the best previous systems. 1},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Yih, Zweig - 2013 - Linguistic regularities in continuous space word representations.pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of NAACL-HLT},
number = {June},
pages = {746--751},
pmid = {1938007},
title = {{Linguistic regularities in continuous space word representations}},
year = {2013}
}
@article{Cosmides1989,
abstract = {In order to successfully engage in social exchange-cooperation between two or more individuals for mutual benefit-humans must be able to solve a number of complex computational problems, and do so with special efficiency. Following Marr (1982), Cosmides (1985) and Cosmides and Tooby (1989) used evolutionary principles to develop a computational theory of these adaptive problems. Specific hypotheses concerning the structure of the algorithms that govern how humans reason about social exchange were derived from this computational theory. This article presents a series of experiments designed to test these hypotheses, using the Wason selection task, a test of logical reasoning. Part I reports experiments testing social exchange theory against the availability theories of reasoning; Part II reports experiments testing it against Cheng and Holyoak's (1985) permission schema theory. The experimental design included eight critical tests designed to choose between social exchange theory and these other two families of theories; the results of all eight tests support social exchange theory. The hypothesis that the human mind includes cognitive processes specialized for reasoning about social exchange predicts the content effects. ?? 1989.},
author = {Cosmides, Leda},
doi = {10.1016/0010-0277(89)90023-1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cosmides - 1989 - The logic of social exchange Has natural selection shaped how humans reason Studies with the Wason selection task.pdf:pdf},
isbn = {0010-0277},
issn = {00100277},
journal = {Cognition},
number = {3},
pages = {187--276},
pmid = {2743748},
title = {{The logic of social exchange: Has natural selection shaped how humans reason? Studies with the Wason selection task}},
volume = {31},
year = {1989}
}
@article{Newell1961,
abstract = {A theory of problem solving expressed as a computer program permits simulation of thinking processes. From Psyc Abstracts 36:04:4CM11N. (PsycINFO Database Record (c) 2003 APA},
author = {Newell, a and Simon, H a},
doi = {10.2307/1708146},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Newell, Simon - 1961 - Computer simulation of human thinking.pdf:pdf},
isbn = {2819611222},
issn = {00368075},
journal = {Science (New York, N.Y.)},
keywords = {AUTOMATIC DATA PROCESSING,MENTAL PROCESSES},
number = {3495},
pages = {2011--2017},
pmid = {14479322},
title = {{Computer simulation of human thinking.}},
volume = {134},
year = {1961}
}
@article{Ericsson1980,
abstract = {Behavior and experience are organized around the enjoyment and pursuit of incentives. During the time that an incentive is behaviorally salient, an organism is especially responsive to incentive-related cues. This sustained sensitivity requires postulating a continuing state (denoted by a construct, current concern) with a definite onset (commitment) and offset (consummation or disengagement). Disengagement follows frustration, accompanies the behavioral process of extinction, and involves an incentive-disengagement cycle of invigoration, aggression, depression, and recovery. Depression is thus a normal part of disengagement that may be either adaptive or maladaptive for the individual but is probably adaptive for the species. The theory offers implications for motivation; etiology, symptomatology, and treatment of depression; drug use; and other social problem areas.},
author = {Ericsson, K. Anders and Simon, Herbert A.},
doi = {10.1037/0033-295X.87.3.215},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ericsson, Simon - 1980 - Verbal Reports as Data.pdf:pdf},
isbn = {0033-295X},
issn = {0033-295X},
journal = {Psychological Review},
number = {3},
pages = {215--251},
pmid = {12759483},
title = {{Verbal Reports as Data}},
volume = {87},
year = {1980}
}
@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory re-sources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demon-strate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.5401v2},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint = {arXiv:1410.5401v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves, Wayne, Danihelka - 2014 - Neural Turing Machines.pdf:pdf},
journal = {Arxiv},
pages = {1--26},
title = {{Neural Turing Machines}},
year = {2014}
}
@article{Braithwaite2012,
author = {Braithwaite, David W. and Goldstone, Robert L.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Braithwaite, Goldstone - 2012 - Inducing mathematical concepts from specific examples The role of schema-level variation.pdf:pdf},
journal = {Proceedings of the 34th Annual Conference of the Cognitive Science Society},
keywords = {analogy,comparison,instruction,mathematics,schemas,transfer},
number = {c},
pages = {138--143},
title = {{Inducing mathematical concepts from specific examples: The role of schema-level variation}},
year = {2012}
}
@misc{RepresentationsGithub,
author = {RepresentationsGithub},
title = {{Cyclic Group Representations Github (https://github.com/lampinen/fyp{\_}cyclic{\_}groups)}}
}
@misc{RonnyForsBerndtStenberg2012,
abstract = {Background. Studies have shown conflicting results on the association between nickelexposure from orthodontic appliances and nickel sensitization.Objectives {\&} Method. In a cross-sectional study, we investigated the associationbetween nickel sensitization and exposure to orthodontic appliances and piercings. 4376adolescents were patch tested following a questionnaire asking for earlier piercing andorthodontic treatment. Exposure to orthodontic appliances was verified in dental records.Results. Questionnaire data demonstrated a reduced risk of nickel sensitization whenorthodontic treatment preceded piercing (OR 0.46; CI 0.27â€“0.78). Data from dentalrecords demonstrated similar results (OR 0.61, CI 0.36â€“1.02), but statistical significancewas lost when adjusting for background factors. Exposure to full, fixed applianceswith NiTi-containing alloys (OR 0.31, CI 0.10â€“0.98) as well as a pooled â€˜high nickel-releasing' appliance group (OR 0.56, CI 0.32â€“0.97) prior to piercing was associated witha significantly reduced risk of nickel sensitization.Conclusion. High nickel-containing orthodontic appliances preceding piercingreduces the risk of nickel sensitization by a factor 1.5â€“2. The risk reduction is associatedwith estimated nickel release of the appliance and length of treatment. Sex, age at piercingand number of piercings are also important risk indicators. Research on the role of dentalmaterials in the development of immunological tolerance is needed.},
author = {{Ronny Fors, Berndt Stenberg}, Hans Stenlund and Maurits Persson},
booktitle = {Contact Dermatitis},
doi = {10.1111/j.1600-0536.2012.02097.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronny Fors, Berndt Stenberg - 2012 - Epdf @ Onlinelibrary.Wiley.Com(2).html:html},
keywords = {cross-sectional,dental braces,patch test,questionnaire,tolerance.},
number = {6},
pages = {342--350},
title = {{Epdf @ Onlinelibrary.Wiley.Com}},
volume = {67},
year = {2012}
}
@article{Kotovsky1985,
abstract = {This paper analyzes the causes for large differences in difficulty of various isomorphic versions of the Tower of Hanoi problem. Some forms of the problem take 16 times as long to solve, on average, as other versions. Since isomorphism rules out size of task domain as a determinant of relative difficulty, these experiments seek and find causes for the differences in features of the problem representation. Analysis of verbal protocols and the temporal patterning of moves allows the problem-solving behavior to be divided into exploratory and final-path phases. Entry into the final-path phase depends on acquisition of the ability to plan pairs of moves, an achievement made difficult by the working memory load it entails. This memory load can be reduced by automating the rules governing moves, either through problem exploration or training. Once automation has occurred, the solution is obtained very rapidly. Memory load is also proposed as the locus of other differences in difficulty found between various problem representations. ?? 1985.},
author = {Kotovsky, K. and Hayes, J. R. and Simon, H. A.},
doi = {10.1016/0010-0285(85)90009-X},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kotovsky, Hayes, Simon - 1985 - Why are some problems hard Evidence from Tower of Hanoi.pdf:pdf},
isbn = {0010-0285},
issn = {00100285},
journal = {Cognitive Psychology},
number = {2},
pages = {248--294},
title = {{Why are some problems hard? Evidence from Tower of Hanoi}},
volume = {17},
year = {1985}
}
@article{Zhang1997,
abstract = {This article proposes a theoretical framework for external representation based problem solving. The Tic-Tat-Toe and its isomorphs are used to illustrate the procedures of the framework as a methodology and test the predictions of the framework as a functional model. Experimental results show that the behavior in the Tic-Tat-Toe is determined by the directly available information in external and internal representations in terms of perceptual and cognitive biases, regardless of whether the biases are consistent with, inconsistent with, or irrelevant to the task. It is shown that external representations are not merely inputs and stimuli to the internal mind and that they have much more important functions than mere memory aids. A representational determinism is suggested-the form of a representation determines what information can be perceived, what processes can be activated, and what structures can be discovered from the specific representation.},
author = {Zhang, Jiajie},
doi = {10.1207/s15516709cog2102_3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - 1997 - The Nature Problem of External in Solving Representations.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
number = {2},
pages = {179--217},
pmid = {9712194170},
title = {{The Nature Problem of External in Solving Representations}},
volume = {21},
year = {1997}
}
@article{Bengio2009,
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illus- trates gradually more concepts, and gradu- ally more complex ones. Here, we formal- ize such training strategies in the context of machine learning, and call them â€œcurricu- lum learningâ€. In the context of recent re- search studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neu- ral networks), we explore curriculum learn- ing in various set-ups. The experiments show that significant improvements in generaliza- tion can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
author = {Bengio, Yoshua and Louradour, J{\'{e}}r{\^{o}}me and Collobert, Ronan and Weston, Jason},
doi = {10.1145/1553374.1553380},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - 2009 - Curriculum learning.pdf:pdf},
isbn = {9781605585161},
issn = {0022-5193},
journal = {Proceedings of the 26th annual international conference on machine learning},
pages = {41--48},
pmid = {5414602},
title = {{Curriculum learning}},
year = {2009}
}
@article{Elman1993,
abstract = {It is a striking fact that in humans the greatest learnmg occurs precisely at that point in time - childhood - when the most dramatic maturational changes also occur. This report describes possible synergistic interactions between maturational change and the ability to learn a complex domain (language), as investigated in connectionist networks. The networks are trained to process complex sentences involving relative clauses, number agreement, and several types of verb argument structure. Training fails in the case of networks which are fully formed and â€˜adultlike' in their capacity. Training succeeds only when networks begin with limited working memory and gradually â€˜mature' to the adult state. This result suggests that rather than being a limitation, developmental restrictions on resources may constitute a necessary prerequisite for mastering certain complex domains. Specifically, successful learning may depend on starting small.},
author = {Elman, Jeffrey L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Altmann - 1993 - Learning and development in neural networks the importance of starting small.pdf:pdf},
isbn = {0010-0277},
journal = {Cognition},
keywords = {Neural networks},
number = {2},
pages = {71--99},
title = {{Learning and development in neural networks: the importance of starting small}},
volume = {48},
year = {1993}
}
@article{Cohen2007,
abstract = {The relationship between parents styles of talking about past events with their children and childrens recall of stressful events was explored. In this investigation, 2- to 5-year-old childrens recall of injuries requiring hospital emergency room treatment was assessed within a few days of the injury and again 2 years later, along with the way their parents reminisced with them about the event. Correlational analyses showed that age and parental reminiscing style were consistently related to child memory; regression analyses showed that although age was most important, parents who were more elaborative had children who recalled more during their initial interview about the harder- to-remember hospital event. Thus, an elaborative parental style may help childrens recall of even highly salient and stressful events.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Cohen, C. A. and Hegarty, M.},
doi = {10.1002/acp},
eprint = {NIHMS150003},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen, Hegarty - 2007 - Individual Differences in Use of External Visualizations to Perform an Internal Visualization Task.pdf:pdf},
isbn = {1591479304},
issn = {0013127X},
journal = {Applied Cognitive Psychology},
pages = {701--711},
pmid = {73986922},
title = {{Individual Differences in Use of External Visualizations to Perform an Internal Visualization Task}},
volume = {21},
year = {2007}
}
@misc{Kohen2008,
abstract = {The present study used Canadian National Longitudinal data to examine a model of the mechanisms through which the effects of neighborhood socioeconomic conditions impact young children's verbal and behavioral outcomes (N 5 3,528; M age 5 5.05 years, SD 5 0.86). Integrating elements of social disorganization theory and family stress models, and results from structural equation models suggest that both neighborhood and family mechanisms played an important role in the transmission of neighborhood socioeconomic effects. Neighborhood disadvantage manifested its effect via lower neighborhood cohesion, which was associated with maternal depression and family dysfunction. These processes were, in turn, related to less consistent, less stimulating, and more punitive parenting behaviors, and ultimately, poorer child outcomes.},
author = {Kohen, D. and Leventhal, T. and Dahinten, V.S. and McIntosh, C.},
booktitle = {Child Development},
doi = {10.1002/14651858.CD006061.pub2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kohen et al. - 2008 - Pdf @ Onlinelibrary.Wiley.Com.html:html},
isbn = {92 832 0405 0},
issn = {00368326},
number = {1},
pages = {156--169},
pmid = {21633523},
title = {{Pdf @ Onlinelibrary.Wiley.Com}},
volume = {79},
year = {2008}
}
@article{Hegarty,
author = {Hegarty, M.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hegarty - 2004 - Diagrams in the Mind and in the World Relations between Internal and External.pdf:pdf},
title = {{Diagrams in the Mind and in the World: Relations between Internal and External}},
year = {2004}
}
@article{Atkinson2000,
abstract = {Worked examples are instructional devices that provide an expert's problem solution for a learner to study. Worked-examples research is a cognitive-experimental program that has relevance to classroom in- struction and the broader educational research community. A frame- work for organizing the findings of this research is proposed, leading to instructional design principles. For instance, one instructional de- sign principle suggests that effective examples have highly integrated components. They employ multiple modalities in presentation and em- phasize conceptual structure by labeling or segmenting. At the lesson level, effective instruction employs multiple examples for each concep- tual problem type, varies example formats within problem type, and employs surface features to signal deep structure. Also, examples should be presented in close proximity to matched practice problems. More- over, learners can be encouraged through direct training or by the structure of the worked example to actively self-explain examples. Worked examples are associated with early stages of skill develop- ment, but the design principles are relevant to constructivist research and teaching. The},
author = {Atkinson, Robert K. and Derry, Sharon J. and Renkl, Alexander and Wortham, Donald},
doi = {10.3102/00346543070002181},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Atkinson et al. - 2000 - Learning from examples Instructional principles from the worked examples research.pdf:pdf},
isbn = {0034654307000},
issn = {0034-6543},
journal = {Review of Educational Research},
number = {2},
pages = {181--214},
title = {{Learning from examples: Instructional principles from the worked examples research}},
volume = {70},
year = {2000}
}
@article{Piech2015,
abstract = {Knowledge tracingâ€”where a machine models the knowledge of a student as they interact with courseworkâ€”is a well established problem in computer supported education. Though effectively modeling student knowledge would have high ed-ucational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neu-ral networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and dis-covery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.05908v1},
author = {Piech, Chris and Spencer, Jonathan and Huang, Jonathan and Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas and Sohl-dickstein, Jascha and Academy, Khan},
eprint = {arXiv:1506.05908v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Piech et al. - 2015 - Deep Knowledge Tracing.pdf:pdf},
pages = {1--13},
title = {{Deep Knowledge Tracing}},
year = {2015}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Osindero, Teh - 2006 - A fast learning algorithm for deep belief nets.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Animals,Humans,Learning,Learning: physiology,Neural Networks (Computer),Neurons,Neurons: physiology},
number = {7},
pages = {1527--54},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets.}},
volume = {18},
year = {2006}
}
@article{Hinton2006a,
abstract = {{\%}Z {\%}U {\%}+ {\%}{\^{}}},
author = {Hinton, Geoffrey and Nair, Vinod},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Nair - 2006 - Inferring motor programs from images of handwritten digits.pdf:pdf},
isbn = {9780262232531},
issn = {10495258},
journal = {Advances in neural information processing systems},
pages = {515},
title = {{Inferring motor programs from images of handwritten digits}},
volume = {18},
year = {2006}
}
@article{Matthews2015,
author = {Matthews, P. G. and Lewis, M. R. and Hubbard, E. M.},
doi = {10.1177/0956797615617799},
issn = {0956-7976},
journal = {Psychological Science},
keywords = {15,19,21,and a,educational psychology,individual differences,know it,mathematical ability,number comprehension,perception,received 6,revision accepted 10,that a man may,what is a number},
title = {{Individual Differences in Nonsymbolic Ratio Processing Predict Symbolic Math Performance}},
year = {2015}
}
@article{Kharratzadeh,
abstract = {Bayesian models of cognition hypothesize that human brains make sense of data by representing prob-ability distributions and applying Bayes' rule to find the best explanation for any given data. Under-standing the neural mechanisms underlying probabilistic models remains important because Bayesian models essentially provide a computational framework, rather than specifying processes at the algo-rithmic level. Here, we propose a constructive neural-network model which estimates and represents probability distributions from observable events â€” a phenomenon related to the concept of proba-bility matching. We use a form of operant learning, where the underlying probabilities are learned from positive and negative reinforcements of inputs. Our model is psychologically plausible because, similar to humans, it learns to represent probabilities without receiving any representation of them from the external world, but rather by experiencing individual events. Moreover, we show that our neural implementation of probability matching can be paired with a neural module applying Bayes' rule, forming a comprehensive neural scheme to simulate human Bayesian learning and inference. Our model also provides novel explanations of several deviations from Bayes, including base-rate neglect and overweighting of rare events.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.03209v1},
author = {Kharratzadeh, Milad and Shultz, Thomas R},
eprint = {arXiv:1501.03209v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kharratzadeh, Shultz - Unknown - Neural Implementation of Probabilistic Models of Cognition.pdf:pdf},
keywords = {Base-rate Neglect,Bayesian Models,Neural Networks,Probability Matching,Reinforcement},
title = {{Neural Implementation of Probabilistic Models of Cognition}}
}
@article{Willingham2015,
abstract = {Theories of learning styles suggest that individuals think and learn best in different ways. These are not differences of ability but rather preferences for processing certain types of information or for processing information in certain types of way. If accurate, learning styles theories could have important implications for instruction because student achievement would be a product of the interaction of instruction and the student's style. There is reason to think that people view learning styles theories as broadly accurate, but, in fact, scientific support for these theories is lacking. We suggest that educators' time and energy are better spent on other theories that might aid instruction. Keywords},
author = {Willingham, D. T. and Hughes, E. M. and Dobolyi, D. G.},
doi = {10.1177/0098628315589505},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Willingham, Hughes, Dobolyi - 2015 - The Scientific Status of Learning Styles Theories.pdf:pdf},
issn = {0098-6283},
journal = {Teaching of Psychology},
keywords = {academic achievement,cognitive style,individual differences,learning styles,teaching methods},
number = {3},
pages = {266--271},
title = {{The Scientific Status of Learning Styles Theories}},
volume = {42},
year = {2015}
}
@article{Movellan1993,
abstract = {In this article we present symmetric diffusion networks, a family of networks that instantiate the principles of continuous, stochastic, adaptive and interactive propagation of information.  Using methods from Markovian diffusion theory, we formalize the activation dynamics of these networks and then show that they can be trained to reproduce entire multivariate probability distributions on their outputs using the contrastive Hebbian learning rule (CHL).  We show that CHL performs gradient descent on an error function that captures differences between desired and obtained continuous multivariate probability distributions.  This allows the learning algorithm to go beyond expected values of output units and to approximate complete probability distributions on continuous multivariate activation spaces.  We argue that learning continuous distributions is an important task underlying a variety of real life situations which were beyond the scope of previous connectionist networks.  Deterministic networks, like back propagation, cannot learn the task because they are limited to learning average values of independent output units.  Previous stochastic connectionist networks could learn probability distribtions but they were limited to discrete variables.  Simulations show that symmetric diffustion networks can be trained with the CHL rule to approximate discrete and continuous probability distributions of various types.},
author = {Movellan, Javier R. and McClelland, James L.},
doi = {10.1207/s15516709cog1704_1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Movellan, McClelland - 1993 - Learning Continuous Probability Distributions with Symmetric Diffusion Networks.pdf:pdf},
isbn = {0364-0213},
issn = {03640213},
journal = {Cognitive Science},
number = {4},
pages = {463--496},
title = {{Learning Continuous Probability Distributions with Symmetric Diffusion Networks}},
volume = {17},
year = {1993}
}
@article{Riener2010,
abstract = {There is no credible evidence that learning styles exist. While we will elaborate on this assertion, it is important to counteract the real harm that may be done by equivocating on the matter. In what follows, we will begin by defining â€œlearning stylesâ€; then we will address the claims made by those who believe that they exist, in the process acknowledging what we consider the valid claims of learning-styles theorists. But in separating the wheat from the pseudoscientific chaff in learning-styles theory, we will make clear that the wheat is contained in other educational approaches as well. A belief in learning styles is not necessary to incorporating useful knowledge about learning into one's teaching. We will then discuss the reasons why learning styles beliefs are so prevalent. Finally, we will offer suggestions about collegiate pedagogy, given that we have no evidence learning styles do not exist.},
author = {Riener, Cedar and Willingham, Daniel},
doi = {10.1080/00091383.2010.503139},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riener, Willingham - 2010 - The Myth of Learning Styles.pdf:pdf},
isbn = {00091383},
issn = {0009-1383},
journal = {Change: The Magazine of Higher Learning},
number = {5},
pages = {32--35},
pmid = {53306479},
title = {{The Myth of Learning Styles}},
volume = {42},
year = {2010}
}
@article{Xu2015,
author = {Xu, Y. and Franconeri, S. L.},
doi = {10.1177/0956797615585002},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Franconeri - 2015 - Capacity for Visual Features in Mental Rotation.pdf:pdf},
isbn = {8474917859},
issn = {0956-7976},
journal = {Psychological Science},
keywords = {10,14,15,16,attention,capacity,creating and transforming representations,eye tracking,mental rotation,of structure is,received 6,revision accepted 4,selection,tracking,visual working memory},
pmid = {26174781},
title = {{Capacity for Visual Features in Mental Rotation}},
year = {2015}
}
@article{Pashler2008,
abstract = {The term â€œlearning stylesâ€ refers to the concept that individuals differ in regard to what mode of instruction or study is most effective for them. Proponents of learning-style assessment contend that optimal instruction requires diagnosing individuals' learning style and tailoring instruction accordingly. Assessments of learning style typically ask people to evaluate what sort of information presentation they prefer (e.g., words versus pictures versus speech) and/or what kind of mental activity they find most engaging or congenial (e.g., analysis versus listening), although assessment instruments are extremely diverse. The most commonâ€”but not the onlyâ€”hypothesis about the instructional relevance of learning styles is the meshing hypothesis, according to which instruction is best provided in a format that matches the preferences of the learner (e.g., for a â€œvisual learner,â€ emphasizing visual presentation of information).The learning-styles view has acquired great influence within the education field, and is frequently encountered at levels ranging from kindergarten to graduate school. There is a thriving industry devoted to publishing learning-styles tests and guidebooks for teachers, and many organizations offer professional development workshops for teachers and educators built around the concept of learning styles.The authors of the present review were charged with determining whether these practices are supported by scientific evidence. We concluded that any credible validation of learning-styles-based instruction requires robust documentation of a very particular type of experimental finding with several necessary criteria. First, students must be divided into groups on the basis of their learning styles, and then students from each group must be randomly assigned to receive one of multiple instructional methods. Next, students must then sit for a final test that is the same for all students. Finally, in order to demonstrate that optimal learning requires that students receive instruction tailored to their putative learning style, the experiment must reveal a specific type of interaction between learning style and instructional method: Students with one learning style achieve the best educational outcome when given an instructional method that differs from the instructional method producing the best outcome for students with a different learning style. In other words, the instructional method that proves most effective for students with one learning style is not the most effective method for students with a different learning style.Our review of the literature disclosed ample evidence that children and adults will, if asked, express preferences about how they prefer information to be presented to them. There is also plentiful evidence arguing that people differ in the degree to which they have some fairly specific aptitudes for different kinds of thinking and for processing different types of information. However, we found virtually no evidence for the interaction pattern mentioned above, which was judged to be a precondition for validating the educational applications of learning styles. Although the literature on learning styles is enormous, very few studies have even used an experimental methodology capable of testing the validity of learning styles applied to education. Moreover, of those that did use an appropriate method, several found results that flatly contradict the popular meshing hypothesis.We conclude therefore, that at present, there is no adequate evidence base to justify incorporating learning-styles assessments into general educational practice. Thus, limited education resources would better be devoted to adopting other educational practices that have a strong evidence base, of which there are an increasing number. However, given the lack of methodologically sound studies of learning styles, it would be an error to conclude that all possible versions of learning styles have been tested and found wanting; many have simply not been tested at all. Further research on the use of learning styles assessment in instruction may in some cases be warranted, but such research needs to be performed appropriately.},
author = {Pashler, Harold and McDaniel, Mark and Rohrer, Doug and Bjork, Robert},
doi = {10.1111/j.1539-6053.2009.01038.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pashler et al. - 2008 - Learning Styles Concepts and Evidence.pdf:pdf},
isbn = {1539-6053},
issn = {15291006},
journal = {Psychological Science in the Public Interest},
number = {3},
pages = {105--119},
pmid = {21197874},
title = {{Learning Styles: Concepts and Evidence}},
volume = {9},
year = {2008}
}
@article{Goldin2011,
abstract = {Two thousand four hundred years ago Socrates gave a remarkable lesson of geometry, perhaps the first detailed record of a pedagogical method in vivo in history [Plato. (2008). Apolog{\'{i}}a de S{\'{o}}crates. Men{\'{o}}n. Cr{\'{a}}tilo. Madrid: Alianza Editorial]. Socrates asked Meno's slave 50 questions requiring simple additions or multiplications. At the end of the lesson the student discovered by himself how to duplicate a square using the diagonal of the given one as the side of the new square. We studied empirically the reproducibility of this dialogue in educated adults and adolescents of the 21st century. Our results show a remarkable agreement between Socratic and empiric dialogues. Even in questions in which Meno's slave made a mistake, within an unbounded number of possible erred responses, the vast majority of participants produced the same error as Meno's slave. Our results show that the Socratic dialogue is built on a strong intuition of human knowledge and reasoning which persists more than 24 centuries after its conception, providing one of the most striking demonstrations of universality across time and cultures. At the same time, they also emphasize its educational failure. After following every single question including Socrates' "diagonal argument," almost 50{\%} of the participants failed to learn the simplest generalization when asked to double the area of a square of different size. {\textcopyright} 2011 The Authors. Journal Compilation {\textcopyright} 2011 International Mind, Brain, and Education Society and Blackwell Publishing, Inc.},
author = {Goldin, Andrea P. and Pezzatti, Laura and Battro, Antonio M. and Sigman, Mariano},
doi = {10.1111/j.1751-228X.2011.01126.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldin et al. - 2011 - From ancient Greece to modern education Universality and lack of generalization of the socratic dialogue.pdf:pdf},
isbn = {17512271 (ISSN)},
issn = {17512271},
journal = {Mind, Brain, and Education},
number = {4},
pages = {180--185},
title = {{From ancient Greece to modern education: Universality and lack of generalization of the socratic dialogue}},
volume = {5},
year = {2011}
}
@article{Barnett2002,
abstract = {Despite a century's worth of research, arguments surrounding the question of whether far transfer occurs have made little progress toward resolution. The authors argue the reason for this confusion is a failure to specify various dimensions along which transfer can occur, resulting in comparisons of "apples and oranges." They provide a framework that describes 9 relevant dimensions and show that the literature can productively be classified along these dimensions, with each study situated at the intersection of various dimensions. Estimation of a single effect size for far transfer is misguided in view of this complexity. The past 100 years of research shows that evidence for transfer under some conditions is substantial, but critical conditions for many key questions are untested.},
author = {Barnett, Susan M and Ceci, Stephen J},
doi = {10.1037/0033-2909.128.4.612},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barnett, Ceci - 2002 - When and where do we apply what we learn A taxonomy for far transfer.pdf:pdf},
isbn = {0033-2909},
issn = {0033-2909},
journal = {Psychological bulletin},
number = {4},
pages = {612--637},
pmid = {12081085},
title = {{When and where do we apply what we learn? A taxonomy for far transfer.}},
volume = {128},
year = {2002}
}
@article{Kawashima2004a,
author = {Kawashima, Ryuta and Taira, Masato and Okita, Katsuo and Inoue, Kentaro and Tajima, Nobumoto and Yoshida, Hajime and Sasaki, Takeo and Sugiura, Motoaki and Watanabe, Job and Fukuda, Hiroshi},
doi = {10.1016/j.cogbrainres.2003.10.009},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawashima et al. - 2004 - A functional MRI study of simple arithmeticâ€”a comparison between children and adults.pdf:pdf},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {adults,children,functional mri study},
number = {3},
pages = {227--233},
title = {{A functional MRI study of simple arithmeticâ€”a comparison between children and adults}},
volume = {18},
year = {2004}
}
@article{Kawashima2004,
abstract = {The purpose of this study was to examine brain areas involved in simple arithmetic, and to compare these areas between adults and children. Eight children (four girls and four boys; age, 9-14 years) and eight adults (four women and four men; age, 40-49 years) were subjected to this study. Functional magnetic resonance imaging (fMRI) was performed during mental calculation of addition, subtraction, and multiplication of single digits. In each group, the left middle frontal, bilateral inferior temporal and bilateral lateral occipital cortices were activated during each task. The adult group showed activation of the right frontal cortex during addition and multiplication tasks, but the children group did not. Activation of the intraparietal cortex was observed in the adult group during each task. Although, activation patterns were slightly different among tasks, as well as between groups, only a small number of areas showed statistically significant differences. The results indicate that cortical networks involved in simple arithmetic are similar among arithmetic operations, and may not show significant changes in the structure during the second decade of life. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Kawashima, Ryuta and Taira, Masato and Okita, Katsuo and Inoue, Kentaro and Tajima, Nobumoto and Yoshida, Hajime and Sasaki, Takeo and Sugiura, Motoaki and Watanabe, Job and Fukuda, Hiroshi},
doi = {10.1016/j.cogbrainres.2003.10.009},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawashima et al. - 2004 - A functional MRI study of simple arithmeticâ€”a comparison between children and adults.pdf:pdf;:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawashima et al. - 2004 - A functional MRI study of simple arithmetic - A comparison between children and adults.html:html},
isbn = {1053-8119},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Adults,Children,Functional MRI study,adults,children,functional mri study},
number = {3},
pages = {227--233},
title = {{A functional MRI study of simple arithmeticâ€”a comparison between children and adults}},
volume = {18},
year = {2004}
}
@article{Menon2002,
author = {Menon, Vinod and Mackenzie, Katherine and Rivera, Susan Michelle and Reiss, Allan Leonard},
doi = {10.1002/hbm.10035},
issn = {1065-9471},
journal = {Human Brain Mapping},
keywords = {angular gyrus,arithmetic,fmri,interference,n400,prefrontal cortex,stroop},
number = {2},
pages = {119--130},
title = {{Prefrontal cortex involvement in processing incorrect arithmetic equations: Evidence from event-related fMRI}},
volume = {16},
year = {2002}
}
@misc{V.Menon,
abstract = {Themainaimofthisstudywastoinvestigatethedifferentialprocessingofcorrectandincorrectequationstogainfurtherinsightintotheneuralprocessesinvolvedinarithmeticreasoning.Electrophysio-logicalstudiesinhumanshavedemonstratedthatprocessingincorrectarithmeticequations(e.g.,2â«¹2â«½5)elicitsaprominentevent-relatedpotential(ERP)comparedtoprocessingcorrectequations(e.g.,2â«¹2â«½4).Inthepresentstudy,weinvestigatedtheneuralsubstratesofthisprocessusingevent-relatedfunctionalmagneticresonanceimaging(fMRI).Subjectswerepresentedwitharithmeticequationsandaskedtoindicatewhetherthesolutiondisplayedwascorrectorincorrect.Wefoundgreateractivationtoincorrect,comparedtocorrectequations,intheleftdorsolateralprefrontalcortex(DLPFC,BA46)andtheleftventrolateralprefrontalcortex(VLPFC,BA47).Ourresultsprovidethefirstbrainimagingevidencefordifferentialprocessingofincorrectvs.correctequations.Theprefrontalcortexactivationobservedinprocessingincorrectequationsoverlapswithbrainareasknowntobeinvolvedinworkingmemoryandinterferenceprocessing.TheDLPFCregiondifferentiallyactivatedbyincorrectequationswasalsoinvolvedinoverallarithmeticprocessing,whereastheVLPFCwasactivatedonlyduringthedifferentialprocessingofincorrectequations.Differentialresponsetocorrectandincorrectarithmeticequationswasnotobservedinparietalcortexregionssuchastheangulargyrusandintra-parietalsulcus,whichareknowntoplayaspecificroleinperformingarithmeticcomputations.Thepatternofbrainresponseobservedisconsistentwiththehypothesisthatprocessingincorrectequationsinvolvesdetectionofanincorrectanswerandresolutionoftheinterferencebetweentheinternallycomputedandexternallypresentedincorrectanswer.Morespecifically,greateractivationduringprocessingofincorrectequationsappearstoreflectadditionaloperationsinvolvedinmaintainingtheresultsinworkingmemory,whilesubjectsattempttoresolvetheconflictandselectaresponse.Thesefindingsallowustofurtherdelineateanddissociatethecontributionsofprefrontalandparietalcorticestoarithmeticreasoning},
author = {{et al. V. Menon}},
doi = {http://onlinelibrary.wiley.com/doi/10.1002/bjs.5677/pdf},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/V. Menon - Unknown - Prefrontal cortex involvement in processing incorrect arithmetic equations Evidence from event-related fMRI.html:html},
title = {{Prefrontal cortex involvement in processing incorrect arithmetic equations: Evidence from event-related fMRI}}
}
@article{Fyfe2014,
abstract = {A longstanding debate concerns the use of concrete versus abstract instructional materials, particularly in domains such as mathematics and science. Although decades of research have focused on the advantages and disadvantages of concrete and abstract materials considered independently, we argue for an approach that moves beyond this dichotomy and combines their advantages. Specifically, we recommend beginning with concretematerials and then explicitly and gradually fading to the more abstract. Theoretical benefits of this â€œcon- creteness fadingâ€ technique for mathematics and science instruction include (1) helping learners interpret ambiguous or opaque abstract symbols in terms of well-understood concrete objects, (2) providing embodied perceptual and physical experiences that can ground abstract thinking, (3) enabling learners to build up a store of memorable images that can be used when abstract symbols lose meaning, and (4) guiding learners to strip away extraneous concrete properties and distill the generic, generalizable properties. In these ways, concreteness fading provides advantages that go beyond the sum of the benefits of concrete and abstract materials},
author = {Fyfe, Emily R and McNeil, Nicole M and Son, Ji Y and Goldstone, Robert L},
doi = {10.1007/s10648-014-9249-3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyfe et al. - 2014 - Concreteness Fading in Mathematics and Science Instruction A Systematic Review.pdf:pdf},
issn = {1040726X},
journal = {Educational Psychology Review},
keywords = {Abstract symbols,Concrete manipulatives,Learning and instruction},
number = {1},
pages = {9--25},
title = {{Concreteness Fading in Mathematics and Science Instruction: A Systematic Review}},
volume = {26},
year = {2014}
}
@article{Sweller1983,
abstract = {Previous studies have found that the strategies used by expert and novice problem solvers differ. Novices tend to use means-ends analysis, which involves working backward from the goal, whereas experts prefer to work forward from the givens of a problem. Experiment 1 was designed to study the course of development of expertise using a subset of kinematics problems. After solving many problems, subjects demonstrated the switch from a means-ends to a forward-chaining strategy. This was associated with the conventional concomitants of expertise such as a decrease in the number of moves required for solution. In addition, the speed at which expertise developed varied for different categories of problems. Subjects appeared to categorize problems according to the order in which equations would be required, with these categories being discovered at nonuniform rates. This was assumed to be due to the differential rate of acquisition of schemas associated with different categories of problems. Experiments 2 and 3, again using kinematics problems, tested the hypothesis that the means-ends strategies used by novices retarded the acquisition of appropriate'schemas. It was suggested that under a means-ends strategy, moves are controlled by the problem goal, which reduces the information obtained by problem solvers concerning problem structure. The use of nonspecific rather than specific goals was found to enhance the acquisition of expertise as measured by the use of a forward-oriented strategy, the number of moves required for solution, and the number of equations written without substitutions. Experiments 4 and 5, using geometry problems, duplicated the enhanced rate of strategy alteration found with reduced goal specificity. The results of Experiments 6 and 7, again using geometry problems, indicated that reduced goal specificity also enhanced the rate at which problem solvers induced appropriate problem categories. It was concluded that in circumstances in which the primary reason for presenting problems is to assist problem solvers in acquiring knowledge concerning problem structure, the use of conventional problems solved by means-ends analysis may not be maximally efficient.},
author = {Sweller, John and Mawer, Robert F and Ward, Mark R},
doi = {10.1037/0096-3445.112.4.639},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sweller, Mawer, Ward - 1983 - Development of expertise in mathematical problem solving.pdf:pdf},
issn = {0096-3445},
journal = {Journal of Experimental Psychology: General},
number = {4},
pages = {639--661},
pmid = {1120},
title = {{Development of expertise in mathematical problem solving.}},
volume = {112},
year = {1983}
}
@article{Kawashima2004,
abstract = {The purpose of this study was to examine brain areas involved in simple arithmetic, and to compare these areas between adults and children. Eight children (four girls and four boys; age, 9-14 years) and eight adults (four women and four men; age, 40-49 years) were subjected to this study. Functional magnetic resonance imaging (fMRI) was performed during mental calculation of addition, subtraction, and multiplication of single digits. In each group, the left middle frontal, bilateral inferior temporal and bilateral lateral occipital cortices were activated during each task. The adult group showed activation of the right frontal cortex during addition and multiplication tasks, but the children group did not. Activation of the intraparietal cortex was observed in the adult group during each task. Although, activation patterns were slightly different among tasks, as well as between groups, only a small number of areas showed statistically significant differences. The results indicate that cortical networks involved in simple arithmetic are similar among arithmetic operations, and may not show significant changes in the structure during the second decade of life. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Kawashima, Ryuta and Taira, Masato and Okita, Katsuo and Inoue, Kentaro and Tajima, Nobumoto and Yoshida, Hajime and Sasaki, Takeo and Sugiura, Motoaki and Watanabe, Job and Fukuda, Hiroshi},
doi = {10.1016/j.cogbrainres.2003.10.009},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawashima et al. - 2004 - A functional MRI study of simple arithmetic - A comparison between children and adults.html:html},
isbn = {1053-8119},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Adults,Children,Functional MRI study},
number = {3},
pages = {225--231},
title = {{A functional MRI study of simple arithmetic - A comparison between children and adults}},
volume = {18},
year = {2004}
}
@article{Blair2008,
abstract = {ABSTRACT â€” This article examines the role of working memo- ry, attention shifting, and inhibitory control executive cogni- tive functions in the development of mathematics knowledge and ability in children. It suggests that an examination of the executive cognitive demand of mathematical thinking can complement procedural and conceptual knowledge-based approaches to understanding the ways in which children become profi cient in mathematics. Task analysis indicates that executive cognitive functions likely operate in concert with procedural and conceptual knowledge and in some instances might act as a unique infl uence on mathematics problem-solving ability. It is concluded that consideration of the executive cognitive demand of mathematics can contribute to research on best practices in mathematics education.},
author = {Blair, Clancy and Knipe, Hilary and Gamson, David},
doi = {10.1111/j.1751-228X.2008.00036.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blair, Knipe, Gamson - 2008 - Is there a role for executive functions in the development of mathematics ability.pdf:pdf},
isbn = {1751-228X},
issn = {17512271},
journal = {Mind, Brain, and Education},
number = {2},
pages = {80--89},
title = {{Is there a role for executive functions in the development of mathematics ability?}},
volume = {2},
year = {2008}
}
@article{Gross-Tsur1996,
abstract = {One hundred and forty-three 11-year-old children with development dyscalculia, from a cohort of 3029 students, were studied to determine demographic features and prevalence of this primary cognitive disorder. They were evaluated for gender, IQ, linguistic and perceptual skills, symptoms of attention-deficit hyperactivity disorder (ADHD), socio-economic status and associated learned disabilities. The IQs of the 140 children (75 girls and 65 boys) retained in the study group (three were excluded because of low IQs) ranged from 80 to 129 (mean 98.2, SD 9.9). 26 per cent of the children had symptoms of ADHD, and 17 per cent had dyslexia. Their socio-economic status was significantly lower than that of the rest of the cohort, and 42 per cent had first-degree relatives with learning disabilities. The prevalence of dyscalculia in the original cohort was 6.5 per cent, similar to that of dyslexia and ADHD. However, unlike these other learning disabilities, dyscalculia affected the two sexes in about the same proportions.},
author = {Gross-Tsur, V and Manor, O and Shalev, R S},
doi = {10.1007/s007870070009},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gross-Tsur, Manor, Shalev - 1996 - Developmental dyscalculia prevalence and demographic features.pdf:pdf},
isbn = {1469-8749},
issn = {0012-1622},
journal = {Developmental medicine and child neurology},
keywords = {developmental dyscalculia,Ã° prevalence Ã° prognosis},
number = {1},
pages = {25--33},
pmid = {8606013},
title = {{Developmental dyscalculia: prevalence and demographic features.}},
volume = {38},
year = {1996}
}
@article{Wille2005,
abstract = {Formal Concept Analysis has been originally developed as a subfield of Applied Mathematics based on the mathematization of concept and concept hierarchy. Only after more than a decade of development, the connections to the philosophical logic of human thought became clearer and even later the connections to Piaget's cognitive structuralism which Thomas Bernhard Seiler convincingly elaborated to a comprehensive theory of concepts in his recent book [Se01]. It is the main concern of this paper to show the surprisingly rich correspondences between Seiler's multifarious aspects of concepts in the human mind and the structural properties and relationships of formal concepts in Formal Concept Analysis. These correspondences make understandable, what has been experienced in a great multitude of applications, that Formal Concept Analysis may function in the sense of transdisciplinary mathematics, i.e., it allows mathematical thought to aggregate with other ways of thinking and thereby to support human thought and action.},
author = {Wille, Rudolf},
doi = {10.1007/11528784_1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wille - 2005 - Formal Concept Analysis as Mathematical Theory of Concepts and Concept Hierarchies.pdf:pdf},
isbn = {978-3-540-31881-1},
journal = {Formal Concept Analysis - Foundations and Applications},
pages = {1--33},
title = {{Formal Concept Analysis as Mathematical Theory of Concepts and Concept Hierarchies}},
year = {2005}
}
@article{DeSmedt2013,
abstract = {Many studies tested the association between numerical magnitude processing and mathematics achievement, results differ depending on the number format used. For symbolic numbers (digits), data are consistent and robust across studies and populations: weak performance correlates with low math achievement and dyscalculia. For non-symbolic formats (dots), many conflicting findings have been reported. These inconsistencies might be explained by methodological issues. Alternatively, it might be that the processes measured by non-symbolic tasks are not critical for school-relevant mathematics. A few neuroimaging studies revealed that brain activation during number comparison correlates with the children's mathematics achievement level, but the consistency of such relationships for symbolic and non-symbolic processing is unclear. These neurocognitive data provide ground for educational interventions, which seem to have positive effects on children's numerical development in (a)typical populations.},
author = {{De Smedt}, Bert and No{\"{e}}l, Marie-Pascale and Gilmore, Camilla and Ansari, Daniel},
doi = {10.1023/A},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Smedt et al. - 2013 - The relationship between symbolic and non-symbolic numerical magnitude processing skills and the typical and at.pdf:pdf},
isbn = {1024471506938},
journal = {Trends in Neuroscience {\&} Education},
pages = {48--55},
title = {{The relationship between symbolic and non-symbolic numerical magnitude processing skills and the typical and atypical development of mathematics: a review of evidence from brain and behavior}},
volume = {2},
year = {2013}
}
@article{Jones2009,
abstract = {In this article, I respond to the recent work of Kaminski, Sloutsky, and Heckler (2008a). I advance two major concerns about their research and its applicability to learning mathematics: a confounding variable that arises from the mathematical differences between the generic examples and concrete examples poses a threat to the construct validity of the experiments, and the overgeneralization of the success of the treatment, given that the measure of success is a prompted near-transfer task.},
author = {Jones, Matthew G},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jones - 2009 - Transfer, Abstraction, and Context.pdf:pdf},
isbn = {0021-8251},
issn = {0021-8251},
journal = {Journal for Research in Mathematics Education},
keywords = {Abstraction,Transfer},
number = {2},
pages = {80--89},
title = {{Transfer, Abstraction, and Context}},
volume = {40},
year = {2009}
}
@article{Kaminski2009,
abstract = {What factors affect transfer of knowledge is a complex question. In recent research, the authors demonstrated that concreteness of the learning domain is one such factor (Kaminski, Sloutsky, {\&} Heckler, 2008). Even when prompted and given no time delay, participants who learned a concrete instantiation of a mathematical concept failed to transfer their knowledge to a novel analogous situation.},
author = {Kaminski, Jennifer a. and Sloutsky, Vladimir M. and Heckler, Andrew F.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaminski, Sloutsky, Heckler - 2009 - Concrete Instantiations of Mathematics A Double-Edged Sword.html:html},
isbn = {0021-8251},
journal = {Journal for Research in Mathematics Education},
pages = {90--93},
title = {{Concrete Instantiations of Mathematics: A Double-Edged Sword}},
volume = {40},
year = {2009}
}
@article{Bennett2011,
author = {Bennett, Craig M and Baird, Abigail a and Miller, Michael B and Wolford, George L},
doi = {10.1016/S1053-8119(09)71202-9},
isbn = {1053-8119},
issn = {10538119},
journal = {Journal of Serendipitous and Unexpected Results},
pages = {1--5},
pmid = {20060480},
title = {{Neural Correlates of Interspecies Perspective Taking in the Post-Mortem Atlantic Salmon: An Argument For Proper Multiple Comparisons Correction}},
volume = {1},
year = {2011}
}
@book{Lang2002,
author = {Lang, Serge},
booktitle = {Graduate Texts in Mathematics-New York-},
edition = {Rev. 3rd},
isbn = {038795385X},
pages = {914},
publisher = {Springer Graduate Texts in Mathematics},
title = {{Algebra (revised third edition)}},
year = {2002}
}
@article{Lesser1998,
author = {Lesser, Larry},
doi = {10.1111/j.1467-9639.1998.tb00750.x},
issn = {0141-982X},
journal = {Teaching Statistics},
number = {1},
pages = {10--12},
title = {{Countering Indifference Using Counterintuitive Examples}},
volume = {20},
year = {1998}
}
@article{Burger1986,
abstract = {This study provides a description of the van Hiele levels of reasoning in geometry according to responses to clinical interview tasks concerning triangles and quadrilaterals. The subjects were 13 students from Grades 1 through 12 plus a university mathematics major. The tasks included drawing shapes, identifying and defining shapes, sorting shapes, determining a mystery shape, establishing properties of parallelograms, and comparing components of a mathematical system. The students' behavior on the tasks was consistent with the van Hieles' original general description of the levels, although the discreteness of levels, particularly of analysis and abstraction, was not confirmed. The use of formal deduction among students who were taking or had taken secondary school geometry was nearly absent, consistent with earlier observations by Usiskin (1982).},
author = {Burger, Wf and Shaughnessy, Jm},
doi = {10.2307/749317},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burger, Shaughnessy - 1986 - Characterizing the van Hiele Levels of Development in Geometry.pdf:pdf},
issn = {0021-8251},
journal = {Journal for research in mathematics {\ldots}},
number = {1},
pages = {31--48},
title = {{Characterizing the van Hiele Levels of Development in Geometry}},
volume = {17},
year = {1986}
}
@article{Nathan2012,
abstract = {I explore a belief about learning and teaching that is commonly held in education and society at large that nonetheless is deeply flawed. The belief asserts that mastery of formalismsâ€”specialized representations such as symbolic equations and diagrams with no inherent meaning except that which is established by conventionâ€”is prerequisite to applied knowledge. A formalisms first (FF) view of learning, rooted in Western dualist philosophy, incorrectly advocates the introduction of formalisms too early in the development of learners' conceptual understanding and can encourage a formalisms-only mind-set toward learning and instruction. I identify the prevalence of FF in curriculum and instruction and outline some of the serious problems engendered by FF approaches. I then turn to promising alternatives that support progressive formalization, problem-based learning, and inquiry learning, which capitalize on the strengths of formalisms but avoid some of the most costly problems found in FF approaches.},
author = {Nathan, Mitchell J.},
doi = {10.1080/00461520.2012.667063},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nathan - 2012 - Rethinking Formalisms in Formal Education.pdf:pdf},
issn = {0046-1520},
journal = {Educational Psychologist},
number = {2},
pages = {125--148},
title = {{Rethinking Formalisms in Formal Education}},
volume = {47},
year = {2012}
}
@misc{V.Menon,
abstract = {Themainaimofthisstudywastoinvestigatethedifferentialprocessingofcorrectandincorrectequationstogainfurtherinsightintotheneuralprocessesinvolvedinarithmeticreasoning.Electrophysio-logicalstudiesinhumanshavedemonstratedthatprocessingincorrectarithmeticequations(e.g.,2â«¹2â«½5)elicitsaprominentevent-relatedpotential(ERP)comparedtoprocessingcorrectequations(e.g.,2â«¹2â«½4).Inthepresentstudy,weinvestigatedtheneuralsubstratesofthisprocessusingevent-relatedfunctionalmagneticresonanceimaging(fMRI).Subjectswerepresentedwitharithmeticequationsandaskedtoindicatewhetherthesolutiondisplayedwascorrectorincorrect.Wefoundgreateractivationtoincorrect,comparedtocorrectequations,intheleftdorsolateralprefrontalcortex(DLPFC,BA46)andtheleftventrolateralprefrontalcortex(VLPFC,BA47).Ourresultsprovidethefirstbrainimagingevidencefordifferentialprocessingofincorrectvs.correctequations.Theprefrontalcortexactivationobservedinprocessingincorrectequationsoverlapswithbrainareasknowntobeinvolvedinworkingmemoryandinterferenceprocessing.TheDLPFCregiondifferentiallyactivatedbyincorrectequationswasalsoinvolvedinoverallarithmeticprocessing,whereastheVLPFCwasactivatedonlyduringthedifferentialprocessingofincorrectequations.Differentialresponsetocorrectandincorrectarithmeticequationswasnotobservedinparietalcortexregionssuchastheangulargyrusandintra-parietalsulcus,whichareknowntoplayaspecificroleinperformingarithmeticcomputations.Thepatternofbrainresponseobservedisconsistentwiththehypothesisthatprocessingincorrectequationsinvolvesdetectionofanincorrectanswerandresolutionoftheinterferencebetweentheinternallycomputedandexternallypresentedincorrectanswer.Morespecifically,greateractivationduringprocessingofincorrectequationsappearstoreflectadditionaloperationsinvolvedinmaintainingtheresultsinworkingmemory,whilesubjectsattempttoresolvetheconflictandselectaresponse.Thesefindingsallowustofurtherdelineateanddissociatethecontributionsofprefrontalandparietalcorticestoarithmeticreasoning},
author = {et al. {V. Menon}},
doi = {http://onlinelibrary.wiley.com/doi/10.1002/bjs.5677/pdf},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/V. Menon - Unknown - Prefrontal cortex involvement in processing incorrect arithmetic equations Evidence from event-related fMRI.html:html},
title = {{Prefrontal cortex involvement in processing incorrect arithmetic equations: Evidence from event-related fMRI}}
}
@article{Menon2002,
author = {Menon, Vinod and Mackenzie, Katherine and Rivera, Susan Michelle and Reiss, Allan Leonard},
doi = {10.1002/hbm.10035},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Menon et al. - 2002 - Prefrontal cortex involvement in processing incorrect arithmetic equations Evidence from event-related fMRI.pdf:pdf},
issn = {1065-9471},
journal = {Human Brain Mapping},
keywords = {angular gyrus,arithmetic,fmri,interference,n400,prefrontal cortex,stroop},
number = {2},
pages = {119--130},
title = {{Prefrontal cortex involvement in processing incorrect arithmetic equations: Evidence from event-related fMRI}},
volume = {16},
year = {2002}
}
@article{Grabner2009,
author = {Grabner, Roland H. and Ansari, Daniel and Koschutnig, Karl and Reishofer, Gernot and Ebner, Franz and Neuper, Christa},
doi = {10.1016/j.neuropsychologia.2008.10.013},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grabner et al. - 2009 - To retrieve or to calculate Left angular gyrus mediates the retrieval of arithmetic facts during problem solving.pdf:pdf},
issn = {00283932},
journal = {Neuropsychologia},
number = {2},
pages = {604--608},
title = {{To retrieve or to calculate? Left angular gyrus mediates the retrieval of arithmetic facts during problem solving}},
volume = {47},
year = {2009}
}
@misc{RonnyForsBerndtStenberg2012a,
abstract = {Background. Studies have shown conflicting results on the association between nickelexposure from orthodontic appliances and nickel sensitization.Objectives {\&} Method. In a cross-sectional study, we investigated the associationbetween nickel sensitization and exposure to orthodontic appliances and piercings. 4376adolescents were patch tested following a questionnaire asking for earlier piercing andorthodontic treatment. Exposure to orthodontic appliances was verified in dental records.Results. Questionnaire data demonstrated a reduced risk of nickel sensitization whenorthodontic treatment preceded piercing (OR 0.46; CI 0.27â€“0.78). Data from dentalrecords demonstrated similar results (OR 0.61, CI 0.36â€“1.02), but statistical significancewas lost when adjusting for background factors. Exposure to full, fixed applianceswith NiTi-containing alloys (OR 0.31, CI 0.10â€“0.98) as well as a pooled â€˜high nickel-releasing' appliance group (OR 0.56, CI 0.32â€“0.97) prior to piercing was associated witha significantly reduced risk of nickel sensitization.Conclusion. High nickel-containing orthodontic appliances preceding piercingreduces the risk of nickel sensitization by a factor 1.5â€“2. The risk reduction is associatedwith estimated nickel release of the appliance and length of treatment. Sex, age at piercingand number of piercings are also important risk indicators. Research on the role of dentalmaterials in the development of immunological tolerance is needed.},
author = {{Ronny Fors, Berndt Stenberg}, Hans Stenlund and Maurits Persson},
booktitle = {Contact Dermatitis},
doi = {10.1111/j.1600-0536.2012.02097.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronny Fors, Berndt Stenberg - 2012 - Epdf @ Onlinelibrary.Wiley.Com.html:html},
keywords = {cross-sectional,dental braces,patch test,questionnaire,tolerance.},
number = {6},
pages = {342--350},
title = {{Epdf @ Onlinelibrary.Wiley.Com}},
volume = {67},
year = {2012}
}
@article{Kong2005,
abstract = {Recent functional neuroimaging studies have begun to clarify how the human brain performs the everyday activities that require mental calculation. We used fMRI to test the hypotheses that there are specific neural networks dedicated to performing an arithmetic operation (e.g. + or -) and to performing processes that support more complex calculations. We found that the right inferior parietal lobule, left precuneus and left superior parietal gyrus are relatively specific for performing subtraction; and bilateral medial frontal/cingulate cortex are relatively specific for supporting arithmetic procedure complexity. We also found that greater difficulty level was associated with activation in a brain network including left inferior intraparietal sulcus, left inferior frontal gyrus and bilateral cingulate. Our results suggest that the network activated by the simplest calculation serves as a common basis, to which more regions are recruited for more difficult problems or different arithmetic operations. ?? 2004 Elsevier B.V. All rights reserved.},
author = {Kong, Jian and Wang, Chunmao and Kwong, Kenneth and Vangel, Mark and Chua, Elizabeth and Gollub, Randy},
doi = {10.1016/j.cogbrainres.2004.09.011},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kong et al. - 2005 - The neural substrate of arithmetic operations and procedure complexity.pdf:pdf},
isbn = {0926-6410 (Print)},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Brain network,Calculation,Mental arithmetic,fMRI},
number = {3},
pages = {397--405},
pmid = {15722210},
title = {{The neural substrate of arithmetic operations and procedure complexity}},
volume = {22},
year = {2005}
}
@article{Fischer1980,
abstract = {Skill theory provides tools for predicting developmental sequences and synchronies in any domain at any point in development by integrating behavioral and cognitive-developmental concepts. Cognitive development is explained by skill structures called "levels," together with transformation rules relating these levels to each other. The transformation rules specify the developmental steps by which a skill moves gradually from one level of complexity to the next. At every step in these developmental sequences, the individual controls a particular skill. Skills are gradually transformed from sensory-motor actions to representations and then to abstractions. The transformations produce continuous behavioral changes; but across the entire profile of a person's skills and within highly practiced task domains, a stagelike shift in skills occurs as the person develops to an optimal level. The theory suggests a common framework for integrating developmental analyses of cognitive, social, language, and perceptual-motor skills and certain behavioral changes in learning and problem solving. (6 p ref) (PsycINFO Database Record (c) 2010 APA )},
author = {Fischer, K W},
doi = {10.1037/0033-295x.87.6.477},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer - 1980 - A theory of cognitive development The control and construction of hierarchies of skills.pdf:pdf},
isbn = {0033-295X$\backslash$r1939-1471},
issn = {0033-295X},
journal = {Psychological Review},
keywords = {2140 History {\&} Systems,2820 Cognitive {\&} Perceptual,Ability,Cognitive Development,Development,Literature Review,Theories,skill theory as predictor of cognitive development},
number = {6},
pages = {477--531},
title = {{A theory of cognitive development: The control and construction of hierarchies of skills}},
volume = {87},
year = {1980}
}
@article{DeSmedt2011,
abstract = {Most studies on mathematics learning in the field of educational neuroscience have focused on the neural correlates of very elementary numerical processing skills in children. Little is known about more complex mathematical skills that are formally taught in school, such as arithmetic. Using functional magnetic resonance imaging, the present study investigated how brain activation during single-digit addition and subtraction is modulated by problem size and arithmetic operation in 28 children aged 10-12. years with different levels of arithmetical fluency. Commensurate with adult data, large problems and subtractions activated a fronto-parietal network, including the intraparietal sulci, the latter of which indicates the influence of quantity-based processes during procedural strategy execution. Different from adults, the present findings revealed that particularly the left hippocampus was active during the solution of those problems that are expected to be solved by means of fact retrieval (i.e. small problems and addition), suggesting a specific role of the hippocampus in the early stages of learning arithmetic facts. Children with low levels of arithmetical fluency showed higher activation in the right intraparietal sulcus during the solution of problems with a relatively small problem size, indicating that they continued to rely to a greater extent on quantity-based strategies on those problems that the children with relatively higher arithmetical fluency already retrieved from memory. This might represent a neural correlate of fact retrieval impairments in children with mathematical difficulties. ?? 2010 Elsevier Inc.},
author = {{De Smedt}, Bert and Holloway, Ian D. and Ansari, Daniel},
doi = {10.1016/j.neuroimage.2010.12.037},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Smedt, Holloway, Ansari - 2011 - Effects of problem size and arithmetic operation on brain activation during calculation in children.pdf:pdf},
isbn = {1053-8119;},
issn = {10538119},
journal = {NeuroImage},
keywords = {Arithmetic fluency,Fact retrieval,Hippocampus,Intraparietal sulcus,Problem size effect,Procedural strategies},
number = {3},
pages = {771--781},
pmid = {21182966},
publisher = {Elsevier Inc.},
title = {{Effects of problem size and arithmetic operation on brain activation during calculation in children with varying levels of arithmetical fluency}},
volume = {57},
year = {2011}
}
@article{Desco2011,
abstract = {NeuroImage, 57 (2011) 281-292. doi:10.1016/j.neuroimage.2011.03.063},
author = {Desco, M and Navas-Sanchez, F J and Sanchez-Gonz{\'{a}}lez, J and Reig, S and Robles, O and Franco, C and Guzm{\'{a}}n-De-Villoria, J a and Garc{\'{i}}a-Barreno, P and Arango, C},
doi = {10.1016/j.neuroimage.2011.03.063},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Desco et al. - 2011 - Mathematically gifted adolescents use more extensive and more bilateral areas of the fronto-parietal network than.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$r1053-8119 (Linking)},
issn = {1053-8119},
journal = {NeuroImage},
keywords = {Complexity,Intelligence,Raven matrices,Tower of London,Visuospatial,Working memory,fMRI},
number = {1},
pages = {281--292},
pmid = {21463696},
publisher = {Elsevier Inc.},
title = {{Mathematically gifted adolescents use more extensive and more bilateral areas of the fronto-parietal network than controls during executive functioning and fluid reasoning tasks}},
volume = {57},
year = {2011}
}
@article{Davis2009,
abstract = {Most studies investigating mental numerical processing involve adult participants and little is known about the functioning of these systems in children. The current study used functional magnetic resonance imaging (fMRI) to investigate the neural correlates of numeracy and the influence of age on these correlates with a group of adults and a group of third graders who had average to above average mathematical ability. Participants performed simple and complex versions of exact and approximate calculation tasks while in the magnet. Like adults, children activated a network of brain regions in the frontal and parietal lobes during the calculation tasks, and they recruited additional brain regions for the more complex versions of the tasks. However, direct comparisons between adults and children revealed significant differences in level of activation across all tasks. In particular, patterns of activation in the parietal lobe were significantly different as a function of age. Findings support previous claims that the parietal lobe becomes more specialized for arithmetic tasks with age.},
author = {Davis, Nicole and Cannistraci, Christopher J. and Rogers, Baxter P. and Gatenby, J. Christopher and Fuchs, Lynn S. and Anderson, Adam W. and Gore, John C.},
doi = {10.1016/j.mri.2009.05.010},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis et al. - 2009 - The neural correlates of calculation ability in children an fMRI study.pdf:pdf},
isbn = {1873-5894 (Electronic)$\backslash$r1873-5894 (Linking)},
issn = {0730725X},
journal = {Magnetic Resonance Imaging},
keywords = {Arithmetic,Mathematical skill,Numerical processing,School-age,fMRI},
number = {9},
pages = {1187--1197},
pmid = {19570639},
publisher = {Elsevier B.V.},
title = {{The neural correlates of calculation ability in children: an fMRI study}},
volume = {27},
year = {2009}
}
@article{Pesenti2001,
abstract = {Calculating prodigies are individuals who are exceptional at quickly and accurately solving complex mental calculations. With positron emission tomography (PET), we investigated the neural bases of the cognitive abilities of an expert calculator and a group of non-experts, contrasting complex mental calculation to memory retrieval of arithmetic facts. We demonstrated that calculation expertise was not due to increased activity of processes that exist in non-experts; rather, the expert and the non-experts used different brain areas for calculation. We found that the expert could switch between short-term effort-requiring storage strategies and highly efficient episodic memory encoding and retrieval, a process that was sustained by right prefrontal and medial temporal areas.},
annote = {Uncorrected p-values},
author = {Pesenti, M and Zago, L and Crivello, F and Mellet, E and Samson, D and Duroux, B and Seron, X and Mazoyer, B and Tzourio-Mazoyer, N},
doi = {10.1038/82831},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pesenti et al. - 2001 - Mental calculation in a prodigy is sustained by right prefrontal and medial temporal areas.pdf:pdf},
isbn = {1097-6256 (Print)$\backslash$r1097-6256 (Linking)},
issn = {1097-6256},
journal = {Nature neuroscience},
pages = {103--107},
pmid = {11135652},
title = {{Mental calculation in a prodigy is sustained by right prefrontal and medial temporal areas.}},
volume = {4},
year = {2001}
}
@article{Dehaene2003,
abstract = {Did evolution endow the human brain with a predisposition to represent and acquire knowledge about numbers? Although the parietal lobe has been suggested as a potential substrate for a domain-specific representation of quantities, it is also engaged in verbal, spatial, and attentional functions that may contribute to calculation. To clarify the organisation of number-related processes in the parietal lobe, we examine the three-dimensional intersection of fMRI activations during various numerical tasks, and also review the corresponding neuropsychological evidence. On this basis, we propose a tentative tripartite organisation. The horizontal segment of the intraparietal sulcus (HIPS) appears as a plausible candidate for domain specificity: It is systematically activated whenever numbers are manipulated, independently of number notation, and with increasing activation as the task puts greater emphasis on quantity processing. Depending on task demands, we speculate that this core quantity system, analogous to an internal "number line," can be supplemented by two other circuits. A left angular gyrus area, in connection with other left-hemispheric perisylvian areas, supports the manipulation of numbers in verbal form. Finally, a bilateral posterior superior parietal system supports attentional orientation on the mental number line, just like on any other spatial dimension.},
author = {Dehaene, Stanislas and Piazza, Manuela and Pinel, Philippe and Cohen, Laurent},
doi = {10.1080/02643290244000239},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dehaene et al. - 2003 - Three parietal circuits for number processing.pdf:pdf},
isbn = {0264329024400},
issn = {0264-3294},
journal = {Cognitive neuropsychology},
number = {July 2015},
pages = {487--506},
pmid = {20957581},
title = {{Three parietal circuits for number processing.}},
volume = {20},
year = {2003}
}
@article{Dehaene1999,
abstract = {Does the human capacity for mathematical intuition depend on linguistic competence or on visuo-spatial representations? A series of behavioral and brain-imaging experiments provides evidence for both sources. Exact arithmetic is acquired in a language-specific format, transfers poorly to a different language or to novel facts, and recruits networks involved in word-association processes. In contrast, approximate arithmetic shows language independence, relies on a sense of numerical magnitudes, and recruits bilateral areas of the parietal lobes involved in visuo-spatial processing. Mathematical intuition may emerge from the interplay of these brain systems.},
author = {Dehaene, S. and Spelke, E. and Pinel, P. and Stanescu, R. and Tsivkin, S.},
doi = {10.1126/science.284.5416.970},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dehaene et al. - 1999 - Sources of Mathematical Thinking Behavioral and Brain-Imaging Evidence.pdf:pdf},
isbn = {00368075 10959203},
issn = {0036-8075, 1095-9203},
journal = {Science},
number = {5416},
pages = {970--974},
pmid = {10320379},
title = {{Sources of Mathematical Thinking: Behavioral and Brain-Imaging Evidence}},
volume = {284},
year = {1999}
}
@article{Dix2015,
abstract = {This study investigates cognitive resource allocation dependent on fluid and numerical intelligence in arithmetic/algebraic tasks varying in difficulty. Sixty-six 11th grade students participated in a mathematical verification paradigm, while pupil dilation as a measure of resource allocation was collected. Students with high fluid intelligence solved the tasks faster and more accurately than those with average fluid intelligence, as did students with high compared to average numerical intelligence. However, fluid intelligence sped up response times only in students with average but not high numerical intelligence. Further, high fluid but not numerical intelligence led to greater task-related pupil dilation. We assume that fluid intelligence serves as a domain-general resource that helps to tackle problems for which domain-specific knowledge (numerical intelligence) is missing. The allocation of this resource can be measured by pupil dilation.},
author = {Dix, Annika and van der Meer, Elke},
doi = {10.1111/psyp.12367},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dix, van der Meer - 2015 - Arithmetic and algebraic problem solving and resource allocation The distinct impact of fluid and numerical i.pdf:pdf},
issn = {00485772},
journal = {Psychophysiology},
number = {4},
pages = {544--554},
pmid = {25327870},
title = {{Arithmetic and algebraic problem solving and resource allocation: The distinct impact of fluid and numerical intelligence}},
volume = {52},
year = {2015}
}
@article{Price2013,
author = {Price, G. R. and Mazzocco, M. M. M. and Ansari, D.},
doi = {10.1523/JNEUROSCI.2936-12.2013},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Price, Mazzocco, Ansari - 2013 - Why Mental Arithmetic Counts Brain Activation during Single Digit Arithmetic Predicts High School Math.pdf:pdf},
issn = {0270-6474},
journal = {Journal of Neuroscience},
number = {1},
pages = {156--163},
title = {{Why Mental Arithmetic Counts: Brain Activation during Single Digit Arithmetic Predicts High School Math Scores}},
volume = {33},
year = {2013}
}
@article{OBoyle2008,
annote = {Such arrogance about aptitude},
author = {O'Boyle, Michael W.},
doi = {10.1080/02783190802199594},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Boyle - 2008 - Mathematically Gifted Children Developmental Brain Characteristics and Their Prognosis for Well-Being.pdf:pdf},
issn = {0278-3193},
journal = {Roeper Review},
number = {3},
pages = {181--186},
title = {{Mathematically Gifted Children: Developmental Brain Characteristics and Their Prognosis for Well-Being}},
volume = {30},
year = {2008}
}
@article{Logothetis2004,
author = {Logothetis, Nikos K. and Wandell, Brian a.},
doi = {10.1146/annurev.physiol.66.082602.092845},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Logothetis, Wandell - 2004 - Interpreting the BOLD Signal.pdf:pdf},
issn = {0066-4278},
journal = {Annual Review of Physiology},
keywords = {blood-oxygen-level-dependent,broad community of scientists,fmri,has brought together a,imaging,interested in measuring the,local field,multiunit activity,of functional magnetic resonance,potential,s abstract the development,visual cortex},
number = {1},
pages = {735--769},
title = {{Interpreting the BOLD Signal}},
volume = {66},
year = {2004}
}
@article{OBoyle2005,
abstract = {Mental rotation involves the creation and manipulation of internal images, with the later being particularly useful cognitive capacities when applied to high-level mathematical thinking and reasoning. Many neuroimaging studies have demonstrated mental rotation to be mediated primarily by the parietal lobes, particularly on the right side. Here, we use fMRI to show for the first time that when performing 3-dimensional mental rotations, mathematically gifted male adolescents engage a qualitatively different brain network than those of average math ability, one that involves bilateral activation of the parietal lobes and frontal cortex, along with heightened activation of the anterior cingulate. Reliance on the processing characteristics of this uniquely bilateral system and the interplay of these anterior/posterior regions may be contributors to their mathematical precocity. ?? 2005 Elsevier B.V. All rights reserved.},
author = {O'Boyle, Michael W. and Cunnington, Ross and Silk, Timothy J. and Vaughan, David and Jackson, Graeme and Syngeniotis, Ari and Egan, Gary F.},
doi = {10.1016/j.cogbrainres.2005.08.004},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Boyle et al. - 2005 - Mathematically gifted male adolescents activate a unique brain network during mental rotation.pdf:pdf},
isbn = {0926-6410 (Print)$\backslash$r0926-6410 (Linking)},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Mathematical giftedness,Mental rotation,Sketch,Visuospatial processing,fMRI},
mendeley-tags = {Sketch},
number = {2},
pages = {583--587},
pmid = {16150579},
title = {{Mathematically gifted male adolescents activate a unique brain network during mental rotation}},
volume = {25},
year = {2005}
}
@article{Davis2013,
author = {Davis, Tyler and Poldrack, Russell a.},
doi = {10.1111/nyas.12156},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Poldrack - 2013 - Measuring neural representations with fMRI practices and pitfalls.pdf:pdf},
issn = {00778923},
journal = {Annals of the New York Academy of Sciences},
keywords = {adaptation,fmri,mvpa,representation},
number = {1},
pages = {108--134},
title = {{Measuring neural representations with fMRI: practices and pitfalls}},
volume = {1296},
year = {2013}
}
@article{Fangmeier2006,
author = {Fangmeier, T and Knauff, Markus and Ruff, Cc and Sloutsky, V},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fangmeier et al. - 2006 - FMRI-Evidence for a Three-Stage-Model of Deductive Reasoning.pdf:pdf},
journal = {Journal of Cognitive {\ldots}},
title = {{FMRI-Evidence for a Three-Stage-Model of Deductive Reasoning}},
year = {2006}
}
@article{Varma2008,
abstract = {Background: There is increasing interest in applying neuroscience findings to topics in education. Purpose: This application requires a proper conceptualisation of the relation between cognition and brain function. This paper considers two such conceptualisations. The area focus understands each cognitive competency as the product of one (and only one) brain area. The network focus explains each cognitive competency as the product of collaborative processing among multiple brain areas. Sources of evidence: We first review neuroscience studies of mathematical reasoningâ€“specifically arithmetic problem-solving and magnitude comparisonâ€“that exemplify the area focus and network focus. We then review neuroscience findings that illustrate the potential of the network focus for informing three topics in mathematics education: the development of mathematical reasoning, the effects of practice and instruction, and the derailment of mathematical reasoning in dyscalculia. Main argument: Although the area focus has historically dominated discussions in educational neuroscience, we argue that the network focus offers a complementary perspective on brain function that should not be ignored. Conclusions: We conclude by describing the current limitations of network-focus theorising and emerging neuroscience methods that promise to make such theorising more tractable in the future.},
author = {Varma, Sashank and Schwartz, Daniel L.},
doi = {10.1080/00131880802082633},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Varma, Schwartz - 2008 - How should educational neuroscience conceptualise the relation between cognition and brain function Mathematica.pdf:pdf},
isbn = {00131881},
issn = {0013-1881},
journal = {Educational Research},
number = {2},
pages = {149--161},
title = {{How should educational neuroscience conceptualise the relation between cognition and brain function? Mathematical reasoning as a network process}},
volume = {50},
year = {2008}
}
@article{Qin2004,
abstract = {In a brain imaging study of children learning algebra, it is shown that the same regions are active in children solving equations as are active in experienced adults solving equations. As with adults, practice in symbol manipulation produces a reduced activation in prefrontal cortex area. However, unlike adults, practice seems also to produce a decrease in a parietal area that is holding an image of the equation. This finding suggests that adolescents' brain responses are more plastic and change more with practice. These results are integrated in a cognitive model that predicts both the behavioral and brain imaging results.},
author = {Qin, Yulin and Carter, Cameron S and Silk, Eli M and Stenger, V Andrew and Fissell, Kate and Goode, Adam and Anderson, John R},
doi = {10.1073/pnas.0401227101},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qin et al. - 2004 - The change of the brain activation patterns as children learn algebra equation solving.pdf:pdf},
isbn = {0027-8424 (Print)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
pages = {5686--5691},
pmid = {15064407},
title = {{The change of the brain activation patterns as children learn algebra equation solving.}},
volume = {101},
year = {2004}
}
@article{Farah2014,
author = {Farah, Martha J.},
doi = {10.1002/hast.295},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farah - 2014 - Brain Images, Babies, and Bathwater iCritiquing Critiques of Functional Neuroimagingi.pdf:pdf},
issn = {00930334},
journal = {Hastings Center Report},
number = {s2},
pages = {S19--S30},
title = {{Brain Images, Babies, and Bathwater: {\textless}i{\textgreater}Critiquing Critiques of Functional Neuroimaging{\textless}/i{\textgreater}}},
volume = {44},
year = {2014}
}
@article{Krueger2008,
abstract = {Only a subset of adults acquires specific advanced mathematical skills, such as integral calculus. The representation of more sophisticated mathematical concepts probably evolved from basic number systems; however its neuroanatomical basis is still unknown. Using fMRI, we investigated the neural basis of integral calculus while healthy participants were engaged in an integration verification task. Solving integrals activated a left-lateralized cortical network including the horizontal intraparietal sulcus, posterior superior parietal lobe, posterior cingulate gyrus, and dorsolateral prefrontal cortex. Our results indicate that solving of more abstract and sophisticated mathematical facts, such as calculus integrals, elicits a pattern of brain activation similar to the cortical network engaged in basic numeric comparison, quantity manipulation, and arithmetic problem solving. },
author = {Krueger, F and Spampinato, M V and Pardini, M and Pajevic, S and Wood, J N and Weiss, G H and Landgraf, S and Grafman, J},
doi = {10.1097/WNR.0b013e328303fd85},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krueger et al. - 2008 - Integral calculus problem solving an fMRI investigation.pdf:pdf},
isbn = {0959-4965},
issn = {0959-4965},
journal = {Neuroreport},
keywords = {arithmetic,dorsolateral prefrontal cortex,intraparietal sulcus,mathematics,superior parietal lobe},
number = {11},
pages = {1095--1099},
pmid = {18596607},
title = {{Integral calculus problem solving: an fMRI investigation}},
volume = {19},
year = {2008}
}
@article{Kucian2008,
abstract = {Neuroimaging findings in adults suggest exact and approximate number processing relying on distinct neural circuits. In the present study we are investigating whether this cortical specialization is already established in 9- and 12-year-old children. Using fMRI, brain activation was measured in 10 third- and 10 sixth-grade school children and 20 adults during trials of symbolic approximate (AP) and exact (EX) calculation, as well as non-symbolic magnitude comparison (MC) of objects. Children activated similar networks like adults, denoting an availability and a similar spatial extent of specified networks as early as third grade. However, brain areas related to number processing become further specialized with schooling. Children showed weaker activation in the intraparietal sulcus during all three tasks, in the left inferior frontal gyrus during EX and in occipital areas during MC. In contrast, activation in the anterior cingulate gyrus, a region associated with attentional effort and working memory load, was enhanced in children. Moreover, children revealed reduced or absent deactivation of regions involved in the so-called default network during symbolic calculation, suggesting a rather general developmental effect. No difference in brain activation patterns between AP and EX was found. Behavioral results indicated major differences between children and adults in AP and EX, but not in MC. Reaction time and accuracy rate were not correlated to brain activation in regions showing developmental changes suggesting rather effects of development than performance differences between children and adults. In conclusion, increasing expertise with age may lead to more automated processing of mental arithmetic, which is reflected by improved performance and by increased brain activation in regions related to number processing and decreased activation in supporting areas.},
author = {Kucian, Karin and von Aster, Michael and Loenneker, Thomas and Dietrich, Thomas and Martin, Ernst},
doi = {10.1080/87565640802101474},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kucian et al. - 2008 - Development of neural networks for exact and approximate calculation a FMRI study.pdf:pdf},
isbn = {1532-6942},
issn = {8756-5641},
journal = {Developmental neuropsychology},
number = {4},
pages = {447--473},
pmid = {18568899},
title = {{Development of neural networks for exact and approximate calculation: a FMRI study.}},
volume = {33},
year = {2008}
}
@article{Delazer2003,
author = {Delazer, M and Domahs, F and Bartha, L and Brenneis, C and Lochy, a and Trieb, T and Benke, T},
doi = {10.1016/j.cogbrainres.2003.09.005},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Delazer et al. - 2003 - Learning complex arithmeticâ€”an fMRI study.pdf:pdf},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {angular gyrus,arithmetic,fmri,learning},
number = {1},
pages = {76--88},
title = {{Learning complex arithmeticâ€”an fMRI study}},
volume = {18},
year = {2003}
}
@article{Prabhakaran2001,
abstract = {Brain activation was examined using functional magnetic resonance imaging during mathematical problem solving in 7 young healthy participants. Problems were selected from the Necessary Arithmetic Operations Test (NAOT; R. B. Ekstrom, J. W. French, H. H. Harman, {\&} D. Dermen, 1976). Participants solved 3 types of problems: 2-operation problems requiring mathematical reasoning and text processing, 1-operation problems requiring text processing but minimal mathematical reasoning, and 0-operation problems requiring minimal text processing and controlling sensorimotor demands of the NAOT problems. Two-operation problems yielded major activations in bilateral frontal regions similar to those found in other problem-solving tasks, indicating that the processes mediated by these regions subserve many forms of reasoning. Findings suggest a dissociation in mathematical problem solving between reasoning, mediated by frontal cortex, and text processing, mediated by temporal cortex.},
author = {Prabhakaran, V and Rypma, B and Gabrieli, J D},
doi = {10.1037/0894-4105.15.1.115},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Prabhakaran, Rypma, Gabrieli - 2001 - Neural substrates of mathematical reasoning a functional magnetic resonance imaging study of neoco.pdf:pdf},
isbn = {0894-4105 (Print)},
issn = {0894-4105},
journal = {Neuropsychology},
number = {1},
pages = {115--127},
pmid = {11216882},
title = {{Neural substrates of mathematical reasoning: a functional magnetic resonance imaging study of neocortical activation during performance of the necessary arithmetic operations test.}},
volume = {15},
year = {2001}
}
@article{Markant2014,
abstract = {People can test hypotheses through either selection or reception. In a selection task, the learner actively chooses observations to test his or her beliefs, whereas in reception tasks data are passively encountered. People routinely use both forms of testing in everyday life, but the critical psychological differences between selection and reception learning remain poorly understood. One hypothesis is that selection learning improves learning performance by enhancing generic cognitive processes related to motivation, attention, and engagement. Alternatively, we suggest that differences between these 2 learning modes derives from a hypothesis-dependent sampling bias that is introduced when a person collects data to test his or her own individual hypothesis. Drawing on influential models of sequential hypothesis-testing behavior, we show that such a bias (a) can lead to the collection of data that facilitates learning compared with reception learning and (b) can be more effective than observing the selections of another person. We then report a novel experiment based on a popular category learning paradigm that compares reception and selection learning. We additionally compare selection learners to a set of "yoked" participants who viewed the exact same sequence of observations under reception conditions. The results revealed systematic differences in performance that depended on the learner's role in collecting information and the abstract structure of the problem.},
author = {Markant, Douglas B and Gureckis, Todd M},
doi = {10.1037/a0032108},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Markant, Gureckis - 2014 - Is it better to select or to receive Learning via active and passive hypothesis testing.pdf:pdf},
isbn = {1939-2222 (Electronic)$\backslash$n0022-1015 (Linking)},
issn = {1939-2222},
journal = {Journal of experimental psychology. General},
keywords = {bayesian modeling,category learning,dependent sampling bias,hypothesis testing,hypothesis-,self-directed learning},
number = {1},
pages = {94--122},
pmid = {23527948},
title = {{Is it better to select or to receive? Learning via active and passive hypothesis testing.}},
volume = {143},
year = {2014}
}
@article{Love2004,
author = {Love, Bradley C. and Medin, Douglas L. and Gureckis, Todd M.},
doi = {10.1037/0033-295X.111.2.309},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Love, Medin, Gureckis - 2004 - SUSTAIN A Network Model of Category Learning.pdf:pdf},
issn = {1939-1471},
journal = {Psychological Review},
number = {2},
pages = {309--332},
title = {{SUSTAIN: A Network Model of Category Learning.}},
volume = {111},
year = {2004}
}
@article{Weber2004,
abstract = {It is widely accepted by mathematics educators and mathematicians that most proof-oriented university mathematics courses are taught in a "definition-theorem-proof" format. However, there are relatively few empirical studies on what takes place during this instruction, why this instruction is used, and how it affects students' learning. In this paper, I investigate these issues by examining a case study of one professor using this type of instruction in an introductory real analysis course. I first describe the professor's actions in the classroom and argue that these actions are the result of the professor's beliefs about mathematics, students, and education, as well as his knowledge of the material being covered. I then illustrate how the professor's teaching style influenced the way that his students attempted to learn the material. Finally, I discuss the implications that the reported data have on mathematics education research. ?? 2004 Elsevier Inc. All rights reserved.},
author = {Weber, Keith},
doi = {10.1016/j.jmathb.2004.03.001},
isbn = {0732-3123},
issn = {07323123},
journal = {Journal of Mathematical Behavior},
keywords = {Advanced mathematical thinking,Collegiate mathematics education,Lecture,Mathematics education,Proof,Real analysis,Teaching},
pages = {115--133},
title = {{Traditional instruction in advanced mathematics courses: A case study of one professor's lectures and proofs in an introductory real analysis course}},
volume = {23},
year = {2004}
}
@article{Hazzan1999,
annote = {Abstraction:
1) Quality of relationships between object of thought and person
- Familiarity, etc.
2) process-object duality/conversion
-E.g. from structural understanding of group as how to perform operation to group as object
-E.g. commutativity as something that happens when *I* do things with group, vs. abstract property 
-Difference between explicit and implicit knowledge in terms of process vs object
3) degree of complexity of object of thought
-E.g. replacing set with an exemplar from it, reasoning about cyclic groups generally in terms of a specific example},
author = {Hazzan, Orit},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hazzan - 1999 - Reducing Abstraction Level When Learning Abstract Algebra Concepts.pdf:pdf},
journal = {Educational Studies in Mathematics},
pages = {71--90},
title = {{Reducing Abstraction Level When Learning Abstract Algebra Concepts}},
volume = {40},
year = {1999}
}
@article{Weber2001,
abstract = {The ability to construct proofs is an important skill for all mathematicians. Despite its importance, students have great difficulty with this task. In this paper, I first demonstrate that undergraduates often are aware of and able to apply the facts required to prove a statement but still fail to prove it. They thus fail to construct a proof because they could not use the syntactic knowledge that they had. By comparing doctoral students and undergraduates constructing proofs in abstract algebra, I have hypothesized four types of `strategic knowledge' â€“ knowledge of how to choose which facts and theorems to apply â€“ which the doctoral students appeared to possess and undergraduates did not. The doctoral students appeared to know the powerful proof techniques in abstract algebra, which theorems are most important, when particular facts and theorems are likely to be useful, and when one should or should not try and prove theorems using symbol manipulation.},
author = {Weber, Keith},
doi = {10.1023/A:1015535614355},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weber - 2001 - Student difficulty in constructing proofs The need for strategic knowledge.pdf:pdf},
issn = {0013-1954, 1573-0816},
journal = {Educational Studies in Mathematics},
keywords = {abstract algebra,group theory,homomorphism,mathematics education},
number = {1},
pages = {101--119},
title = {{Student difficulty in constructing proofs: The need for strategic knowledge}},
volume = {48},
year = {2001}
}
@article{Leron1995,
abstract = {The authors discuss an alternative method for teaching abstract algebra, involving computer investigations and small group as well as large group discussions. The article is structured as a discussion between a traditional instructor and the authors and includes several examples of classroom interactions as well as worksheets used in computer investigations.},
author = {Leron, Uri and Dubinsky, Ed},
doi = {10.2307/2975010},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leron, Dubinsky - 1995 - An Abstract Algebra Story.pdf:pdf},
issn = {0002-9890},
journal = {American Mathematical Monthly},
keywords = {Abstract Algebra,Construct Paper,MTHED 527,Undergraduate education},
number = {3},
pages = {227--242},
title = {{An Abstract Algebra Story}},
volume = {102},
year = {1995}
}
@article{Son2008,
author = {Son, Ji Y. and Smith, Linda B. and Goldstone, Robert L.},
doi = {10.1016/j.cognition.2008.05.002},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Son, Smith, Goldstone - 2008 - Simplicity and generalization Short-cutting abstraction in children's object categorizations.pdf:pdf},
issn = {00100277},
journal = {Cognition},
number = {3},
pages = {626--638},
title = {{Simplicity and generalization: Short-cutting abstraction in children's object categorizations}},
volume = {108},
year = {2008}
}
@article{Kaminski2008,
abstract = {Undergraduate students may benefit more from learning mathematics through a single abstract, symbolic representation than from learning multiple concrete examples.},
author = {Kaminski, Jennifer a. and Sloutsky, Vladimir M. and Heckler, Andrew F.},
doi = {10.1126/science.1154659},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaminski, Sloutsky, Heckler - 2008 - The Advantage of Abstract Examples in Learning Math.pdf:pdf},
isbn = {00368075 (ISSN)},
issn = {0036-8075, 1095-9203},
journal = {Science},
number = {5875},
pages = {454--455},
pmid = {18436760},
title = {{The Advantage of Abstract Examples in Learning Math}},
volume = {320},
year = {2008}
}
@article{Happel1994,
author = {Happel, B and Murre, J M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Happel, Murre - 1994 - Design and Evolution of Modular Neural Network Architectures.html:html},
journal = {Neural Networks},
number = {6/7},
pages = {985--1004},
title = {{Design and Evolution of Modular Neural Network Architectures}},
volume = {7},
year = {1994}
}
@incollection{Nathan2008,
abstract = {The need to understand and predict behaviour in complex settings such as the classroom and the workplace elevates the importance of the role of context and communication in building models of cognition. Embodied cognition is an emerging framework for under- standing intellectual behaviour in relation to the physical and social environment and to the perception- and action-based systems of the body. By reconsidering cognition with regard to interactions with the world, rather than in terms of the sequestered computa- tional nature of the mind, embodied cognition recasts many of the central issues of the study of thought and behaviour. One of the ways that cognition is seen as embodied is through the close relation of hand gestures with thinking and communication. In this chapter, I investigate how gestures enact symbols and thereby ground the meaning of abstract representations used in instructional settings.},
author = {Nathan, Mitchell J},
booktitle = {Symbols and embodiment: Debates on meaning and cognition},
doi = {10.1093/acprof:oso/9780199217274.003.0018},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nathan - 2008 - An embodied cognition perspective on symbols, gesture, and grounding instruction.pdf:pdf},
isbn = {9780191696060},
pages = {375--396},
title = {{An embodied cognition perspective on symbols, gesture, and grounding instruction}},
volume = {18},
year = {2008}
}
@article{Tadepalli2004,
abstract = {Relational reinforcement learning (RRL) is both a young and an old eld. In this paper, we trace the history of the eld to related disciplines, outline some current work and promising new directions, and survey the research issues and opportunities that lie ahead.},
author = {Tadepalli, Prasad and Givan, Robert and Driessens, Kurt},
doi = {10.1109/ICAL.2009.5262787},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tadepalli, Givan, Driessens - 2004 - Relational reinforcement learning An overview.pdf:pdf},
issn = {00747696},
journal = {ICML workshop on Relational Reinforcement Learning},
pages = {1--9},
title = {{Relational reinforcement learning: An overview}},
volume = {4},
year = {2004}
}
@article{Shaughnessy2015,
author = {Shaughnessy, J Michael and Burger, William F and Burger, F},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shaughnessy, Burger, Burger - 2015 - Spadework in Geometry Prior to Deduction Secondary school students.pdf:pdf},
number = {6},
pages = {419--428},
title = {{Spadework in Geometry Prior to Deduction Secondary school students}},
volume = {78},
year = {2015}
}
@article{Kaminski2008a,
abstract = {Undergraduate students may benefit more from learning mathematics through a single abstract, symbolic representation than from learning multiple concrete examples.},
author = {Kaminski, Jennifer a and Sloutsky, Vladimir M and Heckler, Andrew F},
doi = {10.1126/science.1154659},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaminski, Sloutsky, Heckler - 2008 - Learning theory. The advantage of abstract examples in learning math.pdf:pdf},
isbn = {00368075 (ISSN)},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
number = {5875},
pages = {454--455},
pmid = {18436760},
title = {{Learning theory. The advantage of abstract examples in learning math.}},
volume = {320},
year = {2008}
}
@article{Mccallum2008,
author = {Mccallum, William},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mccallum - 2008 - Commentary on Kaminski et al , The Advantage of Abstract Examples in Learning Math , An uncontrolled variable.pdf:pdf},
journal = {Group},
number = {April},
pages = {1--3},
title = {{Commentary on Kaminski et al , The Advantage of Abstract Examples in Learning Math , An uncontrolled variable}},
year = {2008}
}
@article{Sweller1983,
abstract = {Previous studies have found that the strategies used by expert and novice problem solvers differ. Novices tend to use means-ends analysis, which involves working backward from the goal, whereas experts prefer to work forward from the givens of a problem. Experiment 1 was designed to study the course of development of expertise using a subset of kinematics problems. After solving many problems, subjects demonstrated the switch from a means-ends to a forward-chaining strategy. This was associated with the conventional concomitants of expertise such as a decrease in the number of moves required for solution. In addition, the speed at which expertise developed varied for different categories of problems. Subjects appeared to categorize problems according to the order in which equations would be required, with these categories being discovered at nonuniform rates. This was assumed to be due to the differential rate of acquisition of schemas associated with different categories of problems. Experiments 2 and 3, again using kinematics problems, tested the hypothesis that the means-ends strategies used by novices retarded the acquisition of appropriate'schemas. It was suggested that under a means-ends strategy, moves are controlled by the problem goal, which reduces the information obtained by problem solvers concerning problem structure. The use of nonspecific rather than specific goals was found to enhance the acquisition of expertise as measured by the use of a forward-oriented strategy, the number of moves required for solution, and the number of equations written without substitutions. Experiments 4 and 5, using geometry problems, duplicated the enhanced rate of strategy alteration found with reduced goal specificity. The results of Experiments 6 and 7, again using geometry problems, indicated that reduced goal specificity also enhanced the rate at which problem solvers induced appropriate problem categories. It was concluded that in circumstances in which the primary reason for presenting problems is to assist problem solvers in acquiring knowledge concerning problem structure, the use of conventional problems solved by means-ends analysis may not be maximally efficient.},
author = {Sweller, John and Mawer, Robert F. and Ward, Mark R.},
doi = {10.1037/0096-3445.112.4.639},
issn = {0096-3445},
journal = {Journal of Experimental Psychology: General},
number = {4},
pages = {639--661},
pmid = {1120},
title = {{Development of expertise in mathematical problem solving.}},
volume = {112},
year = {1983}
}
@article{DeBock2011,
abstract = {Kaminski, Sloutsky, and Heckler (2008a) published in Science a study on "The advantage of abstract examples in learning math," in which they claim that students may benefit more from learning mathematics through a single abstract, symbolic representation than from multiple concrete examples. This publication elicited both enthusiastic and critical comments by mathematicians, mathematics educators, and policymakers worldwide. The current empirical study involves a partial replication-but also an important validation and extension-of this widely noticed study.},
author = {{De Bock}, Dirk and Deprez, Johan and {Van Dooren}, Wim and Roelens, Michel and Verschaffel, Lieven},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Bock et al. - 2011 - Abstract or Concrete Examples in Learning Mathematics A Replication and Elaboration of Kaminski, Sloutsky, and H.pdf:pdf},
journal = {Journal for Research in Mathematics Education},
keywords = {01,2006,algebra,college,developing adap-,education,from the research fund,k,learning,leuven,research issues,supported by grant goa,testing,this research was partially,tive expertise in mathematics,u,university},
number = {2},
pages = {109},
title = {{Abstract or Concrete Examples in Learning Mathematics? A Replication and Elaboration of Kaminski, Sloutsky, and Heckler's Study}},
volume = {42},
year = {2011}
}
@article{Fyfe2014,
abstract = {A longstanding debate concerns the use of concrete versus abstract instructional materials, particularly in domains such as mathematics and science. Although decades of research have focused on the advantages and disadvantages of concrete and abstract materials considered independently, we argue for an approach that moves beyond this dichotomy and combines their advantages. Specifically, we recommend beginning with concretematerials and then explicitly and gradually fading to the more abstract. Theoretical benefits of this â€œcon- creteness fadingâ€ technique for mathematics and science instruction include (1) helping learners interpret ambiguous or opaque abstract symbols in terms of well-understood concrete objects, (2) providing embodied perceptual and physical experiences that can ground abstract thinking, (3) enabling learners to build up a store of memorable images that can be used when abstract symbols lose meaning, and (4) guiding learners to strip away extraneous concrete properties and distill the generic, generalizable properties. In these ways, concreteness fading provides advantages that go beyond the sum of the benefits of concrete and abstract materials},
author = {Fyfe, Emily R. and McNeil, Nicole M. and Son, Ji Y. and Goldstone, Robert L.},
doi = {10.1007/s10648-014-9249-3},
issn = {1040726X},
journal = {Educational Psychology Review},
keywords = {Abstract symbols,Concrete manipulatives,Learning and instruction},
number = {1},
pages = {9--25},
title = {{Concreteness Fading in Mathematics and Science Instruction: A Systematic Review}},
volume = {26},
year = {2014}
}
@article{Sweller1982,
abstract = {Meansâ€“ends analysis is a mechanism that is assumed to operate when people solve transformation problems. Its use is affected by the extent to which the goal is clearly specified to the problem solver as a problem state and by the extent to which learning occurs during a problem-solving episode. Five maze-tracing experiments were conducted with 116 undergraduates in which the finish point of the maze could be presented either as a specific location or in more general terms. The latter prevented the use of conventional meansâ€“ends analysis. Results indicate that on the particular maze configuration used, the nonspecific goal resulted in fewer errors and more rapid learning of the structure of the problem. Under conditions that facilitated the use of meansâ€“ends analyses, knowledge of the goal location rendered the problem insoluble. General results were replicated with the use of numerical problems. Implications for the generality of meansâ€“ends analysis as a problem-solving mechanism are discussed. (11 ref) (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
author = {Sweller, John and Levine, Marvin},
doi = {10.1037/0278-7393.8.5.463},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sweller, Levine - 1982 - Effects of goal specificity on means-ends analysis and learning.pdf:pdf},
isbn = {1939-1285(Electronic);0278-7393(Print)},
issn = {0278-7393},
journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
number = {5},
pages = {463--474},
title = {{Effects of goal specificity on means-ends analysis and learning.}},
volume = {8},
year = {1982}
}
@article{Kennedy2011,
author = {Kennedy, Bill},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kennedy - 2011 - Notes on teaching ACT-R modeling.pdf:pdf},
number = {September},
title = {{Notes on teaching ACT-R modeling}},
year = {2011}
}
@article{W.Burger1986,
author = {{W. Burger}, J. Michael Shaughnessy},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/W. Burger - 1986 - Characterizing the van Hiele Levels of Development in Geometry.pdf:pdf},
isbn = {0898597595},
journal = {Journal for Research in Matematics Education},
number = {1},
pages = {31--48},
title = {{Characterizing the van Hiele Levels of Development in Geometry}},
volume = {17},
year = {1986}
}
@misc{Sweller1983a,
abstract = {Previous studies have found that the strategies used by expert and novice problem solvers differ. Novices tend to use means-ends analysis, which involves working backward from the goal, whereas experts prefer to work forward from the givens of a problem. Experiment 1 was designed to study the course of development of expertise using a subset of kinematics problems. After solving many problems, subjects demonstrated the switch from a means-ends to a forward-chaining strategy. This was associated with the conventional concomitants of expertise such as a decrease in the number of moves required for solution. In addition, the speed at which expertise developed varied for different categories of problems. Subjects appeared to categorize problems according to the order in which equations would be required, with these categories being discovered at nonuniform rates. This was assumed to be due to the differential rate of acquisition of schemas associated with different categories of problems. Experiments 2 and 3, again using kinematics problems, tested the hypothesis that the means-ends strategies used by novices retarded the acquisition of appropriate'schemas. It was suggested that under a means-ends strategy, moves are controlled by the problem goal, which reduces the information obtained by problem solvers concerning problem structure. The use of nonspecific rather than specific goals was found to enhance the acquisition of expertise as measured by the use of a forward-oriented strategy, the number of moves required for solution, and the number of equations written without substitutions. Experiments 4 and 5, using geometry problems, duplicated the enhanced rate of strategy alteration found with reduced goal specificity. The results of Experiments 6 and 7, again using geometry problems, indicated that reduced goal specificity also enhanced the rate at which problem solvers induced appropriate problem categories. It was concluded that in circumstances in which the primary reason for presenting problems is to assist problem solvers in acquiring knowledge concerning problem structure, the use of conventional problems solved by means-ends analysis may not be maximally efficient.},
author = {Sweller, John and Mawer, Robert F. and Ward, Mark R.},
booktitle = {Journal of Experimental Psychology: General},
doi = {10.1037/0096-3445.112.4.639},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sweller, Mawer, Ward - 1983 - Development of expertise in mathematical problem solving.pdf:pdf},
issn = {0096-3445},
number = {4},
pages = {639--661},
pmid = {1120},
title = {{Development of expertise in mathematical problem solving.}},
volume = {112},
year = {1983}
}
@article{Anderson2014,
abstract = {Multi-voxel pattern recognition techniques combined with Hidden Markov models can be used to discover the mental states that people go through in performing a task. The combined method identifies both the mental states and how their durations vary with experimental conditions. We apply this method to a task where participants solve novel mathematical problems. We identify four states in the solution of these problems: Encoding, Planning, Solving, and Respond. The method allows us to interpret what participants are doing on individual problem-solving trials. The duration of the planning state varies on a trial-to-trial basis with novelty of the problem. The duration of solution stage similarly varies with the amount of computation needed to produce a solution once a plan is devised. The response stage similarly varies with the complexity of the answer produced. In addition, we identified a number of effects that ran counter to a prior model of the task. Thus, we were able to decompose the overall problem-solving time into estimates of its components and in way that serves to guide theory.},
author = {Anderson, John R. and Fincham, Jon M.},
doi = {10.1111/cogs.12068},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anderson, Fincham - 2014 - Discovering the sequential structure of thought.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Cognitive models,Hidden markov models,Multi-voxel pattern recognition,Problem solving},
number = {2},
pages = {322--352},
pmid = {23941168},
title = {{Discovering the sequential structure of thought}},
volume = {38},
year = {2014}
}
@misc{Kim,
author = {Kim, Cho Moon Lee Kim},
booktitle = {Measurement of compensatory hyperplasia of the contralateral kidney: usefulness for differential diagnosis of fetal unilateral empty renal fossa},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim - Unknown - The Step Toward.html:html},
keywords = {mcdk prenatal diagnosis},
title = {{The Step Toward}}
}
@article{Shepard2008,
abstract = {Examples from Archimedes, Galileo, Newton, Einstein, and others suggest that fundamental laws of physics were-or, at least, could have been-discovered by experiments performed not in the physical world but only in the mind. Although problematic for a strict empiricist, the evolutionary emergence in humans of deeply internalized implicit knowledge of abstract principles of transformation and symmetry may have been crucial for humankind's step to rationality-including the discovery of universal principles of mathematics, physics, ethics, and an account of free will that is compatible with determinism.},
author = {Shepard, Roger N},
doi = {10.1080/03640210701801917},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shepard - 2008 - The step to rationality the efficacy of thought experiments in science, ethics, and free will.pdf:pdf},
issn = {0364-0213},
journal = {Cognitive science},
keywords = {agency,determinism,free will,imagined transformations,mental rotation,moral laws,physical laws,rationality,symmetry,the golden rule,thought experiments},
number = {1},
pages = {3--35},
pmid = {21635330},
title = {{The step to rationality: the efficacy of thought experiments in science, ethics, and free will.}},
volume = {32},
year = {2008}
}
@article{Ioannidis2005,
author = {Ioannidis, John P A},
doi = {10.1371/journal.pmed.0020124},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:pdf},
number = {8},
title = {{Why Most Published Research Findings Are False}},
volume = {2},
year = {2005}
}
@article{Moses2006,
abstract = {Generalizations about neural function are often drawn from non-human animal models to human cognition, however, the assumption of cross-species conservation may sometimes be invalid. Humans may use different strategies mediated by alternative structures, or similar structures may operate differently within the context of the human brain. The transitive inference problem, considered a hallmark of logical reasoning, can be solved by non-human species via associative learning rather than logic. We tested whether humans use similar strategies to other species for transitive inference. Results are crucial for evaluating the validity of widely accepted assumptions of similar neural substrates underlying performance in humans and other animals. Here we show that successful transitive inference in humans is unrelated to use of associative learning strategies and is associated with ability to report the hierarchical relationship among stimuli. Our work stipulates that cross-species generalizations must be interpreted cautiously, since performance on the same task may be mediated by different strategies and/or neural systems. ?? 2006 Elsevier Ltd. All rights reserved.},
author = {Moses, Sandra N. and Villate, Christina and Ryan, Jennifer D.},
doi = {10.1016/j.neuropsychologia.2006.01.004},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moses, Villate, Ryan - 2006 - An investigation of learning strategy supporting transitive inference performance in humans compared to ot.pdf:pdf},
issn = {00283932},
journal = {Neuropsychologia},
keywords = {Associative learning,Awareness,Cross-species generalizations,Hierarchical relationship,Logic},
number = {8},
pages = {1370--1387},
pmid = {16503340},
title = {{An investigation of learning strategy supporting transitive inference performance in humans compared to other species}},
volume = {44},
year = {2006}
}
@article{Kumaran2012,
abstract = {In this article, we present a perspective on the role of the hippocampal system in generalization, instantiated in a computational model called REMERGE (recurrency and episodic memory results in generalization). We expose a fundamental, but neglected, tension between prevailing computational theories that emphasize the function of the hippocampus in pattern separation (Marr, 1971; McClelland, McNaughton, {\&} O'Reilly, 1995), and empirical support for its role in generalization and flexible relational memory (Cohen {\&} Eichenbaum, 1993; Eichenbaum, 1999). Our account provides a means by which to resolve this conflict, by demonstrating that the basic representational scheme envisioned by complementary learning systems theory (McClelland et al., 1995), which relies upon orthogonalized codes in the hippocampus, is compatible with efficient generalization-as long as there is recurrence rather than unidirectional flow within the hippocampal circuit or, more widely, between the hippocampus and neocortex. We propose that recurrent similarity computation, a process that facilitates the discovery of higher-order relationships between a set of related experiences, expands the scope of classical exemplar-based models of memory (e.g., Nosofsky, 1984) and allows the hippocampus to support generalization through interactions that unfold within a dynamically created memory space.},
author = {Kumaran, Dharshan and McClelland, James L.},
doi = {10.1037/a0028681},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumaran, McClelland - 2012 - Generalization through the recurrent interaction of episodic memories A model of the hippocampal system.pdf:pdf},
isbn = {1939-1471 (Electronic)$\backslash$r0033-295X (Linking)},
issn = {0033-295X},
journal = {Psychological Review},
keywords = {and the general structure,both specific events from,car,complementary learning systems,e,g,generalization,hippocampus,of our experiences,pattern separation,recurrence,remember,that dogs,the,the past,understanding the world relies,upon the ability to,where we just parked},
number = {3},
pages = {573--616},
pmid = {22775499},
title = {{Generalization through the recurrent interaction of episodic memories: A model of the hippocampal system.}},
volume = {119},
year = {2012}
}
@article{McClelland2013,
abstract = {The complementary learning systems theory of the roles of hippocampus and neocortex (McClelland, McNaughton, {\&} O'Reilly, 1995) holds that the rapid integration of arbitrary new information into neocortical structures is avoided to prevent catastrophic interference with structured knowledge representations stored in synaptic connections among neocortical neurons. Recent studies (Tse et al., 2007, 2011) showed that neocortical circuits can rapidly acquire new associations that are consistent with prior knowledge. The findings challenge the complementary learning systems theory as previously presented. However, new simulations extending those reported in McClelland et al. (1995) show that new information that is consistent with knowledge previously acquired by a putatively cortexlike artificial neural network can be learned rapidly and without interfering with existing knowledge; it is when inconsistent new knowledge is acquired quickly that catastrophic interference ensues. Several important features of the findings of Tse et al. (2007, 2011) are captured in these simulations, indicating that the neural network model used in McClelland et al. has characteristics in common with neocortical learning mechanisms. An additional simulation generalizes beyond the network model previously used, showing how the rate of change of cortical connections can depend on prior knowledge in an arguably more biologically plausible network architecture. In sum, the findings of Tse et al. are fully consistent with the idea that hippocampus and neocortex are complementary learning systems. Taken together, these findings and the simulations reported here advance our knowledge by bringing out the role of consistency of new experience with existing knowledge and demonstrating that the rate of change of connections in real and artificial neural networks can be strongly prior-knowledge dependent.},
author = {McClelland, James L},
doi = {10.1037/a0033812},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McClelland - 2013 - Incorporating rapid neocortical learning of new schema-consistent information into complementary learning systems th.pdf:pdf},
isbn = {6507251232},
issn = {1939-2222},
journal = {Journal of experimental psychology. General},
keywords = {1932,1979,2007,2011,bartlett,bransford,consolidation,hippocampus,in two recent articles,it has long been,known that it is,learning,memory,new,relatively easy to learn,schemas,things that are consistent,tse et al,with prior knowledge},
pages = {1190--210},
pmid = {23978185},
title = {{Incorporating rapid neocortical learning of new schema-consistent information into complementary learning systems theory.}},
volume = {142},
year = {2013}
}
@article{McClelland2010a,
abstract = {The study of human intelligence was once dominated by symbolic approaches, but over the last 30 years an alternative approach has arisen. Symbols and processes that operate on them are often seen today as approximate characterizations of the emergent consequences of sub- or nonsymbolic processes, and a wide range of constructs in cognitive science can be understood as emergents. These include representational constructs (units, structures, rules), architectural constructs (central execu- tive, declarative memory), and developmental processes and outcomes (stages, sensitive periods, neurocognitive modules, developmental disorders). The greatest achievements of human cognition may be largely emergent phenomena. It remains a challenge for the future to learn more about how these greatest achievements arise and to emulate them in artificial systems.},
author = {McClelland, James L.},
doi = {10.1111/j.1756-8765.2010.01116.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcclelland - 2010 - Emergence in Cognitive Science.pdf:pdf},
isbn = {1756-8765},
issn = {17568757},
journal = {Topics in Cognitive Science},
keywords = {Development,Emergence,Explanation,History,Language,Modeling,Neural networks},
pages = {751--770},
title = {{Emergence in Cognitive Science}},
volume = {2},
year = {2010}
}
@article{Mcclelland2010,
author = {McClelland, James L and Botvinick, Matthew M and Noelle, David C and Plaut, David C and Rogers, Timothy T and Seidenberg, Mark S and Smith, Linda B},
doi = {10.1016/j.tics.2010.06.002},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcclelland et al. - 2010 - Letting structure emerge connectionist and dynamical systems approaches to cognition.pdf:pdf},
issn = {1364-6613},
journal = {Trends in Cognitive Sciences},
number = {8},
pages = {348--356},
publisher = {Elsevier Ltd},
title = {{Letting structure emerge: connectionist and dynamical systems approaches to cognition}},
volume = {14},
year = {2010}
}
@article{Griffiths2010,
abstract = {Cognitive science aims to reverse-engineer the mind, and many of the engineering challenges the mind faces involve inductive inference. The probabilistic approach to modeling cognition begins with the goal of understanding these inductive problems in computational terms: what makes them difficult, and how they can be solved in principle. Mental processes are then modeled using algorithms for approximately implementing these ideal solutions, and neural processes are viewed as mechanisms for implementing these algorithms in biological hardware. This top-down approach is analogous to historical progressions of theory building in other natural sciences, moving from macro-level functional explanations of observable phenomena to micro-level mechanistic accounts. Typical connectionist models, by contrast, follow a bottom-up approach, starting from an abstract characterization of neural mechanisms and exploring what macro-level functional phenomena might emerge from those mechanisms. We suggest that the top-down approach is likely to yield more rapid progress towards understanding human inductive inference.},
author = {Griffiths, T L and Chater, N and Kemp, C and Perfors, A and Tenenbaum, J and Griffiths, T},
issn = {13646613},
journal = {Trends in},
pages = {357--364},
title = {{Probabilistic models of cognition: Exploring the laws of thought}},
volume = {14},
year = {2010}
}
@article{Koedinger1990,
abstract = {We present a new model of skilled performance in geometry proof problem solving called the Diagram Configuration model (DC). While previous models plan proofs in a step-by-step fashion, we observed that experts plan at a more abstract level: They focus on the key steps and skip the less important ones. DC models this abstract planning behavior by parsing geometry problem diagrams into perceptual chunks, called diagram configurations, which cue relevant schematic knowledge. We provide verbal protocol evidence that DC's schemas correspond with the step-skipping inferences experts make in their initial planning. We compare DC with other models of geometry expertise and then, in the final section, we discuss more general implications of our research. DC's reasoning has important similarities with Larkin's (1988) display-based reasoning approach and Johnson-Laird's (1983) mental model approach. DC's perceptually based schemas are a step towards a unified explanation of (1) experts' superior problem-solving effectiveness, (2) experts' superior problem-state memory, and (3) experts' ability, in certain domains, to solve relatively simple problems by pure forward inferencing. We also argue that the particular and efficient knowledge organization of DC challenges current theories of skill acquisition as it presents an end-state of learning that is difficult to explain within such theories. Finally, we discuss the implications of DC for geometry instruction. {\textcopyright} 1990.},
author = {Koedinger, Kenneth R. and Anderson, John R.},
doi = {10.1207/s15516709cog1404_2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koedinger, Anderson - 1990 - Abstract Planning and Perceptual Chunks Elements of Expertise in Geometry.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
pages = {511--550},
title = {{Abstract Planning and Perceptual Chunks: Elements of Expertise in Geometry}},
volume = {14},
year = {1990}
}
@inproceedings{SzegedyAdv,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
booktitle = {ICLR},
eprint = {1312.6199},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:pdf},
keywords = {Adversarial,CNN},
mendeley-tags = {Adversarial,CNN},
month = {dec},
title = {{Intriguing properties of neural networks}},
year = {2014}
}
@article{Nguyen2014,
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
eprint = {1412.1897},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Yosinski, Clune - 2014 - Deep Neural Networks are Easily Fooled High Confidence Predictions for Unrecognizable Images.pdf:pdf},
keywords = {CNN},
mendeley-tags = {CNN},
month = {dec},
title = {{Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}},
year = {2014}
}
@article{Sermanet2013,
abstract = {We present an integrated framework for using ConvolutionalNetworks for classi- fication, localization and detection.We showhowamultiscale and slidingwindow approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object bound- aries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simul- taneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNetLarge ScaleVisual RecognitionChallenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competitionwork, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
journal = {arXiv preprint arXiv:1312.6229},
keywords = {CNN,Computer Vision},
mendeley-tags = {CNN,Computer Vision},
pages = {1--15},
title = {{OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks}},
year = {2013}
}
@article{Robertson1981,
abstract = {This article will review the methods currently available to the clinician and research worker for measuring the concentration of ionized calcium in various body fluids including whole blood, serum, plasma, urine, cerebrospinal fluid, milk, and synovial fluid. The methods to be reviewed are based on procedures involving bioluminescence, colorimetry and ion-selective electrodes. Emphasis will be given to the precision and, wherever possible, accuracy of each technique. Possible sources of error and interfering agents will be identified. Attention will be given to the recommended conditions for measuring ionized calcium in each body fluid. An assessment will be made of the theoretical and practical importance of measuring ionized calcium rather than total calcium and of its value in clinical medicine.},
author = {Robertson, W G and Marshall, R W},
doi = {10.3109/10408368109105869},
issn = {1040-8363},
journal = {Critical reviews in clinical laboratory sciences},
keywords = {Auto-immune,PAD4,Physiology},
mendeley-tags = {Auto-immune,PAD4,Physiology},
pages = {85--125},
pmid = {7026165},
title = {{Ionized calcium in body fluids.}},
volume = {15},
year = {1981}
}
@article{Michell1969,
abstract = {A method has been developed for measuring the rate of phagocytosis rather than the quantity of particles ingested per cell when the process is virtually complete. The method, which is simpler and more rapid than those described previously, utilizes cellular monolayers, radioactive particles, and short incubation times. Under the conditions described, the rate of uptake of particles by either guinea-pig peritoneal or human blood leukocytes was proportional to both cell concentration and the time of incubation, and was independent of changes in the concentration of particles during the measurement. The particles were retained by the cells for at least 90 min. The most suitable particles so far used have been (32)P-labeled Salmonella typhimurium, and acetyl-(14)C- or methyl-(14)C-labeled starch particles. The oxidation of (14)C-labeled glucose has been studied under the same conditions that were used for the assays of phagocytosis: the greatest increase in formation of (14)CO(2) from glucose-1-(14)C occurred a few minutes after the most rapid period of phagocytosis.},
author = {Michell, R H and Pancake, S J and Noseworthy, J and Karnovsky, M L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Michell et al. - 1969 - Measurement of rates of phagocytosis the use of cellular monolayers.pdf:pdf},
issn = {0021-9525},
journal = {The Journal of cell biology},
keywords = {Animals,Antimetabolites,Antimetabolites: pharmacology,Auto-immune,Carbon Isotopes,Guinea Pigs,Humans,Leukocytes,Leukocytes: physiology,Methods,PAD4,Phagocytosis,Phagocytosis: drug effects,Phosphorus Isotopes,Physiology,Salmonella typhimurium,Starch},
mendeley-tags = {Auto-immune,PAD4,Physiology},
month = {jan},
number = {1},
pages = {216--24},
pmid = {4881437},
title = {{Measurement of rates of phagocytosis: the use of cellular monolayers.}},
volume = {40},
year = {1969}
}
@article{Darrah2013,
author = {Darrah, Erika and Giles, JT and Ols, ML},
doi = {10.1126/scitranslmed.3005370.Erosive},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Darrah, Giles, Ols - 2013 - Erosive rheumatoid arthritis is associated with antibodies that activate PAD4 by increasing calcium sensitiv.pdf:pdf},
journal = {Science translational {\ldots}},
keywords = {Auto-immune,PAD4,Physiology},
mendeley-tags = {Auto-immune,PAD4,Physiology},
number = {186},
pages = {1--19},
title = {{Erosive rheumatoid arthritis is associated with antibodies that activate PAD4 by increasing calcium sensitivity}},
volume = {5},
year = {2013}
}
