\chapter{Meta-mapping} \label{chapter:zero_shot_via_homm}
\blfootnote{Some of the material in this chapter originally appeared as a workshop paper in the Learning Transferable Skills Workshop at NeurIPS 2019.}
\epigraph{``In the human, internal representations become objects of cognitive manipulation.''}{Annette Karmiloff-Smith,\\\textit{Beyond Modularity}}


As noted in the introduction, humans have the ability to transform our behavior on a task, in accordance with a change in goals. For example, if we are told to try to lose at poker, we can perform quite well on our first try, even if we have always tried to win previously. If we are shown an object, and told to find the same object in a new color or texture, we can do so. How can we adapt our behavior so drastically, without any data on the new task, even when our new goal contradicts all our prior experience? We can do so by exploiting the relationship between the adapted version of the task and the original. In this chapter, I propose a computational model of this adaptation, and demonstrate its success across a variety of domains. Our model is both useful for understanding the flexibility of human cognition, and for designing artificial intelligence systems with more human-like flexibility.   

The model incorporates several key insights into human cognition. First, when performing a task (such as playing poker), humans are aware of what we are doing and why. We propose that this awareness is mediated by an internal representation of the task. Our model therefore performs tasks from a task representation. We take inspiration from various approaches from the machine learning and cognitive science literature, and construct task representations from examples via meta-learning \citep[e.g.][]{Vinyals2016, Santoro2016, Finn2017a, Finn2018, Stadie2018, Botvinick2019}, or from a natural language instruction \citep{Larochelle2008, Hermann2017, Hill2019a}. The model then uses this task representation to respond in a task-appropriate way to its inputs (and for other purposes, such as identifying attributes of the tasks). 

To implement the task computations, we allow a great deal of input processing (perception) to be shared across different tasks. If a human is playing cards, much of the perception of the cards will be identical whether the game is poker or blackjack or bridge, and the task-specific computations will be performed over abstract features such as suit and rank relationships. We thus allow the system to learn a general basis of perceptual features over all tasks with a domain. The system then uses these features in a task-specific way in order to perform task-appropriate behavior. Specifically, the model uses its representation of the current task to parameterize this computation over the perceptual features, and then decodes the result through an output system which is also shared across the tasks. 

We also allow the model to transform its representations of tasks, to accomodate task alterations. We refer to these transformations of tasks as \textbf{meta-mappings}. Meta-mappings allow the model to adapt to a new task \emph{zero-shot} (i.e. without requiring any data from that new task), based on the relationship between the new task and prior tasks. For example, meta-mappings could be used to switch to losing at poker, based on knowledge of poker, and what ``losing'' means across other games. Meta-mappings can be cued either by examples of the meta-mapping applied to other tasks, or by an instruction, just as basic tasks can be inferred from examples or instructions. 

Our model infers and implements these task transformations using the same architetural components that are used to perform the basic tasks. Using the same architectural components for basic tasks and meta-mappings is parsimonious,as it does not require adding new networks for each new type of transformation. Furthermore, it reflects our theory that the brain reuses the same sytems for computations at different levels of abstraction. This may be key to the human ability to repeatedly build abstractions on top of prior abstractions \citep{Wilensky1991, Hazzan1999, Lampinen2017b}, allowing, in principle, for recursion upon our own cognitive processes. Finally, it relates to the computational notion of ``homoiconicity.'' A homoiconic programming language is one in which programs can be manipulated within the language as data. Our implementations are therefore homoiconic, in the sense that tasks can be transformed just like data can. We refer to these architectures as \textbf{homoiconic meta-mapping} (HoMM) architectures. These architectures allow for computational adaptibility that is closer, at least in some ways, to that exhibited by humans.  

The main contributions of this chapter are to propose meta-mapping as a computational framework for understanding task adaptation, and to propose a parsimonious implementation of this framework in the form of homoiconic meta-mapping. In this chapter we will demonstrate the success of our approach in a simple proof of concept domain, and explore some features of its performance. In subsequent chapters, we will compare to human adaptation and show the success of our approach across a variety of tasks, ranging from visual classification to reinforcement learning. See below for a discussion of related work. To our knowledge, this is the first work that proposes transforming a task representation in order to adapt to task alterations zero-shot. 


\section{Task transformation via meta-mappings} \label{sec:HoMM:metamappings}

\textbf{Basic tasks are input-output mappings:} We take as a starting point the construal of basic tasks as mappings from inputs to outputs. For example, poker can be seen as a mapping from hands to bets, chess as a mapping of board positions to moves, and object recognition as a mapping from images to labels. This perspective is common across machine learning approaches, which generally try to infer a task mapping from many examples, or meta-learn how to infer it from fewer examples. In our work, we infer these mappings either from examples, from natural language instructions, or by transforming a prior task mapping. 

\textbf{Tasks can be transformed via meta-mappings:} We propose meta-mappings as a computational approach to the problem of transforming a basic task mapping. A meta-mapping is a higher-order task, which takes a task representation as input, and outputs a representation of the transformed task. For example, we might have a ``lose'' meta-mapping. If given poker as an input, the lose meta-mapping would output a losing variation of poker. If given blackjack, it would output a losing variation of blackjack. If we have such a meta-mapping, we can use it to transform a task in order to perform the transformed version of the task. This allows a model to adapt to a transformed task without having any data on it, just as humans are able to easily switch to trying to lose at a game they have only tried to win in the past. 

How can a meta-mapping be constructed? There is an analogy between meta-mappings and basic task mappings -- they are both simply functions from inputs to outputs. Thus to construct a meta-mapping that we use analogous approaches to those we use for constructing basic task representations. In particular, we infer a meta-mapping from examples (e.g. winning and losing at a set of example games), or natural language (e.g. ``try to switch to losing''). We can then apply this meta-mapping to other basic tasks, in order to infer losing variations of those tasks. Importantly, the system is able to generalize to new meta-mappings, just as it can generalize to new basic tasks. 



\section{Homoiconic meta-mapping architectures} \label{sec:HoMM:architecture}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[auto]
%% a) constructing
\draw[boundingbox, draw=gray, fill=white] (-8, -2.5) rectangle (-0.25, 2.5);

%% from language
\node[gray] at (-6.5, 2.1) {Instructions};
\node at (-6.5, 1.25) (language) {``Play poker.''};

\node[gray, text width=2cm, align=center] at (-3.5, 2.1) {Language network};
\node[block] at (-3.5, 1.25) (languagenet) {\(\mathcal{L}\)}; 
\path[arrow] (language.east) -- ([xshift=-3]languagenet.west);

\node[bpurp, text width=2cm] at (-0.5, 1.25) (languagetaskrep) {\(z_{task}\)}; 
\path[arrow] ([xshift=3]languagenet.east) -- (languagetaskrep.west);


%% from examples

\node[gray, text width=2.5cm, align=center] at (-6.5, -0.2) {Task examples (encoded)};
\node at (-6.5, -1.25) (examples) {
\(\left\{
\begin{matrix}
({\color{bgreen}z_{hand_{1}}}, {\color{bgreen}z_{bet_{1}}})\\
$\vdots$
\end{matrix}\right\}\)};

\node[gray, text width=2cm, align=center] at (-3.5, -0.4) {Example network};
\node[block] at (-3.5, -1.25) (examplenet) {\(\mathcal{E}\)}; 
\path[arrow] (examples.east) -- ([xshift=-3]examplenet.west);

\node[bpurp, text width=2cm] at (-0.5, -1.25) (examplestaskrep) {\(z_{task}\)}; 
\path[arrow] ([xshift=3]examplenet.east) -- (examplestaskrep.west);

%% b) performing 
\draw[boundingbox, draw=gray, fill=white] (0.25, -2.5) rectangle (8, 2.5);
\node[bpurp] at (1, 1.25) (taskrep) {\(z_{task}\)};

\node[gray, text width=2cm, align=center] at (3, 2.1) {Hyper network};
\node[block] at (3, 1.25) (hypernet) {\(\mathcal{H}\)}; 
\path[arrow] (taskrep.east) -- ([xshift=-3]hypernet.west);

\node[text width=0.5cm] at (0.7, -1.25) (inputs) {\includegraphics[width=0.5cm]{2-HoMM/figures/2_of_spades.png}\\\includegraphics[width=0.5cm]{2-HoMM/figures/4_of_hearts.png}};

\node[gray, text width=2cm, align=center] at (1.9, -0.4) {Perception network};
\node[block] at (1.9, -1.25) (perceptionnet) {\(\mathcal{P}\)}; 
\path[arrow] (inputs.east) -- ([xshift=-3]perceptionnet.west);

\node[bgreen] at (3.2, -1.25) (handrep) {\(z_{hand}\)};
\path[arrow] ([xshift=3]perceptionnet.east) -- (handrep.west);

\node[bblue, block, dashed] at (4.5, -1.25) (tasknet) {\(\mathcal{T}\)}; 
\node[bblue, text width=2cm, align=center] at (4.5, -2) {Task network};
\path[arrow] (handrep.east) -- ([xshift=-3]tasknet.west);
\path[arrow, out=0, in=90] ([xshift=3]hypernet.east) to ([yshift=3]tasknet.north);


\node[bgreen] at (5.65, -1.25) (betrep) {\(z_{bet}\)};
\path[arrow] ([xshift=3]tasknet.east) -- (betrep.west);

\node[gray, text width=2cm, align=center] at (6.8, -0.4) {Action network};
\node[block] at (6.8, -1.25) (actionnet) {\(\mathcal{A}\)}; 
\path[arrow] (betrep.east) -- ([xshift=-3]actionnet.west);

\node at (7.7, -1.25) (output) {\bf \$};
\path[arrow] ([xshift=3]actionnet.east) -- (output.west);

\end{tikzpicture}
\begin{subfigure}{0.5\textwidth}
\caption{Constructing a task representation.}\label{fig:HoMM_architecture:constructing_basic}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\caption{Performing a task from its representation.}\label{fig:HoMM_architecture:performing_basic}
\end{subfigure}
\begin{tikzpicture}[auto]
%% a) constructing
\draw[boundingbox, draw=gray, fill=white] (-8, -2.5) rectangle (-0.25, 2.5);

%% from language
\node[gray] at (-6, 2.1) {Instructions};
\node at (-6, 1.25) (language) {``Try to lose.''};

\node[gray, text width=2cm, align=center] at (-3, 2.1) {Language network};
\node[block] at (-3, 1.25) (languagenet) {\(\mathcal{L}\)}; 
\path[arrow] (language.east) -- ([xshift=-3]languagenet.west);

\node[borange, text width=2cm] at (-0.5, 1.25) (languagetaskrep) {\(z_{meta}\)}; 
\path[arrow] ([xshift=3]languagenet.east) -- (languagetaskrep.west);


%% from examples

\node[gray, text width=3.5cm, align=center] at (-6, -0.2) {Mapping examples (input/output tasks)};
\node at (-6, -1.25) (examples) {
\(\left\{
\begin{matrix}
({\color{bpurp}z_{chess}}, {\color{bpurp}z_{lose chess}})\\
$\vdots$
\end{matrix}\right\}\)};

\node[gray, text width=2cm, align=center] at (-3, -0.4) {Example network};
\node[block] at (-3, -1.25) (examplenet) {\(\mathcal{E}\)}; 
\path[arrow] (examples.east) -- ([xshift=-3]examplenet.west);

\node[borange, text width=2cm] at (-0.5, -1.25) (examplestaskrep) {\(z_{meta}\)}; 
\path[arrow] ([xshift=3]examplenet.east) -- (examplestaskrep.west);

%% b) performing 
\draw[boundingbox, draw=gray, fill=white] (0.25, -2.5) rectangle (8, 2.5);
\node[borange] at (1, 1.25) (taskrep) {\(z_{meta}\)};

\node[gray, text width=2cm, align=center] at (3, 2.1) {Hyper network};
\node[block] at (3, 1.25) (hypernet) {\(\mathcal{H}\)}; 
\path[arrow] (taskrep.east) -- ([xshift=-3]hypernet.west);

\node[bpurp] at (2.66, -1.25) (handrep) {\(z_{poker}\)};

\node[bblue, block, dashed] at (4.5, -1.25) (tasknet) {\(\mathcal{T}\)}; 
\node[bblue, text width=2cm, align=center] at (4.5, -2) {Task network};
\path[arrow] (handrep.east) -- ([xshift=-3]tasknet.west);
\path[arrow, out=0, in=90] ([xshift=3]hypernet.east) to ([yshift=3]tasknet.north);

\node[bpurp] at (6.5, -1.25) (output) {\(z_{lose poker}\)};
\path[arrow] ([xshift=3]tasknet.east) -- (output.west);


\end{tikzpicture}
\begin{subfigure}{0.5\textwidth}
\caption{Constructing a meta-mapping representation.}\label{fig:HoMM_architecture:constructing_meta}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\caption{Transforming a task via a meta-mapping.}\label{fig:HoMM_architecture:performing_meta}
\end{subfigure}
\caption[Performing and transforming tasks with the HoMM architecture.]{Performing and transforming tasks with the HoMM architecture. (\subref{fig:HoMM_architecture:constructing_basic},\subref{fig:HoMM_architecture:constructing_meta}) The HoMM architecture performs basic tasks and meta-mappings from a task representation, which can be constructed from appropriate language inputs or examples. (\subref{fig:HoMM_architecture:performing_basic},\subref{fig:HoMM_architecture:performing_meta}) The task representation is used to parameterize a task network which executes the appropriate task mapping. To do this parsimoniously, the HoMM architecture exploits a deep analogy between basic tasks and meta-mappings --- both can be seen as mappings of inputs to outputs, although they have different types of inputs and outputs. To overcome this challenge, the architecture uses type-specific models to embed all basic inputs, as well as tasks and meta-mappings, in a shared representational space. Thus all tasks and meta-mappings can be seen as transformations applied to entities in this space, which can be executed by shared systems. The parallels between the basic tasks and meta-mappings are reflected in the parallels between the top and bottom rows of the figure.} \label{fig:HoMM_architecture}
\end{figure}

We propose homiconic meta-mapping (HoMM) architectures as an implementation of a system that can perform tasks, and adapt to task alterations via meta-mappings. In this section, we describe the architecture and training process. See Appendix \ref{appendix:model_hyperparameters} for full architecture specifications, training details, and hyperparameters. 

\textbf{Constructing a task representation (Fig. \ref{fig:HoMM_architecture:constructing_basic}):} When humans perform a task, we need to know what the task is. In our model, we specify the task using a task representation. Just like humans, our model supports several different ways of cueing a task, such as instructions, examples of appropriate behavior, or memory of performing the task in the past. Specifically, at different times we either construct a task from natural language, from a dataset of (input, target) tuples for the desired task, or we recall a cached task representation. To construct a task representation from language, we process the language through a deep recurrent network (LSTM). This is similar to techniques used in other work \citep[e.g.]{Hermann2017,Oh2017a,Hill2019a}. To construct a task representation from examples, we process the examples individually to construct appropriate representations for each example, and then aggregate across them by taking an element-wise max. (This provides a nonlinear and dataset-order-invariant way of combining examples --- we found other methods such as averaging performed similarly.) This aggregated representation then receives further processing to produce the task representation. This approach shares some common elements with other approaches used for meta-learning \citep{Garnelo2018}. 

\textbf{Performing a task from its representation (Fig. \ref{fig:HoMM_architecture:performing_basic}):} Once we have a task representation, we need to use it to perform the task. We allow a large part of the input processing (perception) and output processing (action) to be shared across the tasks, so that the task-specific computations can be relatively simple and abstract. This idea is related to the long-standing notion that deep networks (both artificial and biological) will construct more disentangled representations of the task relevant features in deeper layers \citep{Dicarlo2007, Erhan2010}, and is used in a number of meta-learning approaches \citep[e.g.]{Vinyals2016}. We use a HyperNetwork \citep{Ha2016} which takes as input the task representation, and adapts the parameters of a task network. Specifically, the HyperNetwork adapts the values of learned ``default'' connection weights, to make the network task-sensitive. The adapted network then transforms the perceptual features into task-appropriate output features, which can then be decoded to outputs via the shared output processing network. The whole model (including the construction of the task representations) can be trained end-to-end, just as a standard meta-learning system would be. (There are other possible architectures; we show that our approach outperforms the simple alternative of concatenating a task representation to an input embedding before passing it through a fixed network in Supplemental Figures \ref{supp_fig:HoMM_arch_cond_vs_hyper} and \label{supp_fig:extending:RL:arch_cond_vs_hyper}.) 

\textbf{Transforming task representations via meta-mappings (Fig. \ref{fig:HoMM_architecture:constructing_meta}-\subref{fig:HoMM_architecture:performing_meta}):} Above, we defined a meta-mapping to be a higher-order task, which takes as input a task representation, and outputs a transformed task representation. Because our model constructs task representations to perform the tasks, all that we need to implement is a way of transforming these representations to perform a meta-mapping. To do so, we exploit the functional analogy between basic-tasks and meta-mappings, noted above. We can infer a representation for a mapping from examples of the meta-mapping, or from a language description, just like we infer a basic task representation from examples or language. We can then use this meta-mapping representation to parameterize a meta-mapping-specific network that can be used to transform other task representations. This is analogous to how we used a basic-task representation to parameterize a network specific to that task, which could then be used to perform the task. (In Section \ref{sec:HoMM:vector_analogies_inadequate} below, we prove that a simpler approach of using vector analogies for meta-mapping is inadequate.) 

We exploit this analogy by using exactly the same networks for inferring a meta-mapping representation from examples or language as we do for inferring a task representation. We use exactly the same hyper network to parameterize a meta-mapping-specific network as we use to parameterize the task-specific network. That is, we use precisely the same architecture (and default weights that are modulated by the same hyper network) for both basic task computations and meta-mappings. To do so, we assume that the shared perceptual processing embeds individual data points into a representation space that is shared with that used for task representations and meta-mapping representations. This means that all task- or meta-mapping-specific computations can be seen as operations on objects in the same space, and can be inferred identically regardless of the objects being transformed. This is parsimonious, homoiconic, and reflects aspects of the Global Workspace Theory of consciousness \citep{Baars2005}, see discussion.   

\textbf{Training \& evaluating the model:} To train the system to perform the basic tasks, we can compute a task-appropriate loss at the output of the action network, and then minimize this loss with respect to the parameters in all networks. This includes the networks used to construct the task representation, and even the representations of the examples or language that they receive as input. That is, we train the sytem end-to-end to perform the basic tasks. When constructing a task representation from examples, we do not allow the example network to see every item, in order to force the system to generalize, in a standard meta-learning fashion. For example, if the basic task is poker, the system will have to construct a task representation from some hands that will be useful for playing other hands. This ensures that the task representations capture the structure of the task, rather than just memorizing the provided examples. 

To train the system to perform meta-mappings, we try to match the output task representations to those constructed when actually performing those tasks. Specifically, we apply an \(\ell_2\) loss to the difference between the output embedding and the embedding constructed when performing the target task. For example, if the system has been trained to play winning and losing variations of blackjack, we would take the task representation for winning blackjack as input, and try to match the output to the task representation for losing blackjack. Again, when we construct a meta-mapping representation from examples of the mapping, we force it to generalize to other examples. Regardless of how the meta-mapping representation is constructed, we can then test this generalization by passing in the representation for a task like poker, that has never been used for any training on this meta-mapping (either as an example or for generalization). We take the output task representation produced by the meta-mapping, and actually perform the task of losing poker with it. This is how we perform all evaluation of the meta-mapping approach in this paper.

In meta-mapping, generalization is possible at different levels of abstraction. The paragraph above refers to basic generalization --- applying a meta-mapping seen during training to a basic task that meta-mapping has not been applied to during training, in order to perform a held-out basic task zero-shot. However, if the system has experienced sufficiently many meta-mappings during training, we can also test its ability to generalize to held-out meta-mappings. For example, if the system has been trained to switch various pairs of colors in a classification task (red for blue, green for yellow, etc.), it should be able to generalize to switching held-out pairs (red for yellow, green for blue, etc.) from an appropriate cue (examples or instructions). This is an important part of human adaptibility --- we are not only able to adapt tasks via meta-mappings that we understand well, but we can infer and use new meta-mappings based on specific instructions or examples. We will demonstrate this ability in the subset of our experimental domains where we can instantiate sufficiently many meta-mappings. 

\textbf{Comparing to language-based generalization:} Natural language instructions are an important part of how humans are able to generalize to a new task, and prior work on zero-shot performance has often assumed a description of the task as input \citep[e.g.]{Larochelle2008}. For example, a system that has learned to behave in accordance with instructions like ``win at poker,'' ``win at blackjack,'' and ``lose at blackjack,'' should be able to generalize to ``lose at blackjack,'' given sufficiently many training tasks. This, too, does not require data on the novel task. However, transforming the task representation via a meta-mapping may be a more useful inductive bias that allows the system to transform the prior task representation in a targeted way. We thus compare our meta-mapping approach to an approach that simply constructs task representations from language. We show in subsequent chapters that meta-mapping results in better performance on the new tasks, especially when the held-out tasks are very different from trained ones (e.g. directly contradicting). That is, in our experiments, meta-mapping generally has better sample-complexity in terms of the number of prior tasks it must have experienced to perform well. This is crucial, because humans have not seen 95\% of the possible task space when they need to generalize to a new setting.

\section{Experiments}

Meta-mapping is an extremely general framework. Because the assumptions are simply that the basic tasks are mappings from inputs to outputs, and that meta-mappings transform basic tasks, the approach can be applied to most paradigms of machine-learning with only minor modifications. We demonstrate the success of meta-mapping in four settings over the next few chapters, ranging from regression to classification to reinforcement learning. We summarize the contributions of the different experiments in Table \ref{table:HoMM_experiment_summary}. In this chapter, we explore the performance of meta-mapping in detail in a proof-of-concept polynomial domain. We also describe some interesting behavior of the model that may intrigue researchers in cognitive control (but is not a focus of the remainder of the dissertation). 

\begin{table}
\centering
\begin{tabular}{|cp{2cm}|p{2cm}ccp{1.8cm}p{1.5cm}|}
\hline
\textbf{Chapter} &\textbf{Experiment} & \textbf{Motivation} & \begin{minipage}[t]{2cm}\centering\textbf{Held-out MMs}\end{minipage} & \begin{minipage}[t]{1cm}\centering\textbf{Lang. Comp.}\end{minipage} & \textbf{Paradigm} & \textbf{Input}\\[1.3em]
\hline
\ref{chapter:zero_shot_via_homm} & Polynomials & Proof of concept & \checkmark & & Regression & Vector (\(\mathbb{R}^4\))\\[0.5em] 
\ref{chapter:human} & Card games & Comparison to humans & & \checkmark & Regression & Several-hot vector\\[0.5em]
\ref{sec:extending:rl} & Reinforce-ment\phantom{blah} learning & Cognitive and AI relevance & & \checkmark & RL & \(91 \times 91\) RGB image\\[0.5em]
\ref{sec:extending:concepts} & Visual\phantom{blah} concepts & Cognitive and AI relevance & \checkmark & \checkmark & Classification & \(50 \times 50\) RGB image\\[0.5em]
\hline
\end{tabular}

\caption[The contributions of our four sets of experiments.]{The contributions of our four sets of meta-mapping experiments. Our results span various computational paradigms and various degrees of input complexity, and are motivated by both cognitive and AI relevance.} \label{table:HoMM_experiment_summary}
\end{table}

\subsection{Polynomial regression}
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[auto]
%% Basic
\node[gray] at (0, 3) {Basic tasks};
\draw[boundingbox, draw=gray, fill=white] (-4, 2.7) rectangle (-0.25, -0.2);
\draw[boundingbox, draw=gray, fill=white] (-4, 2.7) rectangle (-0.25, 1.7);
\node[gray] at (-3.5, 2.45) {Task:};
\node[align=center] at (-2.2, 2) {\(f(w,x,y,z) = x^2 + 1\)};
\node[gray] at (-2.4, 1.45) {Input-output pairs:};
\node[align=center] at (-2.2, 0.5) {%
    \(\begin{aligned}
    (0, 0, 0, 0) & \mapsto 1\\[-0.2em] 
    (1.5, -1, 3.1, 0) & \mapsto 3.25\\[-0.75em] 
    & \vdots
    \end{aligned}\)};

\begin{scope}[shift={(4, 0)}]
\draw[boundingbox, draw=gray, fill=white] (-4, 2.7) rectangle (-0.25, -0.2);
\draw[boundingbox, draw=gray, fill=white] (-4, 2.7) rectangle (-0.25, 1.7);
\node[gray] at (-3.5, 2.45) {Task:};
\node[align=center] at (-2.2, 2) {\(f(w,x,y,z) = 3w + yz\)};
\node[gray] at (-2.4, 1.45) {Input-output pairs:};
\node[align=center] at (-2.2, 0.5) {%
    \(\begin{aligned}
    (0.5, 0, 1, 2) & \mapsto 3.5\\[-0.2em] 
    (1, 0.2, -1, 0.5) & \mapsto 2.5\\[-0.75em] 
    & \vdots
    \end{aligned}\)};
\end{scope}

%% meta-mapping
\node[gray] at (0, -0.5) {Meta-mappings};

\begin{scope}[shift={(0, -3.5)}]
\draw[boundingbox, draw=gray, fill=white] (-4, 2.7) rectangle (-0.25, -0.2);
\draw[boundingbox, draw=gray, fill=white] (-4, 2.7) rectangle (-0.25, 1.7);
\node[gray] at (-2.75, 2.45) {Meta-mapping:};
\node[align=center] at (-2.8, 2) {Multiply by 3.};
\node[gray] at (-2.4, 1.45) {Input-output pairs:};
\node[align=center] at (-2.2, 0.5) {%
    \(\begin{aligned}
    x^2 + 1 & \mapsto 3x^2 + 3\\[-0.2em] 
    3w + yz & \mapsto 9w + 3yz\\[-0.75em] 
    & \vdots
    \end{aligned}\)};
\end{scope}

\begin{scope}[shift={(4, -3.5)}]
\draw[boundingbox, draw=gray, fill=white] (-4, 2.7) rectangle (-0.25, -0.2);
\draw[boundingbox, draw=gray, fill=white] (-4, 2.7) rectangle (-0.25, 1.7);
\node[gray] at (-2.75, 2.45) {Meta-mapping:};
\node[align=center] at (-2.4, 2) {Permute \((w, z, x, y)\)};
\node[gray] at (-2.4, 1.45) {Input-output pairs:};
\node[align=center] at (-2.2, 0.5) {%
    \(\begin{aligned}
    x^2 + 1 & \mapsto z^2 + 1\\[-0.2em] 
    3w + yz & \mapsto 3w + xy\\[-0.75em] 
    & \vdots
    \end{aligned}\)};
\end{scope}
\end{tikzpicture}
\caption[The polynomial task domain.]{The polynomial task domain. A basic polynomial task consists of regressing a single polynomial, i.e. the inputs are points in \(\mathbb{R}^4\) and the outputs are the value of the polynomial at that point. These basic regression tasks can be transformed by various meta-mappings, such as multiplying by a constant, or permuting their variables. }\label{fig:HoMM_polynomials:tasks}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth]{2-HoMM/figures/polynomials_adaptation.png}
\caption[Meta-mapping results in the polynomials domain.]{Meta-mapping results in the polynomials domain. We plot zero-shot performance (normalized, see text) on new tasks via meta-mappings. The system not only-generalizes trained meta-mappings to examples it has never seen before (purple), but also generalizes to held-out meta-mappings from examples (orange), and does both substantially better than a baseline model which does not adapt (dotted lines). Thus our approach is able to flexibly adapt to a new polynomial without any data from that polynomial, based on that polynomial's relationship to polynomials it has experienced.}\label{fig:HoMM_polynomials:results}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.625\textwidth]{2-HoMM/figures/polynomials_adaptation_by_mapping.png}
\caption[Meta-mapping results in the polynomial domain, broken down by meta-mapping type.]{Meta-mapping results in the polynomials domain, broken down by meta-mapping type. We plot a normalized performance measure, as in Fig. \ref{fig:HoMM_polynomials:results}. The system is performing well across all meta-mapping types, although there is some variability. Triangles show performance of a baseline model that does not adapt --- note that this baseline performs decently on some meta-mappings, while in other cases such a model results in worse performance than outputting all zeros.} \label{fig:HoMM_polynomials:results_by_mapping}
\end{figure}

As a proof of concept, we first demonstrate the success of our approach in polynomial regression (see Fig. \ref{fig:HoMM_polynomials:tasks}). Specifically, we construct basic tasks that consist of regressing polynomial functions (of degree \(\leq 2\)) in four variables (i.e. from \(\mathbb{R}^4 \rightarrow \mathbb{R}\)). We sampled these polynomials by first uniformly sampling a subset of variables to be included, and then sampling coefficients from \(\mathcal{N}(0, 2.5)\) for the possible monomials. These basic tasks can be inferred from (input, output) tuples, where the input is a point in \(\mathbb{R}^4\) and the output is the evaluation of that polynomial at that point. (We actually restrict the input range to \([-1, 1]\) to avoid testing extreme outlier points.) This is a simple meta-learning regression problem. 

These tasks/polynomials can then be transformed by various meta-mappings --- we considered squaring a polynomial, permuting its variables, or adding or multiplying by a constant. We trained the model on 100 basic polynomials, and we train mapped versions of 60 of these for each meta-mapping. We can evaluate the performance of that meta-mapping on the remaining 40 target tasks (corresponding to the 40 other basic polynomials) that the model has never experienced before. We also held out some of these meta-mappings to evaluate the ability of our method to generalize at the meta-mapping level (see above). For example, we can train the model to adapt to a subset of the input variable permutations, and then evaluate its ability to adapt to a held-out permutation based on examples of that permutation. In total, we trained on 20 meta-mappings, and held-out 16, corresponding to 2260 (\(=100 + 60 \times 36 \)) trained basic tasks, and 1440 held-out for evaluation. 

\textbf{Training:} We trained the system in epochs, during which it received one training step on each trained basic task and one training step on each trained meta-mapping, interleaved in a random order. For one step of training on a basic task, we used 1024 evaluations of the polynomial --- we present the model with 50 example evaluations from which to generate a task representation, and make one gradient update that improves the predictions on the remaining evaluations (as well as the example ones). This encourages the model to generate an accurate representation of the polynomial from seeing a (relatively) small set of evaluations. 

For one step of meta-mapping training, we take task representations for each of the 60 basic tasks (and corresponding target tasks) on which the meta-mapping is trained, where each basic task representation is computed from 50 examples as above. We randomly chose half of these (input task, output task) pairs to provide as examples of the meta-mapping from which to generate a representation, and train the system to match the output embeddings from the meta-mapping to the targets for all 60 examples. This encourages the model to generate a representation of the meta-mapping from half the available examples that will generalize to the other half.

To evaluate the system on a trained meta-mapping, we parameterize the mapping using all 60 input-output function embedding pairs that were used to train the meta-mapping, and evaluate the performance resulting from applying that mapping to the other 40 basic tasks to perform their 40 corresponding held-out target tasks. The system never experienced those 40 target tasks during training. Similarly, to evaluate on a held-out meta-mapping, we use 60 (input task, output task) pairs where both the input and target basic tasks were trained, and evaluate on 40 trained input tasks for which the corresponding 40 targets have not been trained. However, in the case of a held-out meta-mapping, the meta-mapping itself is never encountered during training. This ensures that the system is able to infer a new meta-mapping based on basic tasks that it has experienced, mapped in a way it has not experienced. 

Furthermore, when evaluating a meta-mapping (either trained or held-out), we do not simply evaluate how closely the output embeddings match the targets. Instead, we use each of those output embeddings to perform the appropriate task --- i.e. we use a dataset of 1024 polynomial evaluations to compute the MSE between the predictions produced by the model with the mapped embedding, and the true target polynomial. See the Supplemental Information for futher details on training and evaluation.

\subsubsection{Results}
In Fig. \ref{fig:HoMM_polynomials:results}, we show the success of our meta-mapping approach in this setting. We plot a normalized performance measure, which is 0\% if the system outputs all zeros for every polynomial, and 100\% if the system performs perfectly. Specifically, we measure performance as \(1 - \text{loss}/c\), where \(c\) is the loss for a baseline model that always outputs zero.\footnote{This measure is closely related to the variance explained, except that the square meta-mapping skews the mean of the output polynomials slightly away from zero.} We also show performance of a baseline model which just performs the original tasks without adaptation (dotted lines). Our HoMM approach is able to achieve 89.0\% performance (bootstrap 95\%-CI across runs [88.3, 89.8]) on a polynomial it has never experienced during training, based on a trained meta-mapping, and 85.5\% performance (bootstrap 95\% CI [85.1, 85.9]) based on a held-out meta-mapping. By comparison, not adapting would yield only 4.3\% and 19.3\% performance, respectively. That is, our system is able to achieve good performance on a new task without any data, based only on its relationship to prior tasks. It is able to do so much better than a baseline model which does not adapt to the new tasks. 

To further explore this performance, in Fig. \ref{fig:HoMM_polynomials:results_by_mapping} we plot the results for each of the different meta-mappings we considered: adding a constant, multiplying by a constant, permuting the variables, and squaring the polynomial. Some of these mappings are more challenging than others, as can be seen from the performance of a baseline model that does not adapt (dotted lines). For example, adding a constant to a polynomial does not alter it too drastically, so the non-adapting baseline performs well there. By contrast, multiplying by a constant sometimes changes the sign of the polynomial, so the non-adapting baseline performs extremely poorly there. The HoMM approach results in good performance across all the types of meta-mappings we considered, although unsurpsingly performance is slightly better on the easier tasks, and generalization to the held-out permutation meta-mappings is more challenging than generalization to e.g. the held-out addition ones. 

\textbf{Representation analysis:} In order to explore the performance of the model further, we performed principal components analysis on the task and meta-mapping representations in the HoMM model after training (Fig. \ref{fig:HoMM_polynomials:reps_overall_PCA}). This analysis reveals strikingly similar organization of the representation space across different training runs, with constant polynomials pushed to the outside in a semi-circle, and more complex polynomials stretching toward the center, where meta-mappings and meta-classifications are located. This may be due to the learning dynamics --- the distance of the task representations from the center appears to be roughly inversely proportional to the complexity of the task, which might imply that the constant polynomials have the largest-magnitude representations because they are easiest to learn. 

To analyze this further, in Fig. \ref{fig:HoMM_polynomials:reps_const_poly_PCA} we plot the representations for only the constant polynomials, colored by their value (square-root compressed for clarity). This shows that the constant polynomials are consistently arrayed angularly from lowest to highest value.

Finally, we examined the meta-mapping representations more closely (Fig. \ref{fig:HoMM_polynomials:reps_meta_mapping_PCA}). This analysis shows that the mappings have a consistent organization across runs, with permutations and addition grouping tightly, but multiplication and squaring, which more drastically alter the polynomials, more dispersed. In particular, multiplying by negative numbers and squaring, which can change polynomials signs and therefore cause a more drastic adaptation, are more separated from the remaining meta-mappings. It is also interesting to note that the addition meta-mappings appear to be organized more by absolute value than sign in at least some runs. 

\begin{figure}[ptbh]
\centering
\includegraphics[width=\textwidth]{2-HoMM/figures/polynomial_reps/overall_PCA.png}
\caption[Principal components of task and meta-mapping representations of HoMM after training on the polynomials domain.]{Principal components of task and meta-mapping representations of HoMM after training on the polynomials domain. The representation space is organized relatively consistently across runs, with constant polynomials pushed to the outside, and meta-mappings and meta-classifications more centrally located.} \label{fig:HoMM_polynomials:reps_overall_PCA}
\end{figure}

\begin{figure}[ptbh]
\centering
\includegraphics[width=\textwidth]{2-HoMM/figures/polynomial_reps/constant_poly_PCA.png}
\caption[Principal components of constant polynomial representations, showing systematic organization by value.]{Principal components of constant polynomial representations, showing systematic organization by value. Intriguingingly, this relationship appears to be systematically non-linear across runs. (PCs computed across all task representations, color scale of values is compressed with a square-root transformation.)} \label{fig:HoMM_polynomials:reps_const_poly_PCA}
\end{figure}

\begin{figure}[ptbh]
\centering
\includegraphics[width=\textwidth]{2-HoMM/figures/polynomial_reps/meta_mapping_PCA.png}
\caption[Principal components of meta-mapping representations in the polynomial domain, showing systematic organization by type.]{Principal components of meta-mapping representations in the polynomial domain, showing systematic organization by type. Permutation mappings cluster tightly, as do addition, while multiplication and squaring are more dispersed. The addition and multiplication mappings are partially organized by absolute value.} \label{fig:HoMM_polynomials:reps_meta_mapping_PCA}
\end{figure}

\subsubsection{Inadequacy of vector analogies for meta-mapping} \label{sec:HoMM:vector_analogies_inadequate}

One possible implementation of meta-mapping would be to just construct an analogy vector and use that for the mapping. This is motivated by work showing that word vector representations often support vector analogical reasoning; for example if we denote the vector for the word king as \(\vec{v}_{king}\), relationships like \(\vec{v}_{queen} \approx \vec{v}_{king} + \left(\vec{v}_{man} - \vec{v}_{woman} \right)\) often hold \citep{Mikolov2013}. Thus, adopting a similar strategy for meta-mapping would be superficially plausible. For example, in the polynomials domain, the meta-mapping ``Permute \((w, z, x, y)\)'' could be estimated by taking the vector differences between the representations of inputs and targets, computing an average difference vector, and adding that to the held-out examples to produce an output for each one.

However, in this section, we prove that such an approach cannot accurately represent all the meta-mappings in the polynomials domain. Furthermore, we sketch a proof by construction that our linear task network (i.e. an affine transformation, matrix multiplication plus a bias vector) parameterized independently for each meta-mapping suffices.

\textbf{Proof that vector analogies are inadequate:} In essence, the proof is simply that many of our meta-mappings are non-commutative, while vector addition is commutative. Consider the mappings for adding 1 to a polynomial, and multiplying by 2. Assume there were vector representations for these mappings, respectively \(\vec{m}_{+1}\) and \(\vec{m}_{\times 2}\). Let \(\vec{f}_{x}\) be the vector representation for the polynomial \(f(w,x,y,z) = x\). Then \(\vec{f}_{x} + \vec{m}_{+1} = \vec{f}_{x+1}\), \(\vec{f}_{x} + \vec{m}_{\times 2} = \vec{f}_{2x}\). But then:
\[ \vec{f}_{2(x + 1)} = \left(\vec{f}_{x} + \vec{m}_{+1}\right) + \vec{m}_{\times 2} = \vec{f}_{x} + \vec{m}_{+1} + \vec{m}_{\times 2} = \left(\vec{f}_{x} + \vec{m}_{\times 2}\right) + \vec{m}_{+1} = \vec{f}_{2x + 1}\]
Thus such a representation would result in contradictions, such as \(2x + 1 = 2x + 2\). Similar issues occur for permutation and other non-commutative mappings.

\textbf{Proof sketch that affine transformations in an appropriate vector space suffice:} Suppose that we have a vector representation for the polynomials, where there is a basis dimension corresponding to each monomial, so that the polynomial can be represented as a vector of its coefficients. (This is the standard vector-space representation for polynomials.) Then permutation corresponds to permuting these monomials, i.e. a permutation of the basis dimensions, which is a linear transformation. Adding a constant corresponds to adding to one dimension, which requires only the vector addition part of the affine transformation. Multiplying by a constant requires multiplying each dimension, i.e. a block-diagonal linear transformation.

Squaring polynomials is slightly more complex, and requires augmenting the vector space with components whose values are the product of the coefficients of each pair of monomials. In this case, squaring corresponds to a simple linear transformation. However, this augmentation makes the other meta-mappings more complex. The most difficult case is adding a constant, which requires shifting each pair term containing a constant by the product of the constant and the coefficient of the other monomial, but this again reduces to simply an appropriately parameterized affine transformation --- each pair term containing a constant term simply needs the added constant as a weight times the component for the other monomial. Thus affine transformations suffice in this setting.

Of course, with a sufficiently complex, deep, recurrent, and non-linear task network, any meta-mapping could be computed in principle, since a sufficiently complex network is Turing-complete \citep{Siegelman1992}. Thus, our approach to meta-mapping is fully general, conditioned on a sufficiently complex task network, while simpler approaches may not be.



\subsection{Aside: HoMM \& cognitive control}
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{2-HoMM/figures/stroop_results.png}
\caption[Measuring the default behavior of the HoMM architecture on a Stroop-like task.]{Measuring the default behavior of the HoMM architecture on a Stroop-like task. We plot the bias of the model towards word or color responses, when given an all-zeros task representation, at different proportions of training on words or colors, and different stages of training. When the model has just mastered the less frequent task, it exhibits a default bias towards the more frequent task. However, later in training, when it has mastered both tasks, it exhibits a paradoxical bias towards the \emph{less} frequent task.} \label{supp_fig:HoMM_cognitive_control}
\end{figure}

Although it is not the primary focus of this project, the HoMM architecture could be of interest to researchers in cognitive control, even beyond the idea of meta-mapping as adaptation. The architecture offers an instantiation of a model which can perform different tasks based on task examples or language inputs, which is fundamentally the same problems human face when we must adapt our behavior. There are a number of features of the model that offer the opportunity for intriguing investigations based on this idea. For example, the task network in our architecture has a default set of bias weights that are modulated by the HyperNetwork. These can be thought of as the ``automatic'' or ``default'' processing habits of the system, whereas the weight alterations the HyperNetwork imposes can be thought of as the exertion of cognitive control to modulate behavior. 

To explore this, we trained our architecture on a very simple stroop task taken from \citet{Cohen1990}. The model receives two sets of two inputs, that can be thought of as corresponding to ``word'' and ``color'' domains. One input in each domain is turned on, representing a color word written in a color. The model's task is to report either the color or the word, depending on context.

The context we give the model is in the form of examples of the task as (input, output) pairs as usual. These are used to construct a task representation, which is then used to modulate the parameters in the task network, via a HyperNetwork.\footnote{For this experiment, we used similar hyperparameters to the polynomials experiments, except we used a much smaller model --- a single-layer task network, a $Z$-dimensionality of 8, and $\mathcal{H}, \mathcal{M}$ had 64 hidden units per layer. We optimized the model via stochastic gradient descent with a learning rate of \(0.01\) to follow more closely the approach taken by Cohen et al., although results are similar with other optimizers.} We trained the model repeatedly with different proportions of training on the word task vs. the color task, in order to investigate the default vs. controlled behavior in different training regimes. Specifically, we compared training the model to the point that it barely mastered the less frequent task (when it first achieves 100\% performance and cross-entropy loss \(< 0.3\) on both tasks) to the point that it had mastered both tasks (100\% performance and cross-entropy loss \(<0.01\) on both). We then tested the model's default behavior by giving it an all-zeros task representation, and seeing whether its performance was more aligned with the ``word'' or ``color'' task.

In Fig. \ref{supp_fig:HoMM_cognitive_control}, we show the results. We plot the bias as \(2 \times (\text{word accuracy} - \text{color accuracy})\), which is \(-1\) if the model is responding only to color, 1 if the model is responding perfectly to word, and 0 if it is responding equally to each (or otherwise responding randomly). When the model has just barely mastered the less-frequent task, it exhibits a default bias towards the more frequent task. However, once we train it to full master of both tasks, it exhibits a surprising paradoxical bias towards the task that was mastered more recently. This may relate to observations that switching from a less-practiced task back to a more practiced one is difficult \citep{Monsell2003}, possibly because performing the less-practiced task requires strong suppression of the default behavior. It's possible that in the course of achieving full mastery on the less-practiced task, the more practiced task must be so suppressed that it fades away from being the default. These phenomena provide possible inspiration for future investigations in cognitive control.


\section{Discussion}\label{sec:HoMM:discussion}

We have proposed meta-mappings as a computational account of the human ability to perform a novel task zero-shot (without any data), based on the relationship between the novel task and prior tasks. We have shown that our homiconic meta-mapping architecture performs well in a proof-of-concept polynomial regression setting. Here, we will briefly discuss some of the work in machine learning and cognitive science that inspired this project. We will discuss the broader implications of our work and future directions in more detail in Chapter \ref{chapter:conclusions}. 

\subsection{Related work in machine learning}
As mentioned above, there is a large body of prior work on zero-shot learning based on natural-language descriptions of tasks. \citet{Larochelle2008} considered the general problem of behaving in accordance with language instructions as simply asking a model to adapt its response when conditioned on different ``instruction'' inputs. Later work explored zero-shot classification based on only a natural language description of the target class \citep{Socher2013,Romera-Paredes2015,Xian2018}, or of a novel task in a language-conditioned RL system \citep{Hermann2017, Hill2019a}. Some of this work has even exploited relationships between tasks as a learning signal \citep{Oh2017a}. Other work has considered how similarity between tasks can be useful for generating representations for a new task \citep{Pal2019}, but without transforming task representations to do so. Furthermore, similarity is less specific than an input-output mapping, since it does not specify \emph{along which dimensions} two tasks are similar. To my knowledge none of the prior work has proposed using meta-mapping-like approaches to adapt to new tasks by transforming task representations, nor has the prior work proposed a parsimonious homoiconic model which can perform these mappings.

This work is also related to the rapidly-growing literature on meta-learning \citep[e.g.][]{Vinyals2016, Santoro2016, Finn2017a, Finn2018, Stadie2018, Botvinick2019, Ravichandran2019}. Our architecture builds directly off of prior work on HyperNetworks \citep{Ha2016} and other recent applications thereof  \citep[e.g.][]{Brock2018a, Zhang2019, Li2019a, Rusu2019}. In particular, recent work in natural language processing has shown that having contextually generated parameters can allow for zero-shot task performance, assuming that a good representation for the novel task is given \citep{Platanios2017} -- in their work this representation was evident from the factorial structure of translating between many pairs of languages. Our work is also related to the longer history of work on different time-scales of weight adaptation \citep{Hinton1982, Kumaran2016} that has more recently been applied to meta-learning contexts \citep[e.g.][]{Ba2016, Munkhdalai2017, Garnelo2018} and continual learning \citep[e.g.]{Hu2019}. It is more abstractly related to work on learning to propose architectures \citep[e.g.][]{Zoph2016, Cao2019}, and to models that learn to select and compose skills to apply to new tasks \citep[e.g.][]{Andreas, Andreas2016, Tessler2016, Reed2015, Chang2019a}. In particular, some of the work in domains like visual question answering has explicitly explored the idea of building a classifier conditioned on a question \citep{Andreas, Andreasa}, which is related to our approach in the visual categorization tasks (Chapter \ref{sec:extending:concepts}).

Work in model-based reinforcement learning has also partly addressed how to transfer knowledge between different reward functions \citep[e.g.][]{Laroche2017}; the HoMM approach is more general. For example, rather than needing a new reward function to be given, meta-mapping provides a principled way to infer a new reward estimator by transforming a prior one. Meta-mapping could also be used to transform a transition function used in the planning model in response to environmental changes. Our insights could therefore complement model-based approaches, which provides an exciting direction for future work.

There has also been other recent interest in task (or function) embeddings. Achille et al. \citep{Achille2019} recently proposed computing embeddings for visual tasks from the Fisher information of the parameters in a model partly tuned on the task. They show that this captures some interesting properties of the tasks, including some types of semantic relationships, and can help identify models that can perform well on a task. Rusu and colleagues recently suggested a similar meta-learning framework where latent codes are computed for a task which can be decoded to a distribution over parameters \citep{Rusu2019}. Other recent work has tried to learn representations for skills \citep[e.g.][]{Eysenbach2019} or tasks \citep[e.g.]{Hsu2019} for exploration and representation learning, but has not explored transforming these representations to achieve zero-shot performance on a novel task.

In summary, our perspective builds on several lines of prior work in machine learning. While there has been substantial prior work on meta-learning, task representation, and there have even been other approaches to zero-shot task performance, to the best of our knowledge none of the prior work has explored zero-shot performance of a task via meta-mappings. In the following chapters, we will show experimentally that this approach yields better performance than alternative approaches across a variety of domains. We suggest that it may complement other approaches to adaptibility, such as model-based RL.

\subsection{Related work in cognitive science}
Our model is inspired by several streams of research in cognitive science as well. We will briefly review some of these here, in order to provide some grounding for the rest of the dissertation. However, we will discuss most of these issues in greater detail in Chapter \ref{chapter:conclusions}, when we reflect on the dissertation as a whole.   

A first inspiration is a long line of research has suggested that analogical transfer between structurally isomorphic domains may be a key component of ``what makes us smart'' \citep{Gentner2003}. Analogical transfer has been demonstrated across various cognitive domains \citep[e.g.][]{Bourne1970, Day2011}. Yet there has been relatively little exploration of adaptation without any examples of the new task at all. 

This dissertation also touches on the issues of compositionality and systematicity. Some researchers have advocated that cognition must rely on strictly compositional representations in order to exhibit systematic and productive generalization \citep[e.g.][]{Fodor2001,Lake2017}.  We avoided explicitly enforcing compositional task representations in our model, instead allowing those representations to emerge. This has several advantages. First, it requires much less hand-engineering for each application domain (e.g. our model did not need the notion of variables or permutation built into it to generalize to held-out permutation meta-mappings), and second, it may allow for novel decompositions at test time. (See Chapter \ref{chapter:conclusions} for further discussion.) 

Some aspects of the HoMM architecture may also seem reminiscent of the modularity of mind for which Fodor advocated \citep{Fodor1983modularity}, particularly the fact that we divided the model into feed-forward input and output systems, with the flexible, task-specific computations in the middle shared across many domains. In fact, we believe that perception and task-processing are mutually constraining and reinforcing, but for simplicity our model does not incorporate all aspects of this. Cognitive modeling always requires some simplification in order to provide a useful description of the system. (Again, see Chapter \ref{chapter:conclusions} for further discussion.)   

Finally, as we showed above, the HoMM architecture and approach may related to areas like cognitive control. Similarly, our shared workspace for data points, tasks, and meta-mappings relates to ideas like the Global Workspace Theory of consciousness. Exploring these connections would be an exciting direction for future work. 

In summary, meta-mapping and the HoMM architecture draw inspiration from many areas of research in cognitive science. I hope that this work will reciprocally provide inspiration to many researchers in this field. In support of this, the subsequent chapters explore our model on a variety of tasks more relevant to human cognition, and make direct comparisons to the adaptation abiilites of humans and language-conditioned models. 

