\chapter{Model details \& hyperparameters for all experiments} \label{appendix:model_hyperparameters}
\begin{figure}[!h]
\begin{subfigure}{\textwidth}
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[auto]
%% from examples
\node[text width=0.5cm] at (-6.8, -0.5) (inputs0) {\includegraphics[width=0.5cm]{2-HoMM/figures/7_of_clubs.png}\\\includegraphics[width=0.5cm]{2-HoMM/figures/2_of_spades.png}};

\node[gray, text width=2cm, align=center] at (-5.6, 0.35) {Perception network};
\node[block] at (-5.6, -0.5) (perceptionnet0) {\(\mathcal{P}\)};
\path[arrow] (inputs0.east) -- ([xshift=-3]perceptionnet0.west);


\node[text width=0.5cm] at (-6.8, -2) (targets0) {\bf \color{red}\(-\)\$\$};

\node[gray, text width=2cm, align=center] at (-5.6, -2.85) {Target network};
\node[block] at (-5.6, -2) (targetnet0) {\(\mathcal{P}\)};
\path[arrow] ([xshift=3]targets0.east) -- ([xshift=-3]targetnet0.west);

\node[gray, text width=2.5cm, align=center] at (-3.25, 2.3) {Task examples (encoded)};
\node at (-3.25, 1.25) (examples) {
\(\left\{
\begin{matrix}
({\color{bgreen}z_{hand_{1}}}, {\color{bgreen}z_{bet_{1}}})\\
$\vdots$
\end{matrix}\right\}\)};

\path[arrow, out=0, in=-90] (perceptionnet0.east) to ([xshift=-11, yshift=20]examples.south);
\path[arrow, out=0, in=-90] (targetnet0.east) to ([xshift=15, yshift=20]examples.south);

\node[gray, text width=2cm, align=center] at (-0.5, 2.1) {Example network};
\node[block] at (-0.5, 1.25) (examplenet) {\(\mathcal{E}\)};
\path[arrow] (examples.east) -- ([xshift=-3]examplenet.west);

%% performing 
\node[bpurp] at (1.25, 1.25) (taskrep) {\(z_{task}\)};
\path[arrow] ([xshift=3]examplenet.east) -- (taskrep.west);

\node[gray, text width=2cm, align=center] at (3, 2.1) {Hyper network};
\node[block] at (3, 1.25) (hypernet) {\(\mathcal{H}\)};
\path[arrow] (taskrep.east) -- ([xshift=-3]hypernet.west);

\node[text width=0.5cm] at (0.7, -1.25) (inputs) {\includegraphics[width=0.5cm]{2-HoMM/figures/2_of_spades.png}\\\includegraphics[width=0.5cm]{2-HoMM/figures/4_of_hearts.png}};

\node[gray, text width=2cm, align=center] at (1.9, -0.4) {Perception network};
\node[block] at (1.9, -1.25) (perceptionnet) {\(\mathcal{P}\)};
\path[arrow] (inputs.east) -- ([xshift=-3]perceptionnet.west);

\node[bgreen] at (3.2, -1.25) (handrep) {\(z_{hand}\)};
\path[arrow] ([xshift=3]perceptionnet.east) -- (handrep.west);

\node[bblue, block, dashed] at (4.5, -1.25) (tasknet) {\(\mathcal{T}\)};
\node[bblue, text width=3cm, align=center] at (4.5, -2) {Task network};
\path[arrow] (handrep.east) -- ([xshift=-3]tasknet.west);
\path[arrow, out=0, in=90] ([xshift=3]hypernet.east) to ([yshift=3]tasknet.north);


\node[bgreen] at (5.65, -1.25) (betrep) {\(z_{bet}\)};
\path[arrow] ([xshift=3]tasknet.east) -- (betrep.west);

\node[gray, text width=2cm, align=center] at (6.8, -0.4) {Action network};
\node[block] at (6.8, -1.25) (actionnet) {\(\mathcal{A}\)};
\path[arrow] (betrep.east) -- ([xshift=-3]actionnet.west);

\node at (7.7, -1.25) (output) {\bf \$};
\path[arrow] ([xshift=3]actionnet.east) -- (output.west);

\node at (8.7, -1.25) (loss) {Loss};
\path[arrow] (output.east) -- (loss.west);

% gradients

\path[arrow, bred, ultra thick] ([yshift=-12, xshift=20]loss.west) to ([yshift=-12, xshift=3]inputs.east);

\path[draw, bred, ultra thick, out=90, in=0] ([yshift=-12, xshift=-10]tasknet.west) to ([yshift=-12, xshift=-10]hypernet.east);

\path[draw, bred, ultra thick] ([yshift=-12, xshift=-10]hypernet.east) to ([yshift=-12, xshift=-40]examples.east);

\path[draw, bred, ultra thick, out=-90, in=0] ([yshift=-12, xshift=-40]examples.east) to ([yshift=-12, xshift=-10]perceptionnet0.east);
\path[arrow, bred, ultra thick] ([yshift=-12, xshift=-10]perceptionnet0.east) to ([yshift=-12]inputs0.east);

\path[draw, bred, ultra thick, out=-90, in=0] ([yshift=-12, xshift=-12]examples.east) to ([yshift=-12, xshift=-10]targetnet0.east);
\path[arrow, bred, ultra thick] ([yshift=-12, xshift=-10]targetnet0.east) to ([yshift=-12]targets0.east);

\end{tikzpicture}
}
\caption{Basic task inference/training (from examples).} \label{supp_fig:HoMM:gradient_flow:basic_tasks}
\end{subfigure}
\begin{subfigure}{\textwidth}
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[auto]
%% from examples
\node at (-6.9, 0) {};

\node[gray, text width=3.5cm, align=center] at (-3.25, 2.3) {Mapping examples (input/output tasks)};
\node at (-3.25, 1.25) (examples){
\(\left\{
\begin{matrix}
({\color{bpurp}z_{chess}}, {\color{bpurp}z_{lose chess}})\\
$\vdots$
\end{matrix}\right\}\)};

\node[gray, text width=2cm, align=center] at (-0.5, 2.1) {Example network};
\node[block] at (-0.5, 1.25) (examplenet) {\(\mathcal{E}\)};
\path[arrow] (examples.east) -- ([xshift=-3]examplenet.west);

%% performing 
\node[borange] at (1.25, 1.25) (taskrep) {\(z_{meta}\)};
\path[arrow] ([xshift=3]examplenet.east) -- (taskrep.west);

\node[gray, text width=2cm, align=center] at (3, 2.1) {Hyper network};
\node[block] at (3, 1.25) (hypernet) {\(\mathcal{H}\)};
\path[arrow] (taskrep.east) -- ([xshift=-3]hypernet.west);

\node[bpurp] at (2.66, -1.25) (handrep) {\(z_{poker}\)};

\node[bblue, block, dashed] at (4.5, -1.25) (tasknet) {\(\mathcal{T}\)};
\node[bblue, text width=3cm, align=center] at (4.5, -2) {Task network};
\path[arrow] (handrep.east) -- ([xshift=-3]tasknet.west);
\path[arrow, out=0, in=90] ([xshift=3]hypernet.east) to ([yshift=3]tasknet.north);

\node[bpurp] at (6.5, -1.25) (output) {\(\hat{z}_{lose poker}\)};
\path[arrow] ([xshift=3]tasknet.east) -- (output.west);

\node at (8.7, -1.25) (loss) {Loss};
\path[arrow] (output.east) -- (loss.west);

% gradients

\path[arrow, bred, ultra thick] ([yshift=-12, xshift=20]loss.west) to ([yshift=-12, xshift=5]handrep.east);

\path[draw, bred, ultra thick, out=90, in=0] ([yshift=-12, xshift=-10]tasknet.west) to ([yshift=-12, xshift=-10]hypernet.east);

\path[arrow, bred, ultra thick] ([yshift=-12, xshift=-10]hypernet.east) to ([yshift=-12, xshift=10]examples.east);
\end{tikzpicture}
}
\caption{Meta-mapping inference/training (from examples).}\label{supp_fig:HoMM:gradient_flow:meta_mappings}
\end{subfigure}
\caption[Schematic of architecture, showing inference and gradient flow through the model on a training step.]{Schematic of architecture, showing inference and gradient flow through the model on a training step. Thin black lines moving rightward represent inference, thick red lines moving leftward represent gradients. (\subref{supp_fig:HoMM:gradient_flow:basic_tasks}) Inference and gradients for the basic tasks. (\subref{supp_fig:HoMM:gradient_flow:basic_tasks}) Inference and gradients for meta-mappings. The gradients end at the examples of the meta-mapping, rather than propagating through to alter how those representations are constructed, due to GPU memory constraints. In the future, it might be useful to explore whether allowing further propagation would improve results for both basic tasks and meta-mappings. (These figures depict the inference/gradient flow when performing tasks and meta-mappings from examples, performing from language is similar, except that the example inputs and example network are replaced with language inputs and the language processing network.)} \label{supp_fig:HoMM:gradient_flow}
\end{figure}

In Fig. \ref{supp_fig:HoMM:gradient_flow}, we show the flow of inference (forward) and gradients (backward) through the HoMM architecture on basic task and meta-mapping training steps.

\begin{table}
\scriptsize
\centering
\begin{tabular}{|p{3cm}||c|c|c|c|}
\hline
& Polynomials & Cards & Visual & RL \\\hline
\hline
$Z$-dimension & 512 & 512 & 512 & 512 \\\hline
$\mathcal{I}$ num. layers & \multicolumn{4}{c|}{2} \\\hline
$\mathcal{I}$ num. hidden units & \multicolumn{4}{c|}{128} \\\hline
$\mathcal{I}$ conv. layers. (num filters, size, all strides are 2) & \multicolumn{2}{c|}{-} & \multicolumn{1}{p{2.3cm}|}{(64, 5), (128, 4), (256, 4), (512, 2), max pool} & \multicolumn{1}{p{2.3cm}|}{(64, 7), (64, 4), (64, 3)}\\\hline
$\mathcal{L}$ architecture & -  & \multicolumn{3}{c|}{2-layer LSTM + 2 fully-connected} \\\hline
$\mathcal{L}$ num. hidden units & -  & \multicolumn{3}{c|}{512} \\\hline
$\mathcal{T}$ num. layers & 1 & 3 & 1 & 3 \\\hline
$\mathcal{T}$ num. hidden units & - & 128 & - & 128 \\\hline
$\mathcal{E}$ architecture & \multicolumn{4}{c|}{2 layers per-datum, max pool across, 2 layers} \\\hline
$\mathcal{H}$ architecture & \multicolumn{4}{c|}{4 layers} \\\hline
$\mathcal{E}$ num. hidden units & \multicolumn{3}{c|}{512} & 1024 \\\hline
$\mathcal{H}$ num. hidden units & \multicolumn{4}{c|}{512} \\\hline
Task, MM representations from & \multicolumn{2}{c|}{Examples} & Language & Examples \\\hline
$\mathcal{F}$ num. layers & 3 & 1 & HoMM: 1, Lang: 3 & 3 \\\hline
$\mathcal{F}$ num. hidden units & \multicolumn{3}{c|}{64} & 128 \\\hline
$\mathcal{F}$ init scale & 1 & 1 & 30 & 10 \\\hline
$\mathcal{F}$ weight norm. \citep{Salimans2016} & \multicolumn{3}{c|}{No} & Yes \\\hline
$\mathcal{A}$ num. layers & \multicolumn{2}{c|}{1} & 2 & 1 \\\hline
$\mathcal{A}$ num. hidden units & \multicolumn{2}{c|}{-} & 128 & -  \\\hline
Nonlinearities & \multicolumn{4}{p{11cm}|}{Leaky ReLU most places, except no non-linearity at final layer of networks outputting to $Z$, sigmoid for classification outputs, and softmax over actions.} \\\hline
Base task loss & $\ell_2$ & $\ell_2$ (masked) & Cross-entropy & $\ell_2$ (masked)\\\hline
Meta-mapping loss & \multicolumn{4}{c|}{$\ell_2$}\\\hline
Partially-persistent task embeddings & \multicolumn{3}{c|}{No} & Yes \\\hline
Persistent embedding match loss weight & \multicolumn{3}{c|}{-} & 0.2 \\\hline
\hline
Optimizer & Adam & \multicolumn{3}{c|}{RMSProp} \\\hline
Learning rate (base) & $3\cdot 10^{-5}$ & $1\cdot 10^{-5}$ & $3\cdot 10^{-5}$ & $1\cdot 10^{-4}$\\\hline
Learning rate (meta) & $1\cdot 10^{-5}$ & $1\cdot 10^{-5}$ & $1\cdot 10^{-5}$ & $1\cdot 10^{-4}$\\\hline
L.R. decay rate (base) & $\times0.85$ & $\times0.85$ & $\times0.8$ & $\times0.8$\\\hline
L.R. decay rate (meta) & $\times0.85$ & $\times0.9$ & $\times0.85$ & $\times0.95$ \\\hline
L.R. min (base) & \multicolumn{2}{c|}{$3 \cdot 10^{-8}$}  & $1 \cdot 10^{-8}$ & $3 \cdot 10^{-8}$\\\hline
L.R. min (meta) & $1 \cdot 10^{-7}$& $3 \cdot 10^{-8}$ &  $1 \cdot 10^{-8}$ & $3 \cdot 10^{-7}$\\\hline
L.R. decays every & 100 epochs & 200 epochs & 400 epochs & 10000 \\\hline
Num. training epochs & 5000 & \multicolumn{1}{p{2.3cm}|}{100000 (optimally stopped)} & \multicolumn{1}{p{2.3cm}|}{10000 for 4 train mappings, 7500 for 8, 5000 for others} & \multicolumn{1}{p{2.3cm}|}{300000 (optimally stopped)} \\\hline
Num. runs & 5 & 5 & 10 & 5 \\ \hline
\hline
Num. base tasks (training) & \multicolumn{1}{p{2.3cm}|}{1300 ( $= 60 + 60 \times  20 + 40$)} & 36 & Varies & 18 \\\hline
Num. base tasks (held out for MM eval) & 800 ($= 40 \times 20$)  & 4 & Varies & 2 \\\hline
Num. meta classifications & 6 & 8 & 8 & - \\\hline
Num. train MMs & 20 & 3 & Varies & 1 \\\hline
Num. held-out MMs & 16 & 0 & 2 & 0  \\\hline
Base dataset size & 1024 & 1024 & 336 & 64 \\\hline
Base examples size & 50 & 768 & - & 32 \\\hline
Meta dataset size (train) & 60 & 36 & Varies & 18 \\\hline
Meta examples (train) & \multicolumn{2}{c|}{Half of train dataset} & - & Half of train dataset \\\hline
Meta examples (eval) & \multicolumn{2}{c|}{All of train dataset} & - & All of train dataset \\\hline
Base datasets refreshed & \multicolumn{2}{c|}{Every 50 epochs} & Every 20 & Every 1500  \\\hline
Target network updated & \multicolumn{3}{c|}{-} & Every 10000 epochs  \\\hline
RL discount & \multicolumn{3}{c|}{-} & 0.85 \\\hline
RL explore prob. (\(\epsilon\)) & \multicolumn{3}{c|}{-} & \multicolumn{1}{p{2.5cm}|}{Init: 1, decay: -0.03}\\\hline
Action softmax \(\beta\) & - & 8 & - & 8\\\hline
\end{tabular}
\caption[Detailed hyperparameter specification.]{Detailed hyperparameter specification for different experiments. A ``-'' indicates a parameter that does not apply to that experiment. As a reminder: the shared representational space is denoted by $Z$. Input encoder: $\mathcal{I}: \text{input} \rightarrow Z$. Action decoder $\mathcal{A}: Z \rightarrow \text{output}$. Target encoder $\mathcal{T}: \text{targets} \rightarrow Z$. Meta-network $\mathcal{E}: \{(Z, Z), ...\} \rightarrow Z $ maps examples to a task representation. Hyper-network $\mathcal{H}: Z \rightarrow \text{parameters}$. Task network $F: Z \rightarrow Z$ is parameterized by $\mathcal{H}$. Language encoder: $\mathcal{L}: \text{language} \rightarrow Z$. } \label{supp_hyperparameter_table}
\end{table}
See table \ref{supp_hyperparameter_table} for detailed architectural description and hyperparameters for each experiment. Hyperparameters were generally found by a heuristic search, where mostly only the optimizer, learning rate annealing schedule, and number of training epochs were varied. Some of the parameters take the values they do for fairly arbitrary reasons, e.g. the polynomial experiments were run earlier, before 1-layer task networks were found to be useful in some settings. While it would be ideal to fully search the space of parameters for all models, unfortunately our computational resource limitations prohibited it. Thus the results in the paper should be interpreted as a lower bound on what would be possible. \par
Each epoch consisted of a separate learning step on each task (both base and meta), in a random order. In each task, the meta-learner would receive only a subset (the ``batch size`` above) of the examples to generate a function embedding, and would have to generalize to the remainder of the examples in the dataset. The embeddings of the basic tasks used for meta-mappings were computed and cached once per epoch, so as the network learned over the course of the epoch, these task-embeddings would get ``stale,'' but this did not seem to be too detrimental. In the case of the RL tasks, where there were persistent task embeddings, they were used insteadd.\par
The results reported in the figures in this paper are averages across multiple runs, with different trained and held-out tasks (in the polynomial and visual concepts cases) and different network initializations and training orders each epoch (in all cases), to ensure the robustness of the findings. \par

\subsubsection{Source repositories}
%The full code for all experiments and analyses will be made available via github in the de-anonymized verison.
The full code for the experiments and analyses can be found on github:
\begin{itemize}
\item HoMM library: \url{https://github.com/lampinen/HoMM}
%\item This paper's source: \url{https://github.com/lampinen/metamapping_paper}
\item Polynomials: \url{https://github.com/lampinen/HoMM_polynomial_analysis}
\item Cards (models): \url{https://github.com/lampinen/HoMM_cards}
\item Cards (human experiment): \url{https://github.com/lampinen/cards_for_humans}
\item Concepts: \url{https://github.com/lampinen/categorization_HoMM}
\item RL: \url{https://github.com/lampinen/HoMM_grids}
\item Stroop results (below): \url{https://github.com/lampinen/stroop}
\end{itemize}



\section{Clarifying meta-mapping} \label{app_clarifying_meta_mapping}
\subsection{A definitional note}
When we discussed meta-mappings in the main text, we equivocated between tasks and behaviors for the sake of brevity. For a perfect model, this is somewhat justifiable, because each task will have a corresponding optimal behavior, and the sytem's embedding of the task will be precisely the embedding which produces this optimal behavior. However, behavior-irrelevant details of the task, like the color of the board, may not be embedded, so this should not really be thought of as a task-to-task mapping. This problem is exacerbated when the system is imperfect, e.g. during learning. It is thus more precise to distinguish between a ground-truth meta-mapping, which maps tasks to tasks, and the computational approach to achieving that meta-mapping, which really maps between representations which combine both task and behavior. \par



\chapter{Supplemental material for Chapter \getrefnumber{chapter:zero_shot_via_homm}} \label{appendix:zero_shot_via_homm}

\section{Details of polynomial task domain} \label{app:HoMM:polynomials_methods}
We randomly sampled the train and test polynomials as follows:
\begin{enumerate}
\item Sample the number of relevant variables ($k$) uniformly at random from 0 (i.e. a constant) to the total number of variables.
\item Sample the subset of $k$ variables that are relevant from all the variables.
\item For each term combining the relevant variables (including the intercept), include the term with probability 0.5. If so give it a random coefficient drawn from $\mathcal{N}(0, 2.5)$.
\end{enumerate}
The data points on which these polynomials were evaluated were sampled uniformly from $[-1, 1]$ independently for each variable, and for each polynomial. The datasets were resampled every 50 epochs of training. \par
\textbf{Meta-tasks:} For meta-tasks, we trained the network on 6 task-embedding classification tasks:
\begin{itemize}
\item Classifying polynomials as constant/non-constant.
\item Classifying polynomials as zero/non-zero intercept.
\item For each variable, identifying whether that variable was relevant to the polynomial.
\end{itemize}
We trained on 20 meta-mapping tasks, and held out 16 related meta-mappings.
\begin{itemize}
\item Squaring polynomials (where applicable).
\item Adding a constant (trained: -3, -1, 1, 3, held-out: 2, -2).
\item Multiplying by a constant (trained: -3, -1, 3, held-out: 2, -2).
\item Permuting inputs (trained: 1320, 1302, 3201, 2103, 3102, 0132, 2031, 3210, 2301, 1203, 1023, 2310, held-out: 0312, 0213, 0321, 3012, 1230, 1032, 3021, 0231, 0123, 3120, 2130, 2013).
\end{itemize}


%\section{Card game $t$-SNE} \label{app_cards_tsne}
%We performed $t$-SNE \citep{LaurensvanderMaaten2008} on the task embeddings of the system at the end of learning the card game tasks, to evaluate the organization of knowledge in the network. In fig. \ref{fig_cards_tsne_basic} we show these embeddings for just the basic tasks. The embeddings show systematic grouping by game attributes. In fig. \ref{fig_cards_tsne_full} we show the embeddings of the meta and basic tasks, showing the organization of the meta-tasks by type. (Note: this analysis was performed in an older version of the model than the main results.)\par 
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.8\textwidth]{2-HoMM/figures/basic_tsne_basic_final.png}
%\caption{$t$-SNE embedding of the function embeddings the system learned for the basic card game tasks. (Note that the pairs of nearby embeddings differ in the ``suits rule`` attribute, discussed in appendix \ref{meth_data_cards}.)} 
%\label{fig_cards_tsne_basic}
%\end{figure}%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.9\textwidth]{2-HoMM/figures/basic_tsne_full_final.png}
%\caption{$t$-SNE embedding of the function embeddings the system learned for the meta tasks (basic tasks are included in the background).} 
%\label{fig_cards_tsne_full}
%\end{figure}

\section{Architecture \& training experiments} \label{app_lesion_results}
In this section we consider a few variations of the architecture and training, to justify the choices made in the paper. \par

\subsection{Inadequacy of vector analogies for meta-mapping} \label{supp_sec:HoMM:vector_analogies_inadequate}

One possible implementation of meta-mapping would be to just construct an analogy vector and use that for the mapping. This idea is motivated by work showing that word vector representations often support vector analogical reasoning; for example if we denote the vector for the word king as \(\vec{v}_{king}\), relationships like \(\vec{v}_{queen} \approx \vec{v}_{king} + \left(\vec{v}_{man} - \vec{v}_{woman} \right)\) often hold \citep{Mikolov2013}. Thus, adopting a similar strategy for meta-mapping would be superficially plausible. For example, in the polynomials domain, the meta-mapping ``Permute \((w, z, x, y)\)'' could be estimated by taking the vector differences between the representations of inputs and targets, computing an average difference vector, and adding that to the held-out examples to produce an output for each one.

However, in this section, we prove that such an approach cannot accurately represent all the meta-mappings in the polynomials domain. Furthermore, we sketch a proof by construction that a linear task network (i.e. an affine transformation, matrix multiplication plus a bias vector) parameterized independently for each meta-mapping suffices.

\textbf{Proof that vector analogies are inadequate:} In essence, the proof is simply that many of our meta-mappings are non-commutative, while vector addition is commutative. Consider the mappings for adding 1 to a polynomial, and multiplying by 2. Assume there were vector representations for these mappings, respectively \(\vec{m}_{+1}\) and \(\vec{m}_{\times 2}\). Let \(\vec{f}_{x}\) be the vector representation for the polynomial \(f(w,x,y,z) = x\). Then \(\vec{f}_{x} + \vec{m}_{+1} = \vec{f}_{x+1}\), \(\vec{f}_{x} + \vec{m}_{\times 2} = \vec{f}_{2x}\). But then:
\[ \vec{f}_{2(x + 1)} = \left(\vec{f}_{x} + \vec{m}_{+1}\right) + \vec{m}_{\times 2} = \vec{f}_{x} + \vec{m}_{+1} + \vec{m}_{\times 2} = \left(\vec{f}_{x} + \vec{m}_{\times 2}\right) + \vec{m}_{+1} = \vec{f}_{2x + 1}\]
Thus such a representation would result in contradictions, such as \(2x + 1 = 2x + 2\). Similar issues occur for permutation and other non-commutative mappings.

\textbf{Proof sketch that affine transformations in an appropriate vector space suffice:} Suppose that we have a vector representation for the polynomials, where there is a basis dimension corresponding to each monomial, so that the polynomial can be represented as a vector of its coefficients. (This is the standard vector-space representation for polynomials.) Then permutation corresponds to permuting these monomials, i.e. a permutation of the basis dimensions, which is a linear transformation. Adding a constant corresponds to adding to one dimension, which requires only the vector addition part of the affine transformation. Multiplying by a constant requires multiplying each dimension, i.e. a block-diagonal linear transformation.

Squaring polynomials is slightly more complex, and requires augmenting the vector space with components whose values are the product of the coefficients of each pair of monomials. In this case, squaring corresponds to a simple linear transformation. However, this augmentation makes the other meta-mappings more complex. The most difficult case is adding a constant, which requires shifting each pair term containing a constant by the product of the constant and the coefficient of the other monomial, but this again reduces to simply an appropriately parameterized affine transformation --- each pair term containing a constant term simply needs the added constant as a weight times the component for the other monomial. Thus affine transformations suffice in this setting.

Of course, with a sufficiently complex, deep, recurrent, and non-linear task network, any meta-mapping could be computed in principle, since a sufficiently complex network is Turing-complete \citep{Siegelman1992}. Thus, our approach to meta-mapping is fully general, conditioned on a sufficiently complex task network, while simpler approaches may not be.


\subsection{Basic meta-learning in the polynomials domain}

In Fig. \ref{supp_fig:HoMM:polynomials_basic_meta_learning}, we show that the basic meta-learning is working well in the cards domain. That is, we show that after the example network is presented with a set of example input, output pairs, the system is generalizing well to other points from that polynomial. At the end of training, the mean loss on trained polynomials is 0.025 (bootstrap 95\%-CI [0.02, 0.03]), and for held-out polynomials it is 0.58 (bootstrap 95\%-CI [0.45, 0.70]). Since chance loss is 11.76 for the trained polynomials, and 11.10 for the eval, this corresponds to about 99.8\% of optimal on the trained polynomials, and 94.8\% on the held-out.

\begin{figure}[H]
\centering
\includegraphics[width=0.66\textwidth]{2-HoMM/figures/basic_meta_learning_polynomials.png}
\caption[Basic meta-learning performance in the polynomials domain over learning.]{Basic meta-learning performance in the polynomials domain over learning. The system is generalizing at the meta-learning level. That is, this graph shows that, after the example network receives a set of (input, output) example tuples, it is generating a sufficiently good representation to regress held-out points from that polynomial. This is true both for polynomials it was trained with (green), and for polynomials that are held-out and never encountered during training (pink). (Thick lines are averages over 5 runs, shown as thin light curves.)} \label{supp_fig:HoMM:polynomials_basic_meta_learning}
\end{figure}

\subsection{Shared $Z$ vs. separate task-embedding and data-embedding space} \label{app_lesion_results_shared_z}
Instead of having a shared $Z$ where data and tasks are embedded, why not have a separate embedding space for data, tasks, and so on? There are a few conceptual reason why we chose to have a shared $Z$, including its greater parameter efficiency, the fact that humans seem to represent our conscious knowledge of different kinds in a shared space \citep[][]{Baars2005}, and the fact that this representation could allow for zero-shot adaptation to new computational pathways through the latent space, analogously to the zero-shot language translation results reported by Johnson and colleagues \citep{Johnson2016a}. In this section, we further show that training with a separate task encoding space worsens performance, see fig. \ref{supp_lesion_shared_z_fig}. This seems to primarily be due to the fact that learning in the shared $Z$ accelerates and de-noises the learning process, see fig. \ref{supp_lesion_shared_z_learn_fig}. (It's therefore worth noting that running this model for longer could result in convergence to the same asymptotic generalization performance.) Note: this analysis was performed in an older implementation of the model. \par
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{2-HoMM/figures/poly/meta_results_unshared_vs_shared.png}
\caption[Having a separate embedding space for tasks results in worse performance on meta-mappings.]{Having a separate embedding space for tasks results in worse performance on meta-mappings. (Results are from only 1 run.)}
\label{supp_lesion_shared_z_fig}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{2-HoMM/figures/poly/meta_learning_curves_with_separate.png}
\caption[Having a separate embedding space for tasks results in noisier, slower learning of meta-mappings.]{Having a separate embedding space for tasks results in noisier, slower learning of meta-mappings. (Results are from only 1 run.)}
\label{supp_lesion_shared_z_learn_fig}
\end{figure}

\subsection{Hyper network vs. conditioned task network} \label{app_lesion_results_hyper}
Instead of having the task network $F$ parameterized by the hyper network $\mathcal{H}$, we could simply have a task network with learned weights which takes a task embedding as another input. In Fig. \ref{supp_fig:HoMM_arch_cond_vs_hyper}, we show that this architecture fails to learn the meta-mapping tasks, although it can successfully perform the basic tasks. We suggest that this is because it is harder for this architecture to prevent interference between the comparatively larger number of basic tasks and the smaller number of meta-tasks. While it might be possible to succeed with this architecture, it was more difficult in the hyper-parameter space we searched. See also Fig. \ref{supp_fig:human:lang_tcnh}, where we show that both architectures perform similarly for language generalization. \par 

\begin{figure}[H]
\centering
\includegraphics[width=0.66\textwidth]{2-HoMM/figures/conditioned_vs_hyper_polynomials.png}
\caption[The HyperNetwork-based architecture we propose outperforms a simpler architecture.]{The HyperNetwork-based architecture we propose in the main text performs better and more consistently on meta-mappings than a simpler architecture that simply concatenates a task representation to the input before passing it through a fixed MLP. Results are in the polynomial domain, c.f. Fig. \ref{fig:HoMM_polynomials:results}. Note that the task-concatenated architecture performs just as well at the trained basic tasks (not shown), it is adapting via meta-mappings that proves challenging for it. See Supp. Fig. \ref{supp_fig:extending:RL:arch_cond_vs_hyper} for a more dramatic comparison in the RL domain. }\label{supp_fig:HoMM_arch_cond_vs_hyper}
\end{figure}

\subsection{Meta-classification lesion} \label{app:homm:metaclass_lesion}

In Fig. \ref{supp_fig:HoMM:metaclass_lesion}, we show that meta-classification training is not beneficial in the polynomials domain. Specifically, on trained meta-mappings the HoMM model is achieving a normalized performance of 88.99\% (bootstrap 95\%-CI [88.20, 89.98]), while without meta-classification it is achieving a normalized performance of 89.7\% (bootstrap 95\%-CI [88.87, 90.61]). On new meta-mappings the HoMM model is achieving a normalized performance of 85.54\% (bootstrap 95\%-CI [85.14, 85.94]), while without meta-classification it is achieving a normalized performance of 86.29\% (bootstrap 95\%-CI [85.54, 86.79]). While these differences are significant (paired \(t\)-tests, respectively \(t(4) = 6.95, p = 0.002\) and \(t(4) = 3.06, p = 0.038\)), the effect is small. See also Fig. \ref{supp_fig:human:homm_metaclass_lesion} for marginal evidence that meta-classification may be helpful in the cards domain, where there are fewer training tasks. 

\begin{figure}[H]
\centering
\includegraphics[width=0.66\textwidth]{2-HoMM/figures/metaclass_lesion_polynomials.png}
\caption[In the polynomials domain, the HoMM model performs slightly better without meta-classification training.]{In the polynomials domain, the HoMM model performs slightly better without meta-classification training. This effect appears for both trained and held-out meta-mappings. However, the effect is small.}\label{supp_fig:HoMM:metaclass_lesion}
\end{figure}

