\chapter{Model details \& hyperparameters for all experiments} \label{appendix:model_hyperparameters}
\begin{table}
\scriptsize
\centering
\begin{tabular}{|p{3cm}||c|c|c|c|}
\hline
& Polynomials & Cards & Visual & RL \\\hline
\hline
$Z$-dimension & 512 & 512 & 512 & 512 \\\hline
$\mathcal{I}$ num. layers & \multicolumn{4}{c|}{2} \\\hline
$\mathcal{I}$ num. hidden units & \multicolumn{4}{c|}{128} \\\hline
$\mathcal{L}$ architecture & -  & \multicolumn{3}{c|}{2-layer LSTM + 2 fully-connected} \\\hline
$\mathcal{L}$ num. hidden units & -  & \multicolumn{3}{c|}{512} \\\hline
$\mathcal{T}$ num. layers & 1 & 3 & 1 & 3 \\\hline
$\mathcal{T}$ num. hidden units & - & 128 & - & 128 \\\hline
$\mathcal{M}$ architecture & \multicolumn{4}{c|}{2 layers per-datum, max pool across, 2 layers} \\\hline
$\mathcal{H}$ architecture & \multicolumn{4}{c|}{4 layers} \\\hline
$\mathcal{M}$ num. hidden units & \multicolumn{3}{c|}{512} & 1024 \\\hline
$\mathcal{H}$ num. hidden units & \multicolumn{4}{c|}{512} \\\hline
$\mathcal{F}$ num. layers & 3 & 1 & 1 & 3 \\\hline
$F$ num. hidden units & \multicolumn{3}{c|}{64} & 128 \\\hline
$F$ weight normalization \citep{Salimans2016} & \multicolumn{3}{c|}{No} & Yes \\\hline
$\mathcal{A}$ num. layers & \multicolumn{2}{c|}{1} & 2 & 1 \\\hline
$\mathcal{A}$ num. hidden units & \multicolumn{2}{c|}{-} & 128 & -  \\\hline
Nonlinearities & \multicolumn{4}{p{11cm}|}{Leaky ReLU most places, except no non-linearity at final layer of networks outputting to $Z$, sigmoid for classification outputs, and softmax over actions.} \\\hline
Base task loss & $\ell_2$ & $\ell_2$ (masked) & Cross-entropy & $\ell_2$ (masked)\\\hline
Meta-mapping loss & \multicolumn{4}{c|}{$\ell_2$}\\\hline
Partially-persistent task embeddings & \multicolumn{3}{c|}{No} & Yes \\\hline
Persistent embedding match loss weight & \multicolumn{3}{c|}{-} & 0.2 \\\hline
\hline
Optimizer & Adam & \multicolumn{3}{c|}{RMSProp} \\\hline
Learning rate (base) & $3\cdot 10^{-5}$ & $1\cdot 10^{-5}$ & $3\cdot 10^{-5}$ & $1\cdot 10^{-4}$\\\hline
Learning rate (meta) & $1\cdot 10^{-5}$ & $1\cdot 10^{-5}$ & $1\cdot 10^{-5}$ & $1\cdot 10^{-4}$\\\hline
L.R. decay rate (base) & $\times0.85$ & $\times0.85$ & $\times0.8$ & $\times0.8$\\\hline
L.R. decay rate (meta) & $\times0.85$ & $\times0.9$ & $\times0.85$ & $\times0.95$ \\\hline
L.R. min (base) & \multicolumn{2}{c|}{$3 \cdot 10^{-8}$}  & $1 \cdot 10^{-8}$ & $3 \cdot 10^{-8}$\\\hline
L.R. min (meta) & $1 \cdot 10^{-7}$& $3 \cdot 10^{-8}$ &  $1 \cdot 10^{-8}$ & $3 \cdot 10^{-7}$\\\hline
L.R. decays every & 100 epochs & 200 epochs & 400 epochs & 10000 \\\hline
Num. training epochs & 5000 & \multicolumn{1}{p{2.3cm}|}{100000 (optimally stopped)} & \multicolumn{1}{p{2.3cm}|}{10000 for 4 train mappings, 7500 for 8, 5000 for others} & \multicolumn{1}{p{2.3cm}|}{1000000 (optimally stopped)} \\\hline
Num. runs & 5 & 5 & 10 & 5 \\ \hline
\hline
Num. base tasks (training) & \multicolumn{1}{p{2.3cm}|}{1300 ( $= 60 + 60 \times  20 + 40$)} & 36 & Varies & 18 \\\hline
Num. base tasks (held out for MM eval) & 800 ($= 40 \times 20$)  & 4 & Varies & 2 \\\hline
Num. meta classifications & 6 & 8 & 8 & - \\\hline
Num. train MMs & 20 & 3 & Varies & 1 \\\hline
Num. held-out MMs & 16 & 0 & 2 & 0  \\\hline
Base dataset size & 1024 & 1024 & 336 & 64 \\\hline
Base examples size & 50 & 768 & - & 32 \\\hline
Meta dataset size (train) & 60 & 36 & Varies & 18 \\\hline
Meta examples (train) & \multicolumn{2}{c|}{Half of train dataset} & - & Half of train dataset \\\hline
Meta examples (eval) & \multicolumn{2}{c|}{All of train dataset} & - & All of train dataset \\\hline
Base datasets refreshed & \multicolumn{2}{c|}{Every 50 epochs} & Every 20 & Every 1500  \\\hline
Target network updated & \multicolumn{3}{c|}{-} & Every 10000 epochs  \\\hline
RL discount & \multicolumn{3}{c|}{-} & 0.85 \\\hline
RL explore prob. (\(\epsilon\)) & \multicolumn{3}{c|}{-} & \multicolumn{1}{p{2.3cm}|}{Initial: 1., decay: -0.03.}\\\hline
Action softmax inv. temp. (\(\beta\)) & - & 8 & - & 8\\\hline
\end{tabular}
\caption[Detailed hyperparameter specification.]{Detailed hyperparameter specification for different experiments. A ``-'' indicates a parameter that does not apply to that experiment. Where only one value is given, it applied to all the experiments discussed. As a reminder: the shared representational space is denoted by $Z$. Input encoder: $\mathcal{I}: \text{input} \rightarrow Z$. Action decoder $\mathcal{A}: Z \rightarrow \text{output}$. Target encoder $\mathcal{T}: \text{targets} \rightarrow Z$. Meta-network $\mathcal{M}: \{(Z, Z), ...\} \rightarrow Z $ maps examples to a task representation. Hyper-network $\mathcal{h}: Z \rightarrow \text{parameters}$. Task network $F: Z \rightarrow Z$ is parameterized by $\mathcal{H}$. Language encoder: $\mathcal{L}: \text{natural language} \rightarrow Z$. } \label{supp_hyperparameter_table}
\end{table}
See table \ref{supp_hyperparameter_table} for detailed architectural description and hyperparameters for each experiment. Hyperparameters were generally found by a heuristic search, where mostly only the optimizer, learning rate annealing schedule, and number of training epochs were varied. Some of the parameters take the values they do for fairly arbitrary reasons, e.g. the polynomial experiments were run earlier, before 1-layer task networks were found to be useful in some settings. While it would be ideal to fully search the space of parameters for all models, unfortunately our computational resource limitations prohibited it. Thus the results in the paper should be interpreted as a lower bound on what would be possible. \par
Each epoch consisted of a separate learning step on each task (both base and meta), in a random order. In each task, the meta-learner would receive only a subset (the ``batch size`` above) of the examples to generate a function embedding, and would have to generalize to the remainder of the examples in the dataset. The embeddings of the basic tasks used for meta-mappings were computed and cached once per epoch, so as the network learned over the course of the epoch, these task-embeddings would get ``stale,'' but this did not seem to be too detrimental. In the case of the RL tasks, where there were persistent task embeddings, they were used insteadd.\par
The results reported in the figures in this paper are averages across multiple runs, with different trained and held-out tasks (in the polynomial and visual concepts cases) and different network initializations and training orders each epoch (in all cases), to ensure the robustness of the findings. \par



\chapter{Supplemental material for Chapter \getrefnumber{chapter:zero_shot_via_homm}} \label{appendix:zero_shot_via_homm}

\section{Clarifying meta-mapping} \label{app_clarifying_meta_mapping}
\subsection{A definitional note}
When we discussed meta-mappings in the main task, we equivocated between tasks and behaviors for the sake of brevity. For a perfect model, this is somewhat justifiable, because each task will have a corresponding optimal behavior, and the sytem's embedding of the task will be precisely the embedding which produces this optimal behavior. However, behavior-irrelevant details of the task, like the color of the board, may not be embedded, so this should not really be thought of as a task-to-task mapping. This problem is exacerbated when the system is imperfect, e.g. during learning. It is thus more precise to distinguish between a ground-truth meta-mapping, which maps tasks to tasks, and the computational approach to achieving that meta-mapping, which really maps between representations which combine both task and behavior. \par

%\section{Card game $t$-SNE} \label{app_cards_tsne}
%We performed $t$-SNE \citep{LaurensvanderMaaten2008} on the task embeddings of the system at the end of learning the card game tasks, to evaluate the organization of knowledge in the network. In fig. \ref{fig_cards_tsne_basic} we show these embeddings for just the basic tasks. The embeddings show systematic grouping by game attributes. In fig. \ref{fig_cards_tsne_full} we show the embeddings of the meta and basic tasks, showing the organization of the meta-tasks by type. (Note: this analysis was performed in an older version of the model than the main results.)\par 
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.8\textwidth]{2-HoMM/figures/basic_tsne_basic_final.png}
%\caption{$t$-SNE embedding of the function embeddings the system learned for the basic card game tasks. (Note that the pairs of nearby embeddings differ in the ``suits rule`` attribute, discussed in appendix \ref{meth_data_cards}.)} 
%\label{fig_cards_tsne_basic}
%\end{figure}%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.9\textwidth]{2-HoMM/figures/basic_tsne_full_final.png}
%\caption{$t$-SNE embedding of the function embeddings the system learned for the meta tasks (basic tasks are included in the background).} 
%\label{fig_cards_tsne_full}
%\end{figure}

\section{Architecture experiments} \label{app_lesion_results}
In this section we consider a few variations of the architecture, to justify the choices made in the paper. \par

\subsection{Shared $Z$ vs. separate task-embedding and data-embedding space} \label{app_lesion_results_shared_z}
Instead of having a shared $Z$ where data and tasks are embedded, why not have a separate embedding space for data, tasks, and so on? There are a few conceptual reason why we chose to have a shared $Z$, including its greater parameter efficiency, the fact that humans seem to represent our conscious knowledge of different kinds in a shared space \citep[][]{Baars2005}, and the fact that this representation could allow for zero-shot adaptation to new computational pathways through the latent space, analogously to the zero-shot language translation results reported by Johnson and colleagues \citep{Johnson2016a}. In this section, we further show that training with a separate task encoding space worsens performance, see fig. \ref{supp_lesion_shared_z_fig}. This seems to primarily be due to the fact that learning in the shared $Z$ accelerates and de-noises the learning process, see fig. \ref{supp_lesion_shared_z_learn_fig}. (It's therefore worth noting that running this model for longer could result in convergence to the same asymptotic generalization performance.) Note: this analysis was performed in an older implementation of the model. \par
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{2-HoMM/figures/poly/meta_results_unshared_vs_shared.png}
\caption[Having a separate embedding space for tasks results in worse performance on meta-mappings.]{Having a separate embedding space for tasks results in worse performance on meta-mappings. (Results are from only 1 run.)}
\label{supp_lesion_shared_z_fig}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{2-HoMM/figures/poly/meta_learning_curves_with_separate.png}
\caption[Having a separate embedding space for tasks results in noisier, slower learning of meta-mappings.]{Having a separate embedding space for tasks results in noisier, slower learning of meta-mappings. (Results are from only 1 run.)}
\label{supp_lesion_shared_z_learn_fig}
\end{figure}

\subsection{Hyper network vs. conditioned task network} \label{app_lesion_results_hyper}
Instead of having the task network $F$ parameterized by the hyper network $\mathcal{H}$, we could simply have a task network with learned weights which takes a task embedding as another input. In Fig. \ref{supp_fig:HoMM_arch_cond_vs_hyper}, we show that this architecture fails to learn the meta-mapping tasks, although it can successfully perform the basic tasks. We suggest that this is because it is harder for this architecture to prevent interference between the comparatively larger number of basic tasks and the smaller number of meta-tasks. While it might be possible to succeed with this architecture, it was more difficult in the hyper-parameter space we searched.\par 

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{2-HoMM/figures/conditioned_vs_hyper_polynomials.png}
\caption[The HyperNetwork-based architecture we propose outperforms a simpler architecture.]{The HyperNetwork-based architecture we propose in the main text performs better and more consistently on meta-mappings than a simpler architecture that simply concatenates a task representation to the input before passing it through a fixed MLP. Results are in the polynomial domain, c.f. Fig. \ref{fig:HoMM_polynomials:results}. Note that the task-concatenated architecture performs just as well at the trained basic tasks (not shown), it is adapting via meta-mappings that proves challenging for it. See Supp. Fig. \ref{supp_fig:extending:RL:arch_cond_vs_hyper} for a more dramatic comparison in the RL domain. }\label{supp_fig:HoMM_arch_cond_vs_hyper}
\end{figure}

\section{Details of polynomial task domain} \label{app:HoMM:polynomials_methods}
We randomly sampled the train and test polynomials as follows:
\begin{enumerate}
\item Sample the number of relevant variables ($k$) uniformly at random from 0 (i.e. a constant) to the total number of variables.
\item Sample the subset of $k$ variables that are relevant from all the variables.
\item For each term combining the relevant variables (including the intercept), include the term with probability 0.5. If so give it a random coefficient drawn from $\mathcal{N}(0, 2.5)$.
\end{enumerate}
The data points on which these polynomials were evaluated were sampled uniformly from $[-1, 1]$ independently for each variable, and for each polynomial. The datasets were resampled every 50 epochs of training. \par
\textbf{Meta-tasks:} For meta-tasks, we trained the network on 6 task-embedding classification tasks:
\begin{itemize}
\item Classifying polynomials as constant/non-constant.
\item Classifying polynomials as zero/non-zero intercept.
\item For each variable, identifying whether that variable was relevant to the polynomial.
\end{itemize}
We trained on 20 meta-mapping tasks, and held out 16 related meta-mappings.
\begin{itemize}
\item Squaring polynomials (where applicable).
\item Adding a constant (trained: -3, -1, 1, 3, held-out: 2, -2).
\item Multiplying by a constant (trained: -3, -1, 3, held-out: 2, -2).
\item Permuting inputs (trained: 1320, 1302, 3201, 2103, 3102, 0132, 2031, 3210, 2301, 1203, 1023, 2310, held-out: 0312, 0213, 0321, 3012, 1230, 1032, 3021, 0231, 0123, 3120, 2130, 2013).
\end{itemize}

