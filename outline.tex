\chapter{Research roadmap} \label{RR_sec}

\section{My prior work}
In my prior work, I have explored transfer and flexibility from both computational and experimental perspectives. In this section I briefly describe some of this work which I intend to include in my dissertation. \par

\textbf{Understanding human concept learning and flexibility:} In prior work \citep{Lampinen2017b}, I explored how different presentations of mathematical concepts affect human learning and ability to generalize this learning to different or more formal tasks. This captures both how transfer/grounding from slowly-learned prior knowledge can affect rapid learning of new knowledge, and how humans are able to flexibly apply this new knowledge to new situations, and generate abstractions about it. \par 
\textbf{Exploring multi-task benefits in non-linear networks:} In another line of work, I have explored the benefits of multi-task learning from a theoretical perspective. I have analyzed empirically the evolution of the representations of a network learning multiple related tasks \citep{Lampinen2017a}, yielding a minimal example of a network which shows extraction of shared structure. In brief, while a linear network cannot exploit or represent shared structure (up to a rotation of its representation space), a nonlinearity at the output suffices to allow representation of this shared structure. \par
I have also made some (unpublished) explorations of multi-task transfer with neither shared inputs nor outputs, based on purely functional relationships. These have been in the domain of learning binary functions, and show interesting patterns of transfer based on some as-yet-difficult-to-define notion of functional similarity, whether the multi-task learning is sequential or simultaneous. This helps us to understand how multi-task transfer generalizes beyond pairs of perfectly analogous tasks. \par
\textbf{Towards a theoretical understanding of multi-task benefits in linear networks:} More recently, we have derived a fully-analytic theory of generalization dynamics in deep linear neural networks \citep{Lampinen2019}, which we apply to understanding the issue of multi-task transfer. We show that transfer is most beneficial in the regime of learning a low SNR task with well-aligned auxiliary tasks (and that this pattern is qualitatively quite similar in deep nonlinear networks). I suggest that this is precisely the regime that humans work in -- we have many auxiliary tasks that are well aligned (often by cultural or pedagogical construction), and these allow us to learn well even from small samples (which are inherently lower SNR). \par 

\section{Towards more flexible deep learning architectures} \label{eml_sec}

In this section I propose a computational framework for thinking about the relationship between slow and fast transfer. This proprosal is in the form of a deep learning architecture which I will argue captures some of the aspects of human flexibility that are still missing from most contemporary deep learning models. \par

This architecture is based on ideas from meta-learning. In particular, as noted above, the fundamental insight of meta-learning is that there is a continuum between tasks and data. I attempt to follow this premise to its logical conclusion by implementing architectures which minimize the distinction between data and tasks. At the same time, my architecture exploits different timescales of learning to allow the system to slowly accumulate knowledge about regularities among data or tasks, and rapidly adapt to new data or tasks consistent with this learned structure. \par

In particular, our system learns to map different kinds of knowledge -- data, tasks, strategies, language, etc. -- into a common representational space, and meta-learns to compute mappings over this space. This allows the system to rapidly and flexibly learn and adapt. Many of the features of human flexibility are represented under a fairly unified computational principle in this framework. For example, following instructions to perform zero-shot on a task is just a mapping from language to a behavioral strategy. Trying to alter behavior on a task, such as switching from trying to win a game to trying to lose, is a mapping from one behavioral strategy to another. Explaining behavior is a mapping from behavioral strategies to language. Our system makes all these mappings in-principle learnable. \par 

One can loosely think of the common representational space as being something like a global workspace of consciousness, and the mappings within it as being the operations that we can consciously perform. Of course, these operations are constrained by the experiences we've had and the mappings we've been required to make, so the space of conscious representation may be set up to make certain types of mappings more easily realizable than others on certain kinds of data. Nevertheless, we are in principle capable of learning an arbitrary mapping between any sets of things of which we are conscious, whereas we are not necessarily capable of learning an arbitrary mapping between features that are unconsciously represented in the brain, such as the activity of a single neuron. The features which the brain learns to consciously represent are precisely those that are useful for the computations we consciously perform. These prior expectations contribute to our flexibility within the broad bounds of learned schemas, but our difficulty with adapting to tasks which are completely unlike those we've encountered before. Our architecture attempts to imitate this combination of inherent flexiblity and learned specialization. \par  

\subsection{Implementation details}
The essential idea of the architecture is that we separate the system into two parts:
\begin{enumerate}
\item Domain specific encoders and decoders (vision, language, etc.) that map into a shared embedding space $Z$
\item A meta-learning system which:
    \begin{enumerate}
    \item Embeds tasks into the shared embedding space $Z$.
    \item Learns to perform tasks in accordance with their embeddings.
    \end{enumerate}
\end{enumerate}
The utility of having a completely shared space $Z$ in which data, language, and tasks are represented is that it allows for arbitrary mappings between these distinct types of data. In addition to basic tasks, the system could in principle learn to map language to tasks (follow instructions) or tasks to language (explaining behavior), or tasks to tasks (changing behavior). This is a step closer to the flexible reasoning humans display. \par
Without training on these mappings, of course, the system will not be able to execute them well. However, ideally if it is trained on a braod enough set of such mappings, it will be able to generalize these to new instances drawn from the same data/task distribution. For instances that fall outside its data distribution or for optimal performance it may require some retraining. This reflects the structure of human behavior -- we are able to learn rapidly when new knowledge is relateively consistent with our previous knowledge, but learning an entirely new paradigm (such as calculus for a new student) can be quite slow. \par

\begin{figure}
\centering
\begin{tikzpicture}[auto]

\node [dashblock] at (0, 0) (rep) {\begin{tabular}{c}$\bm{Z}$: \textbf{Shared} \\ \textbf{representation}\end{tabular}};

% inputs, outputs

\draw [boundingbox] (-9, -5) rectangle (-2.1, 5);
\node [text=gray] at (-8.5, 4.65) {\textbf{I/O}};

\node [conc] at (-6, -4) (perc) {\textbf{Basic Input}};
\node [conc] at (-7.25, -2.5) (lang) {\textbf{Language}};
\node [conc] at (-6, 4) (act) {\textbf{Basic Output}};
\node [block, text width=2cm] at (-4, -2) (IE) {$\bm{\mathcal{I}}$: \textbf{Input Encoder}};
\node [block, text width=2cm] at (-5, -0.5) (LE) {$\bm{\mathcal{L}}$: \textbf{Lang. Encoder}};
\node [block, text width=2.2cm] at (-3.5, 2.5) (OD) {$\bm{\mathcal{O}}$: \textbf{Output Decoder}};
\node [block, text width=2cm] at (-5.5, 1) (TE) {$\bm{\mathcal{T}}$: \textbf{Target Encoder}};
\path [line] (perc.north) to (IE.south);
\path [line] (lang.north) to ([xshift=0.05cm, yshift=0.05cm]LE.south west);
\path [line] ([xshift=-0.05cm, yshift=-0.05cm]IE.north east) to (rep.south west);
\path [line] (LE.east) to (rep.west);
\path [line] (rep.north west) to ([xshift=-0.05cm, yshift=0.05cm]OD.south east);
\path [line] (OD.north) to (act.south);
\path [line] (act.south) to (TE.north);
\path [line] (TE.east) to (rep.north west);

% meta
\draw [boundingbox] (-1.9, 5) rectangle (6, -5);
\node [text=gray] at (-0.4, 4.7) {\textbf{Meta Learner}};

\node [dashblock] at (0, -2.5) (collection) {
\(\left\{
\begin{matrix}
(\text{in}_0, \text{out}_0),\\
(\text{in}_1, \text{out}_1),\\
$\vdots$
\end{matrix}\right\}\)};
\path [line] (rep.south) to (collection);
\path [line] ([xshift=-1em]rep.south) to (collection);
\path [line] ([xshift=1em]rep.south) to (collection);

\node [block] at (4, 0) (meta) {\begin{tabular}{c}$\bm{\mathcal{M}}$: \textbf{Meta} \\ \textbf{network}\end{tabular}};
\path [line] (collection.south) to [out=-90, in=-90] (meta.south);
\path [line] (meta.west) to (rep.east);

% hyper

\node [block] at (4, 3) (hyper) {\begin{tabular}{c}$\bm{\mathcal{H}}$: \textbf{Hyper} \\ \textbf{network}\end{tabular}};
\node [block, dash pattern=on 9pt off 2pt] at (0, 2) (transform) {\(f: \text{rep} \rightarrow \text{rep}\)};

\path [draw, ->, very thick] (rep.north east) to (hyper.south);
\path [draw, ->, very thick] (hyper.west) to (transform.north east);
\path [draw, ->, very thick] ([xshift=-1em]transform.south) to ([xshift=-1em]rep.north);
\path [draw, ->, very thick] ([xshift=1em]rep.north) to ([xshift=1em]transform.south);

\end{tikzpicture}
\caption{Schematic of our general architecure. Blocks with solid edges denote deep networks with learnable parameters, dashed edges represent inputs, outputs, embeddings, etc., and $f$ is a deep network with parameters specified by $\mathcal{H}$.} \label{architecture_fig}
\end{figure}
More formally, we take a \textbf{functional} perspective on learning. A datum can be represented by a constant function which outputs it. (For example, each point in the latent space of an autoencoder can be thought of this way.) This allows us to interpret model inputs or outputs as functions. \par
We can then interpret most machine learning tasks as a mapping of functions to functions. These functions could represent data\footnote{Where ``data'' is a quite flexible term. The approach is relatively agnostic to whether the learning is supervised or reinforcement learning, whether inputs are images or natural language, etc.}, or they could be functions that operate on functions themselves. Under this perspective, learning tasks and learning to flexibly map between tasks, or learning to map from language to tasks, are all the same type of problem. \par
Specifically, we embed inputs, targets, and mappings into a shared representational space $Z$. Inputs are embedded by a deep network $\mathcal{I}: \text{input} \rightarrow Z$. Outputs are decoded from the representational space by a deep network $\mathcal{O}: Z \rightarrow \text{output}$. Targets are encoded by a deep network $\mathcal{T}: \text{targets} \rightarrow Z$. (Targets do not necessarily need to be output-like, e.g. in our RL tasks, we use (action, outcome) tuples as ``targets.'') \par
Given this, the task of mapping inputs to outputs can be framed as trying to find a transformation of the representational space that takes the (embedded) inputs from the training set to the (embedded) targets. These transformations are performed by a system with the following components:
\begin{itemize}
\item $\mathcal{M}: \{(Z, Z), ...\} \rightarrow Z $ -- the meta network, which takes a set of (input embedding, target embedding) pairs and produces a function embedding.
\item $\mathcal{H}: Z \rightarrow \text{parameters}$ -- the hyper network, which takes a function embedding and produces a set of parameters.
\item $f: Z \rightarrow Z$ -- the transformation, implemented by a deep network with parameters specified by $\mathcal{H}$.
\end{itemize}
See Fig. \ref{architecture_fig} for a schematic of the architecture. \par
\textbf{Operation:} A basic forward pass through the system might look as follows.
\begin{enumerate}
\item A training dataset of (input, target) pairs is embedded by $\mathcal{I}$ and $\mathcal{T}$ to produce a set of paired embeddings. Another set of (possibly unlabeled) inputs is provided and embedded.
\item The meta network $\mathcal{M}$ maps the set of embedded (input, target) pairs to a function embedding.
\item The hyper network $\mathcal{H}$ maps the function embedding to parameters for $f$, which is used to transform the second set of inputs to a set of output embeddings.
\item The output embeddings are decoded by $\mathcal{O}$ to produce a set of outputs.
\end{enumerate}
To write this explicitly, suppose we have some dataset of input, target pairs ($D_1 = \{(x_0, y_0), ...\}$), and some input $x$ for which we wish to generate a predicted output $\hat{y}$. This output would be generated as follows:
$$\hat{y} = \mathcal{O}\left(f_{D_1}\left(\mathcal{I} \left(x\right)\right) \right)$$
where $f_{D_1}$ is the transformation the meta-learner guesses for the training dataset $D_1$:
$$f_{D_1} \text{ is parameterized by } \mathcal{H}\left(\mathcal{M}\left( \left\{\left(\mathcal{I}\left(x_0\right), \mathcal{T}\left(y_0\right) \right), \left(\mathcal{I}\left(x_1\right), \mathcal{T}\left(y_1\right) \right), ... \right\}\right)\right)$$
This system can be trained end-to-end if labels are provided for a second set of inputs. In particular, suppose we have some loss function $\mathcal{L}(y, \hat{y})$ defined on a single target output $y$ and actual model output $\hat{y}$, for some input $x$. We define our total loss computed on some dataset $D_2$ as:
$$\mathbb{E}_{(x, y)\in {D}_2} \left[ \mathcal{L}\left(y, \mathcal{O}\left(f_{D_1}\left(\mathcal{I} \left(x\right)\right) \right)\right)\right]$$
More generally, suppose we have some input which is already embedded in the representation space $z_{in} \in Z$, and an embedded dataset $D_Z$ of (embedding, target embedding) pairs $\{(z_{in,0}, z_{out,0})\}$. Then we can generate an output $\hat{z}_{out} \in Z$ as:
$$\hat{z}_{out} = f_{D_Z}(z_{in}) \qquad \text{where } f_{D_Z} \text{ is parameterized by } \mathcal{H}\left(\mathcal{M}\left(D_Z\right)\right)$$
This output can then be appropriately dispatched depending on the task at hand. For example, if the $z_{in,i}$ are the system's embeddings for trying to win various games, and the $z_{out,i}$ are the corresponding embeddings for trying to lose those games, then $z_{out}$ could be interpreted as the system's guess at a losing strategy for the game embedded as $z_{in}$, and then could be used to play that game. We could then evaluate performance by how well the system actually performs at losing with the $z_{out}$ strategy. \par
Similarly, we could map from language to a task embedding, and then ask how well the system performs at the task specified by language. The key feature of our architecture -- the fact that tasks, data, and language are all embedded in a shared space -- allows substantial flexibility within a unified system.



\subsection{Results \& future directions}

\textbf{Results:} Briefly, the system is able to do meta-learning in a toy card-game domain I've constructed, that is, it's able to learn a held-out game from seeing a single batch of examples. It's also able to do mapping of tasks, such as trying to lose (based on task-mapping examples), even on held-out games. It can also achieve some success at these behaviors based on natural language input, although it generalizes less well from that than from examples. This is not too surprising, as the structure of a task is more directly captured by examples than by a symbolic encoding. \par 
\textbf{Future directions:}
There are a number of future directions I would like to explore:
\begin{itemize}
\item Can we find a task that demonstrates the ability of this architecture to perform comparably to humans, like OmniGlot \citep{Lake2015} but for more general flexibility?
\item Is there a way we can do unsupervised learning in the latent representation space that would approximate something like rerepresentation \citep{Karmiloff-Smith1986}? Could we do something to generate hypotheses or interesting experiments for active learning \citep{Markant2014a}? 
\item We've assumed knowledge of which experiences come from which tasks. However, this is an important learning problem in its own right, and it would be nice to try to solve it as well, perhaps with an approach similar to \citet{Achille2018}. 
\end{itemize}


\section{Towards a better understanding of human transfer and flexibility}
In this section I propose a set of behavioral experiments that will help us to probe the extent of human ability to transfer knowledge within an experimental session. This work builds off of work showing that humans can learn artificial grammars \citet{Cleeremans1991}, and can transfer this knowledge to isomorphic grammars with novel symbols \citep{Tunney2001}. It also builds off work on more complex graph-structure learning, starting with work by Anna Schapiro and colleagues \citep{Schapiro2013}, and continuing to more recent work showing that some graph structures are more learnable than others \citep{Kahn2018}. \par
The proposed experiment is as follows. We will have subjects perform two different tasks. The first will be analogous to the initial learning stage in \citet{Kahn2018}, and will consist of subjects trying to press key combinations in response to lit patterns of squares. Unbeknownst to them, these stimuli will come from a random walk on a structured graph. \citet{Kahn2018} showed that participants implicitly learned about the structure of this graph. After the first part of the experiment, participants will advance to a second part. This will be essentially analogous to the first, except that the cover task will be hitting single letters displayed on the screen, as in \citep{Cleeremans1991}. \par
We will experimentally manipulate the underlying graph structure which generates subjects stimuli in each stage of the experiment. In the first stage, subjects may either experience the modular structure of \citep{Schapiro2013} or a random graph which has the same number of nodes and edges (this will be randomly generated but fixed across subjects). See Fig. \ref{graph_struct_fig} for diagrams of the two structures. The structure in the second task will be crossed with this in a 2 x 2 design. We can thus examine whether there is a benefit to learning the second struture (i.e. more rapid learning and/or fewer mistakes) after an isomorphic graph as opposed to a non-isomorphic one.  \par
Finally, because I am interested in flexibility, we will also assess subjects' ability to explicitly reason about the structures they encountered. Specifically, we will reveal that the stimuli were generated from a structure, and give a subjects a two-alternative forced choice between the two structures, and ask them to pick which one they experienced in the second part of the experiment. We will also ask them to place the stimuli they encountered on this graph, and then evaluate how well they were able to explicitly reconstruct the true structure. \par

\subsection{Results and implications}
I believe this is an interesting study for a number of reasons. First, if we see transfer at the level of procedural performance, it will allow us to increase the upper bound on the complexity of structures which can be transferred over a short time scale. Prior work has shown that structures of this complexity can be learned, and that simpler structures can be transferred, but we are not aware of any work showing transfer effects with structures of this complexity. Second, the question of how we can reuse implicitly learned knowledge explicitly is underexplored. If subjects are not at chance on our explicit reasoning outcome measures, it will be interesting to investigate what predicts their performance on these questions. Both of these outcomes will help us to elucidate what sort of flexibility humans have when learning complex structures over short time scales. \par 
Furthermore, this work may have other implications for the literature. For example, \citet{Kahn2018} interpreted their results as showing that some structures are inherently more learnable than others. However, if we observe transfer benefits, an alternative interpretation would be that those results are due to the presence of structures more like this (hierarchical, clustery) in the world rather than any inherent learnability advantages. \par

\section{Conclusion}

In this paper, I have outlined a perspective on human flexibility and transfer. I have argued that flexibility and transfer arise from the interaction of complementary learning systems that learn across distinct timescales. These range from the slow accumulation of cultural knowledge, to our accumulation of statistical knowledge over our lifetimes, to our learning how to follow instructions. By leveraging our slowly-learned prior knowledge and our fast-learning systems, we are able to behave flexibly and learn rapidly in new situations. \par
I have related this cooperation between slow and fast learning systems to two different strategies for transfer in machine learning: multi-task learning and meta-learning. I have reviewed the literature on these topics, examined how it relates to human flexibility, and explored some human capabilities that are still missing. I have argued that combining different learning systems will be necessary to achieve more human-like flexibility. \par 
Building off this conceptual framework, I have outlined some of my prior work that explores the benefits of multi-task transfer in neural networks, and the benefits of multiple presentations in mathematical concept learning. I have also outlined a new study that will further explore the extent of human flexibility and transfer. Finally, I have proposed a new deep-learning architecture which combines fast and slow-learning systems to achieve more human-like flexibility. Taken together, I hope this research will shed more light on human learning and flexibility. \par 
